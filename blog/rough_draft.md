### Mango Evolve

In October Alex Zhang created a blog post about RLMs. [Insert Links] I highly recommend reading the paper as it's a pretty fast read and gives two pretty useful insights in how one could deal with long context (putting the prompt in the llms environment and having an llm call other llms). At the time I was also working on a line of work related to open evolve / shinka evolve esque stuff, and well it seemed like these two things could fit together pretty well. But first let's get into what these do.
### What is AlphaEvolve
- AlphaEvolve [cite paper] is a paper released by Google. The core intuition behind it is to take the ideas of llms and genetic algorithms and sort of mash them together.  The tl;dr of the paper is that if you query an llm a bunch of times, choose some samples of said query, and then feed them back into the llm to iterate and repeat this process a bunch of times it's pretty good at pushing the bounds on verifiable and scorable problems. Think problems like how many circles can fit into a unit square, how to more efficiently multipy 2 matrices, optimizing arithmetic circuits, and managed to write an attention kernel which improved Gemini training time by 1%. There have also been a bunch of offshoots either in the realm of startups, research (link to Anne's thing), or other improvement (link to ShinkaEvolve). Hopefully, this convinces you this thing ends up being useful.
- However, how does alpha evolve work more deeply under the hood. Specifically, we care about how do we choose which samples to iterate on. As the paper is closed source, we will instead refer to the implementation known as openevolve as we can peak at the code.
	- Specifically the architecture involves a controller,  llm samplers, and an environment to eval. The controller is responsible for creating a database of the samples. The database is composed by taking samples and distributing them into islands (this is done arbirtarily). Then each island has a grid which scores the solutions performance and diversity as well as some logic which moves things between islands. Then based off of these performance and diversity scores the controller chooses which samples to iterate on.
- This may seem complicated, but in practice is quite simple and is a great intuition to solve a bunch of problems.
### What is RLM
- Recursive Language Models is some research that was released in October and more fleshed out in a paper at the end tip of last year [insert paper link]. Specifically the paper deals with issues with long context. I mean we all dealt with it. There are problems in which we'd like to stick a bunch of information into an LLM but it's either too much or there is a failure case at long context. A canonical example is a really really fat codebase. The proposed solution to this is to introduce all of the this context / super long prompt as a variable in a python repl and add the ability in that repl to call other llms (ie. add an llm_query function). Then we give a root lm the repl and say go answer the question in `x`. Ends up this works quite well as the lm can peak at the prompt / context and figure out how to dissect and deal with the problem as appropriate. This simple bitter-lesson pilled methodology managed to do really really well in long context tasks. I imagine this will have a large impact on post-training / industry. Sortof like how top-p is used sampling but no one thinks about it.
### How do these concepts fit together?
So I gave you this tl;dr about sampling which sortof is correct, but notice a lot of the conceptual complexity is in how we do the sampling. Part of the reason we do this sampling is that we can't just stick everything into the llm as the number of samples explode, the context blows up way over the max size. Therefore, instead of adatabase, we can have the llm explore solutions by itself using an RLM(ish), and have it select which samples to evolve in the database.

# Architecture
Relative to AlphaEvolve, MangoEvolve uses Repl Based Evolution. We also allow a single root lm, and multiple possible children lms (ie. the root lm can invoke opus, gpt, gemini, etc. in the same run). The way it works is as follows
1. We do a calibration phase where the root lm is allowed to query the various children lms questions to figure out its capabilities. This produce a note that is attached to the scratchpad during evolutions. The scratchpad here is a bit of text that the root lm produces that is attached to its prompt in future evolutions.
- The Root LLM is asked to produce an output with ` ```python ``` ` blocks that call `spawn_children()` with bespoke prompts; prompts can include `{{CODE_TRIAL_X_Y}}` tokens to reference prior code which are prostprocessed to corresponding code from past trials (this is to reduce costs but in practice you don't need the `{{CODE_TRIAL_X_Y}}` tokens)
- Child LLMs receive a minimal, task‑focused system prompt; their responses are parsed for code, evaluated, and logged.
- The Root LLM is fed a compact generation summary (scores, errors, code refs, lineage + scratchpad), plus an explicit selection request. This is done by emitting a trial variable which it can interact with in order to prep prompts for its next `spawn_children()` call. This feedback is the main refinement signal for the next prompt round.
- We repeat this process a preset number of times.
### Performance gains and interesting behaviors
- Generally, the main results here is that given the following configurations we are able to outperform openevolve at similar costs to circle packing as shown below. Outperform here means either hitting a hire score or generally being able to hit roughly the 2.635983 threshold which tends to be the maxima for this porblem (partiailly this is do to numerical accuracy beyond 1e-6 being difficult to do). The top scores (as well as relaxed scores which are working scores in cases there is no overlap can be seen below). As well as how many trials it took to reach roughly 2.635983 .  Trial here is defined as an llm call to generate code to evaluate rather than an evaluation like the case with shinkaevolve. Also note above this threshhold that things tend to get fuzzy as in the case with shinka evolve where there was a massive hit in maximum size of (look up number) to (look up number) when the strict formulation was used. The openevolve config uses the same models as open evolve (claude 3.7 and gemini 2 flash) and the gemini 3 config uses gemini 3 config uses gemini 3 flash with thinking.


### Abalations
 The important thing about abalations is actually the convergence behavior tends to be much smoother when memory is used with the scratchpad and query_llm is used. I hypothesize that this is do to stuff.
(Insert a graph here)

We can also go deep into how query_llm and the scratchpad was used. As well as what was done in the REPL (thank you claude for doing this categorization)
Do tables

Generally you can see ....

### Emergent Behaviors
The root_llm acts as a strict orchestrator, not just a recommender. It asserts constraints, prioritizes execution efficiency, and actively suppresses risky or slow approaches:

- Hard limits on iterations and timeouts show a controlling posture. Example: root explicitly instructs “do not increase niter above 30” and warns about timeout issues. See `saved_experiments/circle_packing_opus_thinking_mixed_20251231_135159/root_llm_log.jsonl:44`.
- It directly manages model roles, selecting low‑cost models for exploration and restricting heavier models when they fail or are slow. In 163512, it explicitly says to avoid opus for some rounds after timeouts. See `saved_experiments/circle_packing_opus_thinking_mixed_20251231_163512/root_llm_log.jsonl:69`.
- It includes explicit “selection reasons” (performance + diversity + potential), indicating the controller is mediating both exploitation and exploration, rather than letting random variance drive progression. See `saved_experiments/circle_packing_opus_thinking_mixed_20251231_163512/root_llm_log.jsonl:37`
