{
  "trial_id": "trial_6_4",
  "generation": 6,
  "parent_id": null,
  "code": "",
  "metrics": {},
  "prompt": "\nImplement multi-start, multi-anneal L-BFGS-B (budget-friendly) with analytic gradients on (x,y,r), then final LP:\n- Use 6-10 restarts across 2 anneal schedules for weights (e.g., [(50,50)->(300,300)] and [(100,50)->(600,400)])\n- Bounds: x,y in [0,1], r in [0,0.6]\n- Start seeds: deterministic hex, edge lines with slight jitter, interior clusters\n- Use robust, vectorized gradients; eps only in divisions; no prints\n- Finalize with LP and strict feasibility\n\nWrite a single Python file with NO prose/explanations, only code.\nRequirements:\n- Allowed modules: numpy, scipy\n- Provide entry function: construct_packing() -> (centers, radii, sum_radii)\n  * centers: np.ndarray shape (26,2), radii: np.ndarray shape (26,), sum_radii: float\n- All circles strictly inside [0,1]^2 and pairwise non-overlapping (tolerance 1e-6)\n- Return a strictly feasible solution; if your optimizer ends infeasible, post-process by LP to strict feasibility\n- Use runtime-conscious design (<=300s), no prints, deterministic seeds\n\nLP Feasibilization spec (maximize sum of radii for fixed centers):\n- Variables: r_i (i=1..26), objective: maximize sum(r_i)\n- Constraints:\n  - r_i >= 0\n  - r_i <= x_i\n  - r_i <= y_i\n  - r_i <= 1 - x_i\n  - r_i <= 1 - y_i\n  - r_i + r_j <= dist_ij - 1e-6 for all i<j\n- Implement with scipy.optimize.linprog (method='highs') by minimizing -sum(r_i)\n- Always call the LP at the end to ensure strict feasibility; do not rely solely on penalties\n\nNumerical/gradient guidance:\n- Use fully vectorized NumPy; no Python loops over pairs\n- For pairwise distances, use d = sqrt((dx)^2 + (dy)^2); when dividing by d, stabilize with denom = sqrt(d^2 + eps^2) or (d + eps). Prefer applying eps only in division (not in the penalty) to avoid bias.\n- Overlap penalty (for i<j): hinge = max(0, r_i + r_j - d_ij); use 0.5 * w_over * hinge^2\n- Boundary penalty: for each i, sum hinges on [r_i - x_i, r_i - y_i, r_i - (1-x_i), r_i - (1-y_i)], squared with weight w_b\n- Optionally include a small smoothness/centering/edge-bias term but ensure it doesn't bias the final LP step\n- Final feasibility check (assertions) must pass with tolerance 1e-6\n\nSeeding:\n- Provide diverse deterministic seeds: triangular/hex grids, edge-biased seeds, center-biased clusters\n- Prefer a few high-quality restarts to many poor starts\n\nReturn strictly feasible packing with the highest possible sum of radii.\n\n",
  "response": "import numpy as np\nfrom scipy.optimize import minimize, linprog\n\ndef construct_packing():\n    np_rng = np.random.RandomState(123456)\n\n    n = 26\n    eps_div = 1e-8\n    pair_eps_lp = 1e-6\n\n    # Bounds for variables: x,y in [0,1], r in [0,0.6]\n    bounds = [(0.0, 1.0)] * (2 * n) + [(0.0, 0.6)] * n\n\n    def unpack(z):\n        x = z[:n].copy()\n        y = z[n:2 * n].copy()\n        r = z[2 * n:].copy()\n        return x, y, r\n\n    def pairwise_d(x, y):\n        dx = x[:, None] - x[None, :]\n        dy = y[:, None] - y[None, :]\n        d = np.sqrt(dx * dx + dy * dy)\n        return dx, dy, d\n\n    def objective_and_grad(z, w_over, w_b):\n        x, y, r = unpack(z)\n\n        dx = x[:, None] - x[None, :]\n        dy = y[:, None] - y[None, :]\n        d = np.sqrt(dx * dx + dy * dy)\n\n        # Prevent self-interaction\n        diag_idx = np.arange(n)\n        d[diag_idx, diag_idx] = 1.0  # safe for division\n        s_pair = (r[:, None] + r[None, :]) - d\n        mask = s_pair > 0\n        mask[diag_idx, diag_idx] = False\n        s_pair_masked = s_pair * mask\n\n        # Overlap penalty\n        over_pen = 0.5 * w_over * np.sum(s_pair_masked * s_pair_masked)\n\n        # Boundary hinges\n        s_xlo = r - x\n        s_ylo = r - y\n        s_xhi = r - (1.0 - x)\n        s_yhi = r - (1.0 - y)\n        s_bnd = np.vstack([s_xlo, s_ylo, s_xhi, s_yhi]).T\n        mask_bnd = s_bnd > 0\n        bnd_pen = 0.5 * w_b * np.sum((s_bnd * mask_bnd) ** 2)\n\n        # Small regularizer (very small)\n        reg = 1e-8 * (np.sum((x - 0.5) ** 2) + np.sum((y - 0.5) ** 2))\n\n        f = -np.sum(r) + over_pen + bnd_pen + reg\n\n        # Gradients\n        grad_x = np.zeros_like(x)\n        grad_y = np.zeros_like(y)\n        grad_r = np.zeros_like(r)\n\n        # grad_r: -1 + w_over * sum_j s_ij(masked) + w_b * sum_b s_bi(masked)\n        grad_r[:] = -1.0 + w_over * np.sum(s_pair_masked, axis=1) + w_b * np.sum(s_bnd * mask_bnd, axis=1)\n\n        # grad_x and grad_y from pair penalties\n        denom = d + eps_div\n        factor = w_over * s_pair_masked / denom  # shape n x n\n        # grad_x_i contribution: - sum_j factor_ij * dx_ij\n        grad_x += -np.sum(factor * dx, axis=1)\n        grad_y += -np.sum(factor * dy, axis=1)\n\n        # grad_x and grad_y from boundary penalties\n        # s_xlo = r - x -> derivative wrt x is -1 when active\n        grad_x += w_b * (-s_xlo * (s_xlo > 0) + s_xhi * (s_xhi > 0))\n        grad_y += w_b * (-s_ylo * (s_ylo > 0) + s_yhi * (s_yhi > 0))\n\n        # reg grads\n        grad_x += 2e-8 * (x - 0.5)\n        grad_y += 2e-8 * (y - 0.5)\n\n        grad = np.concatenate([grad_x, grad_y, grad_r])\n        return f, grad\n\n    # Seeds generation: deterministic diverse seeds\n    seeds = []\n\n    # 1) Hex-like packing with row lengths summing to 26: [6,5,6,5,4]\n    rows = [6, 5, 6, 5, 4]\n    y_positions = np.linspace(0.1, 0.9, len(rows))\n    hex_pts = []\n    for yi, cnt in zip(y_positions, rows):\n        if cnt % 2 == 0:\n            xs = np.linspace(0.1, 0.9, cnt)\n        else:\n            xs = np.linspace(0.1, 0.9, cnt)\n        # stagger alternate rows\n        hex_pts.extend([(x, yi) for x in xs])\n    hex_pts = np.array(hex_pts)[:n]\n    jitter = (np_rng.rand(n, 2) - 0.5) * 0.01\n    hex_pts = np.clip(hex_pts + jitter, 0.02, 0.98)\n    seeds.append(hex_pts)\n\n    # 2) Edge-biased: distribute around perimeter and a few interior\n    edge_pts = []\n    m_edge = 20\n    t = np.linspace(0, 1, m_edge, endpoint=False)\n    for tt in t[:7]:\n        edge_pts.append((tt * 0.95 + 0.025, 0.02))\n    for tt in t[:7]:\n        edge_pts.append((tt * 0.95 + 0.025, 0.98))\n    for tt in t[:6]:\n        edge_pts.append((0.02, tt * 0.95 + 0.025))\n    for tt in t[:6]:\n        edge_pts.append((0.98, tt * 0.95 + 0.025))\n    edge_pts = np.array(edge_pts)[:n]\n    jitter = (np_rng.rand(n, 2) - 0.5) * 0.02\n    edge_pts = np.clip(edge_pts + jitter, 0.02, 0.98)\n    seeds.append(edge_pts)\n\n    # 3) Three clusters\n    clusters = []\n    centers = np.array([[0.25, 0.25], [0.75, 0.25], [0.5, 0.75]])\n    counts = [9, 9, 8]\n    idx = 0\n    for c, cnt in zip(centers, counts):\n        for _ in range(cnt):\n            ang = np_rng.rand() * 2 * np.pi\n            rad = 0.05 + np_rng.rand() * 0.03\n            pt = c + np.array([np.cos(ang), np.sin(ang)]) * rad\n            clusters.append(pt)\n            idx += 1\n    clusters = np.array(clusters)[:n]\n    clusters = np.clip(clusters, 0.02, 0.98)\n    seeds.append(clusters)\n\n    # 4) Grid-ish with slight offset\n    gx = np.linspace(0.08, 0.92, 6)\n    gy = np.linspace(0.08, 0.92, 5)\n    grid_pts = []\n    for xi in gx:\n        for yi in gy:\n            grid_pts.append((xi, yi))\n    grid_pts = np.array(grid_pts)[:n]\n    jitter = (np_rng.rand(n, 2) - 0.5) * 0.015\n    grid_pts = np.clip(grid_pts + jitter, 0.02, 0.98)\n    seeds.append(grid_pts)\n\n    # 5) Diagonal + anti-diagonal mix\n    diag_pts = []\n    for i in range(n):\n        t = i / (n - 1)\n        if i % 2 == 0:\n            diag_pts.append((t * 0.84 + 0.08, t * 0.84 + 0.08))\n        else:\n            diag_pts.append(((1 - t) * 0.84 + 0.08, t * 0.84 + 0.08))\n    diag_pts = np.array(diag_pts)[:n]\n    diag_pts = np.clip(diag_pts + (np_rng.rand(n, 2) - 0.5) * 0.02, 0.02, 0.98)\n    seeds.append(diag_pts)\n\n    # 6) random low-discrepancy like but deterministic\n    rnd = np_rng.rand(n, 2) * 0.9 + 0.05\n    rnd = np.clip(rnd, 0.02, 0.98)\n    seeds.append(rnd)\n\n    # Limit number of restarts to between 6 and 10; we have 6 seeds above\n    seeds = seeds[:8] if len(seeds) >= 8 else seeds\n\n    # Anneal schedules: list of (w_over_start, w_b_start, w_over_end, w_b_end)\n    schedules = [\n        (50.0, 50.0, 300.0, 300.0),\n        (100.0, 50.0, 600.0, 400.0)\n    ]\n\n    best_centers = None\n    best_radii = None\n    best_sum = -1.0\n\n    # For each seed and schedule perform annealed L-BFGS-B\n    for seed_pts in seeds:\n        # initialize z\n        x0 = seed_pts[:, 0].copy()\n        y0 = seed_pts[:, 1].copy()\n        # initial radii small: half minimal distance to boundary and neighbor approx\n        dx0 = x0 - 0.5\n        dy0 = y0 - 0.5\n        r0 = np.minimum.reduce([x0, y0, 1.0 - x0, 1.0 - y0]) * 0.5\n        r0 = np.clip(r0, 0.01, 0.06)\n        z0 = np.concatenate([x0, y0, r0])\n\n        for (ws, bs, we, be) in schedules:\n            # create 3 anneal stages: start, mid, end\n            for alpha in [0.0, 0.5, 1.0]:\n                w_over = ws + (we - ws) * alpha\n                w_b = bs + (be - bs) * alpha\n\n                def fun_wr(z):\n                    f, g = objective_and_grad(z, w_over, w_b)\n                    return f, g\n\n                res = minimize(fun_wr, z0, method='L-BFGS-B',\n                               jac=True, bounds=bounds,\n                               options={'maxiter': 200, 'ftol': 1e-9})\n                if res.x is None:\n                    z0 = z0  # keep previous\n                else:\n                    z0 = res.x.copy()\n\n        # After annealing, extract centers and run LP to maximize radii for these centers\n        x_final, y_final, r_final = unpack(z0)\n        centers = np.vstack([x_final, y_final]).T\n\n        # Build LP matrices\n        c = -np.ones(n)\n        A_ub_rows = []\n        b_ub = []\n\n        # boundary constraints: r_i <= x_i, y_i, 1-x_i, 1-y_i\n        for i in range(n):\n            row = np.zeros(n)\n            row[i] = 1.0\n            A_ub_rows.append(row.copy()); b_ub.append(centers[i, 0])  # r_i <= x_i\n            A_ub_rows.append(row.copy()); b_ub.append(centers[i, 1])  # r_i <= y_i\n            A_ub_rows.append(row.copy()); b_ub.append(1.0 - centers[i, 0])  # r_i <= 1-x_i\n            A_ub_rows.append(row.copy()); b_ub.append(1.0 - centers[i, 1])  # r_i <= 1-y_i\n\n        # pairwise constraints\n        dxp = centers[:, 0][:, None] - centers[:, 0][None, :]\n        dyp = centers[:, 1][:, None] - centers[:, 1][None, :]\n        dp = np.sqrt(dxp * dxp + dyp * dyp)\n        for i in range(n):\n            for j in range(i + 1, n):\n                row = np.zeros(n)\n                row[i] = 1.0\n                row[j] = 1.0\n                A_ub_rows.append(row)\n                b_ub.append(dp[i, j] - pair_eps_lp)\n\n        A_ub = np.array(A_ub_rows)\n        b_ub = np.array(b_ub)\n\n        # Solve LP\n        lp_res = linprog(c, A_ub=A_ub, b_ub=b_ub, bounds=[(0, None)] * n, method='highs')\n\n        if lp_res.success:\n            r_lp = lp_res.x\n        else:\n            # Fallback: produce conservative radii ensuring strict feasibility by shrinkage\n            # Set radii to minimal distance to boundaries and half distances to neighbors minus eps\n            to_boundary = np.minimum.reduce([centers[:, 0], centers[:, 1], 1.0 - centers[:, 0], 1.0 - centers[:, 1]])\n            min_pair = np.full(n, 1.0)\n            for i in range(n):\n                dij = dp[i, :]\n                dij[i] = np.inf\n                min_pair[i] = np.min(dij)\n            r_guess = np.minimum(to_boundary, 0.5 * min_pair) - 2e-6\n            r_lp = np.maximum(r_guess, 0.0)\n\n        # Enforce strictness for boundaries: r_i <= x_i - 1e-6 etc\n        r_lp = np.minimum(r_lp, centers[:, 0] - 1e-6)\n        r_lp = np.minimum(r_lp, centers[:, 1] - 1e-6)\n        r_lp = np.minimum(r_lp, 1.0 - centers[:, 0] - 1e-6)\n        r_lp = np.minimum(r_lp, 1.0 - centers[:, 1] - 1e-6)\n        r_lp = np.maximum(r_lp, 0.0)\n\n        # Ensure pairwise strictness: if any pair violates, shrink both proportionality\n        dxp = centers[:, 0][:, None] - centers[:, 0][None, :]\n        dyp = centers[:, 1][:, None] - centers[:, 1][None, :]\n        dp = np.sqrt(dxp * dxp + dyp * dyp)\n        sumr = r_lp[:, None] + r_lp[None, :]\n        violate = (sumr > dp - pair_eps_lp)\n        if np.any(violate):\n            # simple iterative shrink: for violating pairs reduce radii slightly\n            r_work = r_lp.copy()\n            for _ in range(10):\n                dxp = centers[:, 0][:, None] - centers[:, 0][None, :]\n                dyp = centers[:, 1][:, None] - centers[:, 1][None, :]\n                dp = np.sqrt(dxp * dxp + dyp * dyp)\n                sumr = r_work[:, None] + r_work[None, :]\n                viol = (sumr > dp - pair_eps_lp)\n                if not np.any(viol):\n                    break\n                # for each i compute maximum allowed sum with others\n                max_allow = dp - pair_eps_lp\n                # For each i, compute maximal r_i allowed considering others fixed: r_i <= min_j (max_allow_ij - r_j)\n                limits = max_allow - r_work[None, :]\n                limits[np.eye(n, dtype=bool)] = np.inf\n                new_r = np.minimum(r_work, np.min(limits, axis=1))\n                # also respect boundaries\n                new_r = np.minimum(new_r, centers[:, 0] - 1e-6)\n                new_r = np.minimum(new_r, centers[:, 1] - 1e-6)\n                new_r = np.minimum(new_r, 1.0 - centers[:, 0] - 1e-6)\n                new_r = np.minimum(new_r, 1.0 - centers[:, 1] - 1e-6)\n                # ensure non-negative\n                new_r = np.maximum(new_r, 0.0)\n                # if no change break\n                if np.allclose(new_r, r_work, atol=1e-12):\n                    break\n                r_work = new_r\n            r_lp = r_work\n\n        # final strict checks; if still violates, make extremely small radii\n        dxp = centers[:, 0][:, None] - centers[:, 0][None, :]\n        dyp = centers[:, 1][:, None] - centers[:, 1][None, :]\n        dp = np.sqrt(dxp * dxp + dyp * dyp)\n        sumr = r_lp[:, None] + r_lp[None, :]\n        if np.any(sumr > dp - pair_eps_lp + 1e-12):\n            # as last resort, set all radii to minimal safe value based on nearest neighbor and boundary\n            to_boundary = np.minimum.reduce([centers[:, 0], centers[:, 1], 1.0 - centers[:, 0], 1.0 - centers[:, 1]])\n            min_pair = np.full(n, 1.0)\n            for i in range(n):\n                dij = dp[i, :].copy()\n                dij[i] = np.inf\n                min_pair[i] = np.min(dij)\n            safe_r = np.minimum(to_boundary, 0.5 * min_pair) - 2e-6\n            r_lp = np.maximum(safe_r, 0.0)\n\n        sum_r = float(np.sum(r_lp))\n\n        # Validate strict feasibility assertions (tolerance 1e-6)\n        # radii non-negative\n        assert np.all(r_lp >= -1e-12)\n        # inside boundaries strictly\n        assert np.all(centers[:, 0] - r_lp >= 1e-6 - 1e-12)\n        assert np.all(centers[:, 1] - r_lp >= 1e-6 - 1e-12)\n        assert np.all(1.0 - centers[:, 0] - r_lp >= 1e-6 - 1e-12)\n        assert np.all(1.0 - centers[:, 1] - r_lp >= 1e-6 - 1e-12)\n        # pairwise non-overlap strictly\n        dxp = centers[:, 0][:, None] - centers[:, 0][None, :]\n        dyp = centers[:, 1][:, None] - centers[:, 1][None, :]\n        dp = np.sqrt(dxp * dxp + dyp * dyp)\n        sumr = r_lp[:, None] + r_lp[None, :]\n        for i in range(n):\n            for j in range(i + 1, n):\n                assert sumr[i, j] <= dp[i, j] - pair_eps_lp + 1e-12\n\n        if sum_r > best_sum + 1e-12:\n            best_sum = sum_r\n            best_centers = centers.copy()\n            best_radii = r_lp.copy()\n\n    # Final return\n    centers_out = np.asarray(best_centers)\n    radii_out = np.asarray(best_radii)\n    sum_out = float(np.sum(radii_out))\n    return centers_out, radii_out, sum_out\n\nif __name__ == \"__main__\":\n    c, r, s = construct_packing()",
  "reasoning": "import numpy as np\nfrom scipy.optimize import minimize, linprog\n\ndef construct_packing():\n    np_rng = np.random.RandomState(123456)\n\n    n = 26\n    eps_div = 1e-8\n    pair_eps_lp = 1e-6\n\n    # Bounds for variables: x,y in [0,1], r in [0,0.6]\n    bounds = [(0.0, 1.0)] * (2 * n) + [(0.0, 0.6)] * n\n\n    def unpack(z):\n        x = z[:n].copy()\n        y = z[n:2 * n].copy()\n        r = z[2 * n:].copy()\n        return x, y, r\n\n    def pairwise_d(x, y):\n        dx = x[:, None] - x[None, :]\n        dy = y[:, None] - y[None, :]\n        d = np.sqrt(dx * dx + dy * dy)\n        return dx, dy, d\n\n    def objective_and_grad(z, w_over, w_b):\n        x, y, r = unpack(z)\n\n        dx = x[:, None] - x[None, :]\n        dy = y[:, None] - y[None, :]\n        d = np.sqrt(dx * dx + dy * dy)\n\n        # Prevent self-interaction\n        diag_idx = np.arange(n)\n        d[diag_idx, diag_idx] = 1.0  # safe for division\n        s_pair = (r[:, None] + r[None, :]) - d\n        mask = s_pair > 0\n        mask[diag_idx, diag_idx] = False\n        s_pair_masked = s_pair * mask\n\n        # Overlap penalty\n        over_pen = 0.5 * w_over * np.sum(s_pair_masked * s_pair_masked)\n\n        # Boundary hinges\n        s_xlo = r - x\n        s_ylo = r - y\n        s_xhi = r - (1.0 - x)\n        s_yhi = r - (1.0 - y)\n        s_bnd = np.vstack([s_xlo, s_ylo, s_xhi, s_yhi]).T\n        mask_bnd = s_bnd > 0\n        bnd_pen = 0.5 * w_b * np.sum((s_bnd * mask_bnd) ** 2)\n\n        # Small regularizer (very small)\n        reg = 1e-8 * (np.sum((x - 0.5) ** 2) + np.sum((y - 0.5) ** 2))\n\n        f = -np.sum(r) + over_pen + bnd_pen + reg\n\n        # Gradients\n        grad_x = np.zeros_like(x)\n        grad_y = np.zeros_like(y)\n        grad_r = np.zeros_like(r)\n\n        # grad_r: -1 + w_over * sum_j s_ij(masked) + w_b * sum_b s_bi(masked)\n        grad_r[:] = -1.0 + w_over * np.sum(s_pair_masked, axis=1) + w_b * np.sum(s_bnd * mask_bnd, axis=1)\n\n        # grad_x and grad_y from pair penalties\n        denom = d + eps_div\n        factor = w_over * s_pair_masked / denom  # shape n x n\n        # grad_x_i contribution: - sum_j factor_ij * dx_ij\n        grad_x += -np.sum(factor * dx, axis=1)\n        grad_y += -np.sum(factor * dy, axis=1)\n\n        # grad_x and grad_y from boundary penalties\n        # s_xlo = r - x -> derivative wrt x is -1 when active\n        grad_x += w_b * (-s_xlo * (s_xlo > 0) + s_xhi * (s_xhi > 0))\n        grad_y += w_b * (-s_ylo * (s_ylo > 0) + s_yhi * (s_yhi > 0))\n\n        # reg grads\n        grad_x += 2e-8 * (x - 0.5)\n        grad_y += 2e-8 * (y - 0.5)\n\n        grad = np.concatenate([grad_x, grad_y, grad_r])\n        return f, grad\n\n    # Seeds generation: deterministic diverse seeds\n    seeds = []\n\n    # 1) Hex-like packing with row lengths summing to 26: [6,5,6,5,4]\n    rows = [6, 5, 6, 5, 4]\n    y_positions = np.linspace(0.1, 0.9, len(rows))\n    hex_pts = []\n    for yi, cnt in zip(y_positions, rows):\n        if cnt % 2 == 0:\n            xs = np.linspace(0.1, 0.9, cnt)\n        else:\n            xs = np.linspace(0.1, 0.9, cnt)\n        # stagger alternate rows\n        hex_pts.extend([(x, yi) for x in xs])\n    hex_pts = np.array(hex_pts)[:n]\n    jitter = (np_rng.rand(n, 2) - 0.5) * 0.01\n    hex_pts = np.clip(hex_pts + jitter, 0.02, 0.98)\n    seeds.append(hex_pts)\n\n    # 2) Edge-biased: distribute around perimeter and a few interior\n    edge_pts = []\n    m_edge = 20\n    t = np.linspace(0, 1, m_edge, endpoint=False)\n    for tt in t[:7]:\n        edge_pts.append((tt * 0.95 + 0.025, 0.02))\n    for tt in t[:7]:\n        edge_pts.append((tt * 0.95 + 0.025, 0.98))\n    for tt in t[:6]:\n        edge_pts.append((0.02, tt * 0.95 + 0.025))\n    for tt in t[:6]:\n        edge_pts.append((0.98, tt * 0.95 + 0.025))\n    edge_pts = np.array(edge_pts)[:n]\n    jitter = (np_rng.rand(n, 2) - 0.5) * 0.02\n    edge_pts = np.clip(edge_pts + jitter, 0.02, 0.98)\n    seeds.append(edge_pts)\n\n    # 3) Three clusters\n    clusters = []\n    centers = np.array([[0.25, 0.25], [0.75, 0.25], [0.5, 0.75]])\n    counts = [9, 9, 8]\n    idx = 0\n    for c, cnt in zip(centers, counts):\n        for _ in range(cnt):\n            ang = np_rng.rand() * 2 * np.pi\n            rad = 0.05 + np_rng.rand() * 0.03\n            pt = c + np.array([np.cos(ang), np.sin(ang)]) * rad\n            clusters.append(pt)\n            idx += 1\n    clusters = np.array(clusters)[:n]\n    clusters = np.clip(clusters, 0.02, 0.98)\n    seeds.append(clusters)\n\n    # 4) Grid-ish with slight offset\n    gx = np.linspace(0.08, 0.92, 6)\n    gy = np.linspace(0.08, 0.92, 5)\n    grid_pts = []\n    for xi in gx:\n        for yi in gy:\n            grid_pts.append((xi, yi))\n    grid_pts = np.array(grid_pts)[:n]\n    jitter = (np_rng.rand(n, 2) - 0.5) * 0.015\n    grid_pts = np.clip(grid_pts + jitter, 0.02, 0.98)\n    seeds.append(grid_pts)\n\n    # 5) Diagonal + anti-diagonal mix\n    diag_pts = []\n    for i in range(n):\n        t = i / (n - 1)\n        if i % 2 == 0:\n            diag_pts.append((t * 0.84 + 0.08, t * 0.84 + 0.08))\n        else:\n            diag_pts.append(((1 - t) * 0.84 + 0.08, t * 0.84 + 0.08))\n    diag_pts = np.array(diag_pts)[:n]\n    diag_pts = np.clip(diag_pts + (np_rng.rand(n, 2) - 0.5) * 0.02, 0.02, 0.98)\n    seeds.append(diag_pts)\n\n    # 6) random low-discrepancy like but deterministic\n    rnd = np_rng.rand(n, 2) * 0.9 + 0.05\n    rnd = np.clip(rnd, 0.02, 0.98)\n    seeds.append(rnd)\n\n    # Limit number of restarts to between 6 and 10; we have 6 seeds above\n    seeds = seeds[:8] if len(seeds) >= 8 else seeds\n\n    # Anneal schedules: list of (w_over_start, w_b_start, w_over_end, w_b_end)\n    schedules = [\n        (50.0, 50.0, 300.0, 300.0),\n        (100.0, 50.0, 600.0, 400.0)\n    ]\n\n    best_centers = None\n    best_radii = None\n    best_sum = -1.0\n\n    # For each seed and schedule perform annealed L-BFGS-B\n    for seed_pts in seeds:\n        # initialize z\n        x0 = seed_pts[:, 0].copy()\n        y0 = seed_pts[:, 1].copy()\n        # initial radii small: half minimal distance to boundary and neighbor approx\n        dx0 = x0 - 0.5\n        dy0 = y0 - 0.5\n        r0 = np.minimum.reduce([x0, y0, 1.0 - x0, 1.0 - y0]) * 0.5\n        r0 = np.clip(r0, 0.01, 0.06)\n        z0 = np.concatenate([x0, y0, r0])\n\n        for (ws, bs, we, be) in schedules:\n            # create 3 anneal stages: start, mid, end\n            for alpha in [0.0, 0.5, 1.0]:\n                w_over = ws + (we - ws) * alpha\n                w_b = bs + (be - bs) * alpha\n\n                def fun_wr(z):\n                    f, g = objective_and_grad(z, w_over, w_b)\n                    return f, g\n\n                res = minimize(fun_wr, z0, method='L-BFGS-B',\n                               jac=True, bounds=bounds,\n                               options={'maxiter': 200, 'ftol': 1e-9})\n                if res.x is None:\n                    z0 = z0  # keep previous\n                else:\n                    z0 = res.x.copy()\n\n        # After annealing, extract centers and run LP to maximize radii for these centers\n        x_final, y_final, r_final = unpack(z0)\n        centers = np.vstack([x_final, y_final]).T\n\n        # Build LP matrices\n        c = -np.ones(n)\n        A_ub_rows = []\n        b_ub = []\n\n        # boundary constraints: r_i <= x_i, y_i, 1-x_i, 1-y_i\n        for i in range(n):\n            row = np.zeros(n)\n            row[i] = 1.0\n            A_ub_rows.append(row.copy()); b_ub.append(centers[i, 0])  # r_i <= x_i\n            A_ub_rows.append(row.copy()); b_ub.append(centers[i, 1])  # r_i <= y_i\n            A_ub_rows.append(row.copy()); b_ub.append(1.0 - centers[i, 0])  # r_i <= 1-x_i\n            A_ub_rows.append(row.copy()); b_ub.append(1.0 - centers[i, 1])  # r_i <= 1-y_i\n\n        # pairwise constraints\n        dxp = centers[:, 0][:, None] - centers[:, 0][None, :]\n        dyp = centers[:, 1][:, None] - centers[:, 1][None, :]\n        dp = np.sqrt(dxp * dxp + dyp * dyp)\n        for i in range(n):\n            for j in range(i + 1, n):\n                row = np.zeros(n)\n                row[i] = 1.0\n                row[j] = 1.0\n                A_ub_rows.append(row)\n                b_ub.append(dp[i, j] - pair_eps_lp)\n\n        A_ub = np.array(A_ub_rows)\n        b_ub = np.array(b_ub)\n\n        # Solve LP\n        lp_res = linprog(c, A_ub=A_ub, b_ub=b_ub, bounds=[(0, None)] * n, method='highs')\n\n        if lp_res.success:\n            r_lp = lp_res.x\n        else:\n            # Fallback: produce conservative radii ensuring strict feasibility by shrinkage\n            # Set radii to minimal distance to boundaries and half distances to neighbors minus eps\n            to_boundary = np.minimum.reduce([centers[:, 0], centers[:, 1], 1.0 - centers[:, 0], 1.0 - centers[:, 1]])\n            min_pair = np.full(n, 1.0)\n            for i in range(n):\n                dij = dp[i, :]\n                dij[i] = np.inf\n                min_pair[i] = np.min(dij)\n            r_guess = np.minimum(to_boundary, 0.5 * min_pair) - 2e-6\n            r_lp = np.maximum(r_guess, 0.0)\n\n        # Enforce strictness for boundaries: r_i <= x_i - 1e-6 etc\n        r_lp = np.minimum(r_lp, centers[:, 0] - 1e-6)\n        r_lp = np.minimum(r_lp, centers[:, 1] - 1e-6)\n        r_lp = np.minimum(r_lp, 1.0 - centers[:, 0] - 1e-6)\n        r_lp = np.minimum(r_lp, 1.0 - centers[:, 1] - 1e-6)\n        r_lp = np.maximum(r_lp, 0.0)\n\n        # Ensure pairwise strictness: if any pair violates, shrink both proportionality\n        dxp = centers[:, 0][:, None] - centers[:, 0][None, :]\n        dyp = centers[:, 1][:, None] - centers[:, 1][None, :]\n        dp = np.sqrt(dxp * dxp + dyp * dyp)\n        sumr = r_lp[:, None] + r_lp[None, :]\n        violate = (sumr > dp - pair_eps_lp)\n        if np.any(violate):\n            # simple iterative shrink: for violating pairs reduce radii slightly\n            r_work = r_lp.copy()\n            for _ in range(10):\n                dxp = centers[:, 0][:, None] - centers[:, 0][None, :]\n                dyp = centers[:, 1][:, None] - centers[:, 1][None, :]\n                dp = np.sqrt(dxp * dxp + dyp * dyp)\n                sumr = r_work[:, None] + r_work[None, :]\n                viol = (sumr > dp - pair_eps_lp)\n                if not np.any(viol):\n                    break\n                # for each i compute maximum allowed sum with others\n                max_allow = dp - pair_eps_lp\n                # For each i, compute maximal r_i allowed considering others fixed: r_i <= min_j (max_allow_ij - r_j)\n                limits = max_allow - r_work[None, :]\n                limits[np.eye(n, dtype=bool)] = np.inf\n                new_r = np.minimum(r_work, np.min(limits, axis=1))\n                # also respect boundaries\n                new_r = np.minimum(new_r, centers[:, 0] - 1e-6)\n                new_r = np.minimum(new_r, centers[:, 1] - 1e-6)\n                new_r = np.minimum(new_r, 1.0 - centers[:, 0] - 1e-6)\n                new_r = np.minimum(new_r, 1.0 - centers[:, 1] - 1e-6)\n                # ensure non-negative\n                new_r = np.maximum(new_r, 0.0)\n                # if no change break\n                if np.allclose(new_r, r_work, atol=1e-12):\n                    break\n                r_work = new_r\n            r_lp = r_work\n\n        # final strict checks; if still violates, make extremely small radii\n        dxp = centers[:, 0][:, None] - centers[:, 0][None, :]\n        dyp = centers[:, 1][:, None] - centers[:, 1][None, :]\n        dp = np.sqrt(dxp * dxp + dyp * dyp)\n        sumr = r_lp[:, None] + r_lp[None, :]\n        if np.any(sumr > dp - pair_eps_lp + 1e-12):\n            # as last resort, set all radii to minimal safe value based on nearest neighbor and boundary\n            to_boundary = np.minimum.reduce([centers[:, 0], centers[:, 1], 1.0 - centers[:, 0], 1.0 - centers[:, 1]])\n            min_pair = np.full(n, 1.0)\n            for i in range(n):\n                dij = dp[i, :].copy()\n                dij[i] = np.inf\n                min_pair[i] = np.min(dij)\n            safe_r = np.minimum(to_boundary, 0.5 * min_pair) - 2e-6\n            r_lp = np.maximum(safe_r, 0.0)\n\n        sum_r = float(np.sum(r_lp))\n\n        # Validate strict feasibility assertions (tolerance 1e-6)\n        # radii non-negative\n        assert np.all(r_lp >= -1e-12)\n        # inside boundaries strictly\n        assert np.all(centers[:, 0] - r_lp >= 1e-6 - 1e-12)\n        assert np.all(centers[:, 1] - r_lp >= 1e-6 - 1e-12)\n        assert np.all(1.0 - centers[:, 0] - r_lp >= 1e-6 - 1e-12)\n        assert np.all(1.0 - centers[:, 1] - r_lp >= 1e-6 - 1e-12)\n        # pairwise non-overlap strictly\n        dxp = centers[:, 0][:, None] - centers[:, 0][None, :]\n        dyp = centers[:, 1][:, None] - centers[:, 1][None, :]\n        dp = np.sqrt(dxp * dxp + dyp * dyp)\n        sumr = r_lp[:, None] + r_lp[None, :]\n        for i in range(n):\n            for j in range(i + 1, n):\n                assert sumr[i, j] <= dp[i, j] - pair_eps_lp + 1e-12\n\n        if sum_r > best_sum + 1e-12:\n            best_sum = sum_r\n            best_centers = centers.copy()\n            best_radii = r_lp.copy()\n\n    # Final return\n    centers_out = np.asarray(best_centers)\n    radii_out = np.asarray(best_radii)\n    sum_out = float(np.sum(radii_out))\n    return centers_out, radii_out, sum_out\n\nif __name__ == \"__main__\":\n    c, r, s = construct_packing()",
  "timestamp": "2026-02-11T01:36:01.750086",
  "cost_data": null,
  "model_config": {
    "model": "openai/gpt-5-mini",
    "temperature": 0.25
  },
  "llm_call_duration_s": 93.09808999905363,
  "eval_duration_s": 0.0
}