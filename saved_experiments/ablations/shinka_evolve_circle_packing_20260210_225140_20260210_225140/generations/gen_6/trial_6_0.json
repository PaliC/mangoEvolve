{
  "trial_id": "trial_6_0",
  "generation": 6,
  "parent_id": null,
  "code": "",
  "metrics": {},
  "prompt": "\nImplement penalized, annealed L-BFGS-B on (x,y,r) with analytic vectorized gradients, then final LP.\n- Objective: minimize P - sum(r), where P = w_over * overlap_hinge^2 + w_b * boundary_hinge^2\n- Anneal: run several restarts with increasing (w_over, w_b): e.g., [(50,50), (200,200), (1000,1000)]\n- Bounds: x,y in [0,1], r in [0,0.6]\n- Use 3+ seeds: hex grid, edge-biased, jittered uniform\n- After best penalized solution, run LP to maximize radii for fixed centers, return strictly feasible result\n- Fully vectorized gradients with eps only in divisions; no prints\n\nWrite a single Python file with NO prose/explanations, only code.\nRequirements:\n- Allowed modules: numpy, scipy\n- Provide entry function: construct_packing() -> (centers, radii, sum_radii)\n  * centers: np.ndarray shape (26,2), radii: np.ndarray shape (26,), sum_radii: float\n- All circles strictly inside [0,1]^2 and pairwise non-overlapping (tolerance 1e-6)\n- Return a strictly feasible solution; if your optimizer ends infeasible, post-process by LP to strict feasibility\n- Use runtime-conscious design (<=300s), no prints, deterministic seeds\n\nLP Feasibilization spec (maximize sum of radii for fixed centers):\n- Variables: r_i (i=1..26), objective: maximize sum(r_i)\n- Constraints:\n  - r_i >= 0\n  - r_i <= x_i\n  - r_i <= y_i\n  - r_i <= 1 - x_i\n  - r_i <= 1 - y_i\n  - r_i + r_j <= dist_ij - 1e-6 for all i<j\n- Implement with scipy.optimize.linprog (method='highs') by minimizing -sum(r_i)\n- Always call the LP at the end to ensure strict feasibility; do not rely solely on penalties\n\nNumerical/gradient guidance:\n- Use fully vectorized NumPy; no Python loops over pairs\n- For pairwise distances, use d = sqrt((dx)^2 + (dy)^2); when dividing by d, stabilize with denom = sqrt(d^2 + eps^2) or (d + eps). Prefer applying eps only in division (not in the penalty) to avoid bias.\n- Overlap penalty (for i<j): hinge = max(0, r_i + r_j - d_ij); use 0.5 * w_over * hinge^2\n- Boundary penalty: for each i, sum hinges on [r_i - x_i, r_i - y_i, r_i - (1-x_i), r_i - (1-y_i)], squared with weight w_b\n- Optionally include a small smoothness/centering/edge-bias term but ensure it doesn't bias the final LP step\n- Final feasibility check (assertions) must pass with tolerance 1e-6\n\nSeeding:\n- Provide diverse deterministic seeds: triangular/hex grids, edge-biased seeds, center-biased clusters\n- Prefer a few high-quality restarts to many poor starts\n\nReturn strictly feasible packing with the highest possible sum of radii.\n\n\nImprove on the strongest reference if helpful:\n# Reference 1 (trial_id=trial_5_0, score=2.6357967398504405):\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.optimize import linprog\n\ndef _pairwise_distances(centers):\n    diff = centers[:, None, :] - centers[None, :, :]\n    d = np.sqrt(np.sum(diff**2, axis=2))\n    return d, diff[..., 0], diff[..., 1]\n\ndef _objective_and_grad(z, N, P, w_b=1.0, w_o=1.0):\n    # z = [x(0..N-1), y(0..N-1), r(0..N-1)]\n    x = z[:N]\n    y = z[N:2*N]\n    r = z[2*N:]\n    tiny = 1e-12\n\n    # Boundary penalties: hinges\n    s1 = r - x\n    s2 = r - y\n    s3 = r + x - 1.0\n    s4 = r + y - 1.0\n\n    h1 = np.maximum(0.0, s1)\n    h2 = np.maximum(0.0, s2)\n    h3 = np.maximum(0.0, s3)\n    h4 = np.maximum(0.0, s4)\n\n    pen_b = 0.5 * np.sum(h1*h1 + h2*h2 + h3*h3 + h4*h4)\n\n    # Overlap penalties: hinges for i<j\n    centers = np.stack([x, y], axis=1)\n    d, dx_mat, dy_mat = _pairwise_distances(centers)\n    iu = np.triu_indices(N, 1)\n    dab = d[iu]\n    dxab = dx_mat[iu]\n    dyab = dy_mat[iu]\n    d_safe = np.maximum(dab, tiny)\n    hij = r[iu[0]] + r[iu[1]] - dab\n    hpos = np.maximum(0.0, hij)\n    pen_o = 0.5 * np.sum(hpos*hpos)\n\n    # Objective\n    f = -np.sum(r) + P * (w_b * pen_b + w_o * pen_o)\n\n    # Gradients for boundary penalties\n    dr_b = h1 + h2 + h3 + h4\n    dx_b = -h1 + h3\n    dy_b = -h2 + h4\n\n    # Overlap gradients (accumulate with np.add.at)\n    dx_o = np.zeros(N, dtype=float)\n    dy_o = np.zeros(N, dtype=float)\n    dr_o = np.zeros(N, dtype=float)\n\n    # For centers:\n    # grad_x_a += -h * (dxab/d), grad_x_b += +h * (dxab/d)\n    # grad_y_a += -h * (dyab/d), grad_y_b += +h * (dyab/d)\n    coeff = hpos / d_safe\n    cx = coeff * dxab\n    cy = coeff * dyab\n    np.add.at(dx_o, iu[0], -cx)\n    np.add.at(dx_o, iu[1], +cx)\n    np.add.at(dy_o, iu[0], -cy)\n    np.add.at(dy_o, iu[1], +cy)\n    # For radii:\n    np.add.at(dr_o, iu[0], hpos)\n    np.add.at(dr_o, iu[1], hpos)\n\n    dx_total = P * (w_b * dx_b + w_o * dx_o)\n    dy_total = P * (w_b * dy_b + w_o * dy_o)\n    dr_total = -1.0 + P * (w_b * dr_b + w_o * dr_o)\n\n    grad = np.concatenate([dx_total, dy_total, dr_total])\n    return f, grad\n\ndef _objective_centers_only(z_xy, r_fixed, P, w_b=1.0, w_o=1.0):\n    # z_xy = [x(0..N-1), y(0..N-1)], radii fixed\n    N = r_fixed.shape[0]\n    x = z_xy[:N]\n    y = z_xy[N:2*N]\n    r = r_fixed\n    tiny = 1e-12\n\n    # Boundary hinges\n    s1 = r - x\n    s2 = r - y\n    s3 = r + x - 1.0\n    s4 = r + y - 1.0\n    h1 = np.maximum(0.0, s1)\n    h2 = np.maximum(0.0, s2)\n    h3 = np.maximum(0.0, s3)\n    h4 = np.maximum(0.0, s4)\n    pen_b = 0.5 * np.sum(h1*h1 + h2*h2 + h3*h3 + h4*h4)\n\n    # Overlap hinges\n    centers = np.stack([x, y], axis=1)\n    d, dx_mat, dy_mat = _pairwise_distances(centers)\n    iu = np.triu_indices(N, 1)\n    dab = d[iu]\n    dxab = dx_mat[iu]\n    dyab = dy_mat[iu]\n    d_safe = np.maximum(dab, tiny)\n    hij = r[iu[0]] + r[iu[1]] - dab\n    hpos = np.maximum(0.0, hij)\n    pen_o = 0.5 * np.sum(hpos*hpos)\n\n    f = P * (w_b * pen_b + w_o * pen_o)\n\n    # Gradients wrt x,y only\n    dx_b = -h1 + h3\n    dy_b = -h2 + h4\n\n    dx_o = np.zeros(N, dtype=float)\n    dy_o = np.zeros(N, dtype=float)\n    coeff = hpos / d_safe\n    cx = coeff * dxab\n    cy = coeff * dyab\n    np.add.at(dx_o, iu[0], -cx)\n    np.add.at(dx_o, iu[1], +cx)\n    np.add.at(dy_o, iu[0], -cy)\n    np.add.at(dy_o, iu[1], +cy)\n\n    dx_total = P * (w_b * dx_b + w_o * dx_o)\n    dy_total = P * (w_b * dy_b + w_o * dy_o)\n    g = np.concatenate([dx_total, dy_total])\n    return f, g\n\ndef _lp_feasibilize(centers, r_max=None, eps_pair=1e-6):\n    \"\"\"\n    Maximize sum(r) subject to:\n      r_i >= 0\n      r_i <= r_max[i] (if provided)\n      r_i <= x_i, r_i <= y_i, r_i <= 1-x_i, r_i <= 1-y_i\n      r_i + r_j <= dist(c_i,c_j) - eps_pair\n    \"\"\"\n    N = centers.shape[0]\n    x = centers[:, 0]\n    y = centers[:, 1]\n\n    # Build A_ub, b_ub\n    A_rows = []\n    b_vals = []\n\n    # Boundary constraints\n    for i in range(N):\n        e = np.zeros(N); e[i] = 1.0\n        A_rows.append(e.copy()); b_vals.append(x[i])\n        A_rows.append(e.copy()); b_vals.append(y[i])\n        A_rows.append(e.copy()); b_vals.append(1.0 - x[i])\n        A_rows.append(e.copy()); b_vals.append(1.0 - y[i])\n\n    # Pairwise constraints\n    d, _, _ = _pairwise_distances(centers)\n    for i in range(N):\n        for j in range(i+1, N):\n            e = np.zeros(N); e[i] = 1.0; e[j] = 1.0\n            A_rows.append(e); b_vals.append(d[i, j] - eps_pair)\n\n    # Optional upper bounds r_i <= r_max[i]\n    if r_max is not None:\n        for i in range(N):\n            e = np.zeros(N); e[i] = 1.0\n            A_rows.append(e); b_vals.append(r_max[i])\n\n    A_ub = np.array(A_rows)\n    b_ub = np.array(b_vals)\n\n    # Bounds on r: [0, 0.5]\n    bounds = [(0.0, 0.5) for _ in range(N)]\n    c = -np.ones(N)\n\n    res = linprog(c, A_ub=A_ub, b_ub=b_ub, bounds=bounds, method='highs')\n    if res.success:\n        return np.clip(res.x, 0.0, 0.5)\n\n    # Retry with eps_pair = 0 to get a feasible base, then scale to satisfy eps_pair\n    b_vals2 = []\n    A_rows2 = []\n    for i in range(N):\n        e = np.zeros(N); e[i] = 1.0\n        A_rows2.append(e.copy()); b_vals2.append(x[i])\n        A_rows2.append(e.copy()); b_vals2.append(y[i])\n        A_rows2.append(e.copy()); b_vals2.append(1.0 - x[i])\n        A_rows2.append(e.copy()); b_vals2.append(1.0 - y[i])\n\n    for i in range(N):\n        for j in range(i+1, N):\n            e = np.zeros(N); e[i] = 1.0; e[j] = 1.0\n            A_rows2.append(e); b_vals2.append(d[i, j])\n\n    if r_max is not None:\n        for i in range(N):\n            e = np.zeros(N); e[i] = 1.0\n            A_rows2.append(e); b_vals2.append(r_max[i])\n\n    A_ub2 = np.array(A_rows2)\n    b_ub2 = np.array(b_vals2)\n\n    res2 = linprog(c, A_ub=A_ub2, b_ub=b_ub2, bounds=bounds, method='highs')\n    if not res2.success:\n        return np.zeros(N)\n\n    r_base = np.clip(res2.x, 0.0, 0.5)\n    d, _, _ = _pairwise_distances(centers)\n    denom = r_base[:, None] + r_base[None, :]\n    with np.errstate(divide='ignore', invalid='ignore'):\n        ratio = np.where(denom > 0, (d - eps_pair) / denom, np.inf)\n    iu = np.triu_indices(N, 1)\n    if ratio[iu].size > 0:\n        s_pairs = np.min(ratio[iu])\n        if not np.isfinite(s_pairs):\n            s_pairs = 1.0\n        s = min(1.0, s_pairs)\n    else:\n        s = 1.0\n    s = max(0.0, s)\n    r = s * r_base\n    return np.clip(r, 0.0, 0.5)\n\ndef _verify(centers, radii, tol_pair=1e-6):\n    N = len(radii)\n    x = centers[:, 0]\n    y = centers[:, 1]\n    if centers.shape != (N, 2):\n        return False\n    if np.any(radii < -1e-12):\n        return False\n    if np.any(x - radii < -1e-9):\n        return False\n    if np.any(y - radii < -1e-9):\n        return False\n    if np.any(x + radii > 1.0 + 1e-9):\n        return False\n    if np.any(y + radii > 1.0 + 1e-9):\n        return False\n    d, _, _ = _pairwise_distances(centers)\n    iu = np.triu_indices(N, 1)\n    lhs = radii[:, None] + radii[None, :]\n    viol = lhs[iu] - d[iu] + tol_pair\n    if np.any(viol > 1e-12):\n        return False\n    return True\n\ndef _hex_initial_centers_from_rows(rows, margin=0.05, jitter_scale=0.03, seed=0):\n    # rows: list of counts per row, centered with hex offset when fewer than max\n    assert sum(rows) == 26\n    max_cols = max(rows)\n    nrows = len(rows)\n    dx = (1.0 - 2*margin) / (max_cols - 1) if max_cols > 1 else 0.0\n    dy = (1.0 - 2*margin) / (nrows - 1) if nrows > 1 else 0.0\n    centers = []\n    rng = np.random.default_rng(seed)\n    for ri, cols in enumerate(rows):\n        y = margin + ri * dy if nrows > 1 else 0.5\n        if cols == max_cols:\n            startx = margin\n        else:\n            startx = margin + 0.5 * dx\n        if cols == 1:\n            startx = 0.5\n        for k in range(cols):\n            x = startx + (k * dx if cols > 1 else 0.0)\n            centers.append([x, y])\n    centers = np.array(centers)\n    # Jitter and clip\n    jitter = rng.normal(scale=jitter_scale, size=centers.shape)\n    centers = centers + jitter\n    centers = np.clip(centers, 1e-3, 1 - 1e-3)\n    return centers\n\ndef _multi_start_inits(N):\n    # Various row patterns summing to 26\n    patterns = [\n        [6, 5, 6, 5, 4],\n        [6, 5, 5, 5, 5],\n        [5, 6, 5, 5, 5],\n        [5, 5, 6, 5, 5],\n        [6, 6, 5, 5, 4],\n        [5, 6, 6, 5, 4],\n        [6, 5, 6, 4, 5],\n    ]\n    params = [\n        (0.03, 0.02), (0.04, 0.03), (0.05, 0.03),\n        (0.06, 0.02), (0.07, 0.03), (0.05, 0.04),\n    ]\n    inits = []\n    seed = 0\n    for pat in patterns:\n        for (m, j) in params:\n            inits.append(_hex_initial_centers_from_rows(pat, margin=m, jitter_scale=j, seed=seed))\n            seed += 1\n            if len(inits) >= 16:  # Limit number of starts to keep runtime in check\n                return inits\n    return inits\n\ndef _center_only_relax(centers, r_fixed, P=2000.0, w_b=1.0, w_o=1.0, iters=80):\n    N = centers.shape[0]\n    z0 = np.concatenate([centers[:, 0], centers[:, 1]])\n    bounds_xy = [(0.0, 1.0)] * (2 * N)\n    res = minimize(\n        fun=lambda zz: _objective_centers_only(zz, r_fixed, P, w_b, w_o),\n        x0=z0,\n        method='L-BFGS-B',\n        jac=True,\n        bounds=bounds_xy,\n        options={'maxiter': iters, 'ftol': 1e-12, 'gtol': 1e-8, 'maxcor': 20}\n    )\n    z = res.x\n    centers_new = np.stack([z[:N], z[N:2*N]], axis=1)\n    return centers_new\n\ndef construct_packing():\n    N = 26\n    best = {'centers': None, 'radii': None, 'sum': -np.inf}\n\n    # Penalty schedule and bounds\n    P_schedule = [8.0, 80.0, 800.0, 3000.0, 10000.0]\n    maxiter_stage = [90, 100, 110, 120, 130]\n    bounds = [(0.0, 1.0)] * (2*N) + [(0.0, 0.5)] * N\n\n    # Weighting of penalties\n    w_b = 1.0\n    w_o = 1.0\n\n    # Initializations\n    inits = _multi_start_inits(N)\n    if not inits:\n        inits = [_hex_initial_centers_from_rows([6,5,6,5,4], margin=0.05, jitter_scale=0.03, seed=42)]\n\n    for init_idx, centers0 in enumerate(inits):\n        x0 = centers0[:, 0]\n        y0 = centers0[:, 1]\n        # Start with small radii within local boundary\n        base_bound = np.minimum.reduce([x0, y0, 1 - x0, 1 - y0])\n        r0 = 0.05 * base_bound\n        z = np.concatenate([x0, y0, r0])\n\n        # Annealing optimization with periodic LP tightening and center-only relaxation\n        for si, P in enumerate(P_schedule):\n            res = minimize(\n                fun=lambda zz: _objective_and_grad(zz, N, P, w_b, w_o),\n                x0=z,\n                method='L-BFGS-B',\n                jac=True,\n                bounds=bounds,\n                options={'maxiter': maxiter_stage[si], 'ftol': 1e-12, 'gtol': 1e-8, 'maxcor': 30}\n            )\n            z = res.x\n            centers_tmp = np.stack([z[:N], z[N:2*N]], axis=1)\n            r_cur = np.clip(z[2*N:], 0.0, 0.5)\n            # Conservative LP cap slightly above current radii\n            cap_factor = 1.08 if si < len(P_schedule) - 1 else 1.15\n            r_cap = np.minimum(0.5, cap_factor * np.maximum(1e-9, r_cur))\n            r_lp = _lp_feasibilize(centers_tmp, r_max=r_cap, eps_pair=1e-6)\n            # Small center-only relaxation to open space, then re-LP\n            centers_relaxed = _center_only_relax(centers_tmp, r_lp, P=max(1000.0, P), w_b=w_b, w_o=w_o, iters=70)\n            r_lp2 = _lp_feasibilize(centers_relaxed, r_max=None, eps_pair=1e-6)\n            z = np.concatenate([centers_relaxed[:, 0], centers_relaxed[:, 1], np.clip(r_lp2, 0.0, 0.5)])\n\n        # Final LP without cap to fully utilize geometry\n        centers_cur = np.stack([z[:N], z[N:2*N]], axis=1)\n        r_final = _lp_feasibilize(centers_cur, r_max=None, eps_pair=1e-6)\n\n        # Verify and update best\n        if _verify(centers_cur, r_final, tol_pair=1e-6):\n            s = float(np.sum(r_final))\n            if s > best['sum']:\n                best['centers'] = centers_cur.copy()\n                best['radii'] = r_final.copy()\n                best['sum'] = s\n        else:\n            # Try slight shrink and re-LP to fix tiny violations\n            d, _, _ = _pairwise_distances(centers_cur)\n            denom = r_final[:, None] + r_final[None, :]\n            with np.errstate(divide='ignore', invalid='ignore'):\n                ratio = np.where(denom > 0, (d - 1.0e-6) / denom, np.inf)\n            iu = np.triu_indices(N, 1)\n            s_pairs = np.min(ratio[iu]) if ratio[iu].size > 0 else 1.0\n            if not np.isfinite(s_pairs):\n                s_pairs = 1.0\n            s_pairs = max(0.0, min(1.0, s_pairs))\n            r_try = s_pairs * r_final\n            # Boundary scaling\n            bx = np.minimum.reduce([centers_cur[:,0], centers_cur[:,1], 1-centers_cur[:,0], 1-centers_cur[:,1]])\n            with np.errstate(divide='ignore', invalid='ignore'):\n                scale_b = np.min(np.where(r_try > 0, bx / r_try, 1.0))\n            scale_b = min(1.0, scale_b if np.isfinite(scale_b) else 1.0)\n            r_try = scale_b * r_try\n            if _verify(centers_cur, r_try, tol_pair=1e-6):\n                s = float(np.sum(r_try))\n                if s > best['sum']:\n                    best['centers'] = centers_cur.copy()\n                    best['radii'] = r_try.copy()\n                    best['sum'] = s\n\n    # Fallback if nothing verified\n    if best['centers'] is None:\n        centers_fb = _hex_initial_centers_from_rows([6,5,6,5,4], margin=0.06, jitter_scale=0.02, seed=123)\n        r_fb = _lp_feasibilize(centers_fb, r_max=None, eps_pair=1e-6)\n        if not _verify(centers_fb, r_fb, tol_pair=1e-6):\n            r_fb = 0.95 * r_fb\n        best['centers'] = centers_fb\n        best['radii'] = np.clip(r_fb, 0.0, 0.5)\n        best['sum'] = float(np.sum(best['radii']))\n\n    # Ensure strict feasibility and final verify\n    radii_strict = _lp_feasibilize(best['centers'], r_max=None, eps_pair=1.1e-6)\n    if _verify(best['centers'], radii_strict, tol_pair=1e-6):\n        radii = radii_strict\n    else:\n        radii = best['radii']\n\n    centers = best['centers']\n    # Final hardening: if any violation persists, scale radii conservatively\n    if not _verify(centers, radii, tol_pair=1e-6):\n        d, _, _ = _pairwise_distances(centers)\n        denom = radii[:, None] + radii[None, :]\n        with np.errstate(divide='ignore', invalid='ignore'):\n            ratio = np.where(denom > 0, (d - 1e-6) / denom, np.inf)\n        iu = np.triu_indices(N, 1)\n        s_pairs = np.min(ratio[iu]) if ratio[iu].size > 0 else 1.0\n        s_pairs = min(1.0, s_pairs if np.isfinite(s_pairs) else 1.0)\n        s_pairs = max(0.0, s_pairs)\n        radii = s_pairs * radii\n        bx = np.minimum.reduce([centers[:,0], centers[:,1], 1-centers[:,0], 1-centers[:,1]])\n        with np.errstate(divide='ignore', invalid='ignore'):\n            scale_b = np.min(np.where(radii > 0, bx / radii, 1.0))\n        scale_b = min(1.0, scale_b if np.isfinite(scale_b) else 1.0)\n        radii = scale_b * radii\n\n    radii = np.clip(radii, 0.0, 0.5)\n    sum_radii = float(np.sum(radii))\n    return centers.astype(float), radii.astype(float), sum_radii\n\ndef run_packing():\n    centers, radii, sum_radii = construct_packing()\n    return centers, radii, sum_radii\n\n# Reference 2 (trial_id=trial_5_1, score=2.6196158131359644):\nimport numpy as np\nfrom scipy.optimize import minimize, linprog\n\ndef _pairwise_distances(centers):\n    diff = centers[:, None, :] - centers[None, :, :]\n    d = np.sqrt(np.sum(diff**2, axis=2))\n    return d, diff[..., 0], diff[..., 1]\n\ndef _objective_and_grad(z, N, P):\n    x = z[:N]; y = z[N:2*N]; r = z[2*N:]\n    tiny = 1e-12\n    h1 = np.maximum(0.0, r - x)\n    h2 = np.maximum(0.0, r - y)\n    h3 = np.maximum(0.0, r + x - 1.0)\n    h4 = np.maximum(0.0, r + y - 1.0)\n    pen_b = np.sum(h1*h1 + h2*h2 + h3*h3 + h4*h4)\n    centers = np.stack([x, y], axis=1)\n    d, dx_mat, dy_mat = _pairwise_distances(centers)\n    d_safe = np.maximum(d, tiny)\n    rr = r[:, None] + r[None, :]\n    hinge = np.maximum(0.0, rr - d)\n    H = np.triu(hinge, 1)\n    pen_o = np.sum(H*H)\n    f = -np.sum(r) + P*(pen_b + pen_o)\n    dr_b = 2.0*(h1 + h2 + h3 + h4)\n    dx_b = -2.0*h1 + 2.0*h3\n    dy_b = -2.0*h2 + 2.0*h4\n    with np.errstate(divide='ignore', invalid='ignore'):\n        invd = 1.0/d_safe\n        fx = dx_mat*invd\n        fy = dy_mat*invd\n    Qx = H*fx; Qy = H*fy\n    dx_o = -2.0*np.sum(Qx, axis=1) + 2.0*np.sum(Qx, axis=0)\n    dy_o = -2.0*np.sum(Qy, axis=1) + 2.0*np.sum(Qy, axis=0)\n    dr_o = 2.0*(np.sum(H, axis=1) + np.sum(H, axis=0))\n    dx_total = P*dx_b + P*dx_o\n    dy_total = P*dy_b + P*dy_o\n    dr_total = -1.0 + P*dr_b + P*dr_o\n    grad = np.concatenate([dx_total, dy_total, dr_total])\n    return f, grad\n\ndef _lp_feasibilize(centers, r_max=None, eps_pair=1e-6):\n    N = centers.shape[0]\n    x = centers[:, 0].astype(float); y = centers[:, 1].astype(float)\n    A_rows = []; b_vals = []\n    for i in range(N):\n        e = np.zeros(N, dtype=float); e[i] = 1.0\n        A_rows += [e.copy(), e.copy(), e.copy(), e.copy()]\n        b_vals += [x[i], y[i], 1.0-x[i], 1.0-y[i]]\n    d, _, _ = _pairwise_distances(centers)\n    for i in range(N):\n        for j in range(i+1, N):\n            e = np.zeros(N, dtype=float); e[i]=1.0; e[j]=1.0\n            A_rows.append(e); b_vals.append(d[i,j]-eps_pair)\n    if r_max is not None:\n        for i in range(N):\n            e = np.zeros(N, dtype=float); e[i]=1.0\n            A_rows.append(e); b_vals.append(float(r_max[i]))\n    A_ub = np.array(A_rows, dtype=float) if A_rows else None\n    b_ub = np.array(b_vals, dtype=float) if b_vals else None\n    bounds = [(0.0, 0.6) for _ in range(N)]\n    c = -np.ones(N, dtype=float)\n    res = linprog(c, A_ub=A_ub, b_ub=b_ub, bounds=bounds, method='highs')\n    if res.success:\n        return res.x\n    # Retry without eps\n    A_rows2 = []; b_vals2 = []\n    for i in range(N):\n        e = np.zeros(N, dtype=float); e[i]=1.0\n        A_rows2 += [e.copy(), e.copy(), e.copy(), e.copy()]\n        b_vals2 += [x[i], y[i], 1.0-x[i], 1.0-y[i]]\n    for i in range(N):\n        for j in range(i+1, N):\n            e = np.zeros(N, dtype=float); e[i]=1.0; e[j]=1.0\n            A_rows2.append(e); b_vals2.append(d[i,j])\n    if r_max is not None:\n        for i in range(N):\n            e = np.zeros(N, dtype=float); e[i]=1.0\n            A_rows2.append(e); b_vals2.append(float(r_max[i]))\n    A_ub2 = np.array(A_rows2, dtype=float)\n    b_ub2 = np.array(b_vals2, dtype=float)\n    res2 = linprog(c, A_ub=A_ub2, b_ub=b_ub2, bounds=bounds, method='highs')\n    if not res2.success:\n        r_try = np.minimum.reduce([x,y,1-x,1-y])*0.1\n        denom = r_try[:,None]+r_try[None,:]\n        with np.errstate(divide='ignore', invalid='ignore'):\n            ratio = np.where(denom>0,(d-eps_pair)/denom,np.inf)\n        iu = np.triu_indices(N,1)\n        s_pairs = np.min(ratio[iu]) if ratio[iu].size>0 else 1.0\n        s_pairs = max(0.0, min(1.0, s_pairs))\n        return np.clip(s_pairs*r_try,0.0,0.6)\n    r_base = res2.x\n    denom = r_base[:,None]+r_base[None,:]\n    with np.errstate(divide='ignore', invalid='ignore'):\n        ratio = np.where(denom>0,(d-eps_pair)/denom,np.inf)\n    iu = np.triu_indices(N,1)\n    s_pairs = np.min(ratio[iu]) if ratio[iu].size>0 else 1.0\n    s = max(0.0, min(1.0, s_pairs))\n    return s*r_base\n\ndef _verify(centers, radii, tol_pair=1e-6):\n    N = len(radii)\n    x = centers[:,0]; y = centers[:,1]\n    if np.any(radii < -1e-12): return False\n    if np.any(x-radii < -1e-9): return False\n    if np.any(y-radii < -1e-9): return False\n    if np.any(x+radii > 1.0+1e-9): return False\n    if np.any(y+radii > 1.0+1e-9): return False\n    d, _, _ = _pairwise_distances(centers)\n    iu = np.triu_indices(N,1)\n    lhs = radii[:,None] + radii[None,:]\n    viol = lhs[iu] - d[iu] + tol_pair\n    if np.any(viol > 1e-12): return False\n    return True\n\ndef _hex_initial_centers(N, margin=0.03, seed=0):\n    rows = [6,5,6,5,4]; assert sum(rows)==N\n    max_cols = max(rows); nrows=len(rows)\n    dx = (1.0-2*margin)/(max_cols-1) if max_cols>1 else 1.0\n    dy = (1.0-2*margin)/(nrows-1) if nrows>1 else 1.0\n    centers=[]; rng=np.random.default_rng(seed)\n    for ri,cols in enumerate(rows):\n        y=margin+ri*dy\n        startx=margin if cols==max_cols else margin+0.5*dx\n        for k in range(cols):\n            centers.append([startx+k*dx, y])\n    centers=np.array(centers)\n    jitter_scale = min(dx,dy)*0.12 if nrows>1 else 0.02\n    centers = np.clip(centers + rng.normal(scale=jitter_scale,size=centers.shape),1e-5,1-1e-5)\n    return centers\n\ndef _lhs_initial_centers(N, seed=0):\n    rng = np.random.default_rng(seed)\n    xs = (np.arange(N)+rng.uniform(size=N))/N\n    ys = (np.arange(N)+rng.uniform(size=N))/N\n    rng.shuffle(xs); rng.shuffle(ys)\n    centers = np.vstack([xs, ys]).T\n    return np.clip(centers,0.02,0.98)\n\ndef construct_packing():\n    N=26\n    best={'centers':None,'radii':None,'sum':-np.inf}\n    P_schedule=[20.0,200.0,1500.0]; maxiter_stage=300\n    bounds=[(0.0,1.0)]*(2*N)+[(0.0,0.6)]*N\n    inits=[]\n    inits.append(_hex_initial_centers(N,margin=0.03,seed=0))\n    inits.append(_hex_initial_centers(N,margin=0.06,seed=1))\n    inits.append(_hex_initial_centers(N,margin=0.08,seed=2))\n    for s in range(9):\n        if s%2==0:\n            inits.append(_lhs_initial_centers(N,seed=10+s))\n        else:\n            rng=np.random.default_rng(20+s)\n            inits.append(rng.uniform(0.06,0.94,size=(N,2)))\n    inits=inits[:12]\n    for centers0 in inits:\n        x0=centers0[:,0]; y0=centers0[:,1]\n        r0=np.minimum.reduce([x0,y0,1-x0,1-y0])*0.15\n        z=np.concatenate([x0,y0,r0])\n        for P in P_schedule:\n            res = minimize(lambda zz: _objective_and_grad(zz,N,P), x0=z,\n                           method='L-BFGS-B', jac=True, bounds=bounds,\n                           options={'maxiter':maxiter_stage,'ftol':1e-12,'gtol':1e-8,'maxcor':20})\n            z=res.x\n            centers_tmp=np.stack([z[:N],z[N:2*N]],axis=1)\n            r_current=np.clip(z[2*N:],0.0,0.6)\n            r_lp=_lp_feasibilize(centers_tmp,r_max=r_current,eps_pair=1e-6)\n            z[2*N:]=np.clip(r_lp,0.0,0.6)\n        centers_cur=np.stack([z[:N],z[N:2*N]],axis=1)\n        r_final=_lp_feasibilize(centers_cur,r_max=None,eps_pair=1e-6)\n        if not _verify(centers_cur,r_final,tol_pair=1e-6):\n            r_try=np.clip(r_final,0.0,0.6)\n            if np.sum(r_try)<=0:\n                r_try=np.clip(z[2*N:]*0.95,0.0,0.6)\n            r_try2=_lp_feasibilize(centers_cur,r_max=r_try,eps_pair=1e-6)\n            if _verify(centers_cur,r_try2,tol_pair=1e-6):\n                r_final=r_try2\n            else:\n                d,_,_=_pairwise_distances(centers_cur)\n                denom=r_try2[:,None]+r_try2[None,:]\n                with np.errstate(divide='ignore',invalid='ignore'):\n                    ratio=np.where(denom>0,(d-1e-6)/denom,np.inf)\n                iu=np.triu_indices(N,1)\n                s_pairs=np.min(ratio[iu]) if ratio[iu].size>0 else 1.0\n                s_pairs=max(0.0,min(1.0,s_pairs))\n                r_final=np.clip(s_pairs*r_try2,0.0,0.6)\n        if _verify(centers_cur,r_final,tol_pair=1e-6):\n            s=float(np.sum(r_final))\n            if s>best['sum']:\n                best={'centers':centers_cur.copy(),'radii':r_final.copy(),'sum':s}\n    if best['centers'] is None:\n        centers_fb=_hex_initial_centers(N,margin=0.08,seed=42)\n        r_fb=_lp_feasibilize(centers_fb,r_max=None,eps_pair=1e-6)\n        if not _verify(centers_fb,r_fb,tol_pair=1e-6):\n            r_fb=0.9*r_fb\n        best={'centers':centers_fb,'radii':r_fb,'sum':float(np.sum(r_fb))}\n    r_strict=_lp_feasibilize(best['centers'],r_max=None,eps_pair=1.1e-6)\n    if _verify(best['centers'],r_strict,tol_pair=1e-6):\n        radii=r_strict\n    else:\n        radii=best['radii']\n    centers=best['centers'].astype(float)\n    if not _verify(centers,radii,tol_pair=1e-6):\n        d,_,_=_pairwise_distances(centers)\n        denom=radii[:,None]+radii[None,:]\n        with np.errstate(divide='ignore',invalid='ignore'):\n            ratio=np.where(denom>0,(d-1e-6)/denom,np.inf)\n        iu=np.triu_indices(N,1)\n        s_pairs=np.min(ratio[iu]) if ratio[iu].size>0 else 1.0\n        s_pairs=max(0.0,min(1.0,s_pairs))\n        radii=s_pairs*radii\n        bx=np.minimum.reduce([centers[:,0],centers[:,1],1-centers[:,0],1-centers[:,1]])\n        with np.errstate(divide='ignore',invalid='ignore'):\n            scale_b=np.min(np.where(radii>0,bx/radii,np.inf))\n        if not np.isfinite(scale_b): scale_b=1.0\n        scale_b=max(0.0,min(1.0,scale_b))\n        radii=np.clip(scale_b*radii,0.0,0.6)\n    sum_radii=float(np.sum(radii))\n    return centers, radii, sum_radii\n\ndef run_packing():\n    return construct_packing()\n\n# Reference 3 (trial_id=trial_2_0, score=2.614178549664672):\nimport numpy as np\nfrom scipy.optimize import minimize, linprog\n\ndef _pairwise_distances(centers):\n    diff = centers[:, None, :] - centers[None, :, :]\n    d = np.sqrt(np.sum(diff**2, axis=2))\n    return d, diff[..., 0], diff[..., 1]\n\ndef _objective_and_grad(z, N, P):\n    # z = [x(0..N-1), y(0..N-1), r(0..N-1)]\n    x = z[:N]\n    y = z[N:2*N]\n    r = z[2*N:]\n    tiny = 1e-12\n\n    # Boundary hinges s = r - x, r - y, r - (1-x), r - (1-y)\n    s1 = r - x\n    s2 = r - y\n    s3 = r + x - 1.0\n    s4 = r + y - 1.0\n\n    h1 = np.maximum(0.0, s1)\n    h2 = np.maximum(0.0, s2)\n    h3 = np.maximum(0.0, s3)\n    h4 = np.maximum(0.0, s4)\n\n    pen_b = 0.5 * np.sum(h1*h1 + h2*h2 + h3*h3 + h4*h4)\n\n    # Overlap hinges\n    centers = np.stack([x, y], axis=1)\n    d, dx_mat, dy_mat = _pairwise_distances(centers)\n    d_safe = np.maximum(d, tiny)\n    hinge_full = r[:, None] + r[None, :] - d\n    H = np.triu(np.maximum(0.0, hinge_full), 1)\n\n    pen_o = 0.5 * np.sum(H*H)\n\n    # Objective\n    f = -np.sum(r) + P * (pen_b + pen_o)\n\n    # Gradients for boundary penalties: grad 0.5*h^2 = h * grad(arg)\n    dr_b = h1 + h2 + h3 + h4\n    dx_b = -h1 + h3\n    dy_b = -h2 + h4\n\n    # Overlap gradients\n    Qx = H * (dx_mat / d_safe)\n    Qy = H * (dy_mat / d_safe)\n    # For variable i: -sum_j Q[i,j] + sum_k Q[k,i]\n    dx_o = -np.sum(Qx, axis=1) + np.sum(Qx, axis=0)\n    dy_o = -np.sum(Qy, axis=1) + np.sum(Qy, axis=0)\n    dr_o = np.sum(H, axis=1) + np.sum(H, axis=0)\n\n    dx_total = P * (dx_b + dx_o)\n    dy_total = P * (dy_b + dy_o)\n    dr_total = -1.0 + P * (dr_b + dr_o)\n\n    grad = np.concatenate([dx_total, dy_total, dr_total])\n    return f, grad\n\ndef _lp_feasibilize(centers, r_max=None, eps_pair=1e-6):\n    \"\"\"\n    Maximize sum(r) subject to:\n      r_i >= 0\n      r_i <= r_max[i] (if provided)\n      r_i <= x_i, r_i <= y_i, r_i <= 1-x_i, r_i <= 1-y_i\n      r_i + r_j <= dist(c_i,c_j) - eps_pair\n    Use linprog highs. If infeasible with eps_pair, retry with eps_pair=0 and then scale radii down.\n    \"\"\"\n    N = centers.shape[0]\n    x = centers[:, 0]\n    y = centers[:, 1]\n\n    # Build A_ub, b_ub\n    A_rows = []\n    b_vals = []\n\n    # Boundary constraints\n    for i in range(N):\n        e = np.zeros(N)\n        e[i] = 1.0\n        A_rows.append(e.copy()); b_vals.append(x[i])\n        A_rows.append(e.copy()); b_vals.append(y[i])\n        A_rows.append(e.copy()); b_vals.append(1.0 - x[i])\n        A_rows.append(e.copy()); b_vals.append(1.0 - y[i])\n\n    # Pairwise constraints\n    d, _, _ = _pairwise_distances(centers)\n    for i in range(N):\n        for j in range(i+1, N):\n            e = np.zeros(N)\n            e[i] = 1.0; e[j] = 1.0\n            A_rows.append(e)\n            b_vals.append(d[i, j] - eps_pair)\n\n    # Optional upper bounds r_i <= r_max[i]\n    if r_max is not None:\n        for i in range(N):\n            e = np.zeros(N)\n            e[i] = 1.0\n            A_rows.append(e)\n            b_vals.append(r_max[i])\n\n    A_ub = np.array(A_rows)\n    b_ub = np.array(b_vals)\n\n    # Bounds on r: [0, 0.5]\n    bounds = [(0.0, 0.5) for _ in range(N)]\n    c = -np.ones(N)\n\n    res = linprog(c, A_ub=A_ub, b_ub=b_ub, bounds=bounds, method='highs')\n    if res.success:\n        r = res.x\n        return r\n\n    # Retry with eps_pair = 0 to get a feasible base, then scale to satisfy eps_pair\n    b_vals2 = []\n    A_rows2 = []\n    for i in range(N):\n        e = np.zeros(N); e[i] = 1.0\n        A_rows2.append(e.copy()); b_vals2.append(x[i])\n        A_rows2.append(e.copy()); b_vals2.append(y[i])\n        A_rows2.append(e.copy()); b_vals2.append(1.0 - x[i])\n        A_rows2.append(e.copy()); b_vals2.append(1.0 - y[i])\n\n    for i in range(N):\n        for j in range(i+1, N):\n            e = np.zeros(N); e[i] = 1.0; e[j] = 1.0\n            A_rows2.append(e); b_vals2.append(d[i, j])\n\n    if r_max is not None:\n        for i in range(N):\n            e = np.zeros(N); e[i] = 1.0\n            A_rows2.append(e); b_vals2.append(r_max[i])\n\n    A_ub2 = np.array(A_rows2)\n    b_ub2 = np.array(b_vals2)\n\n    res2 = linprog(c, A_ub=A_ub2, b_ub=b_ub2, bounds=bounds, method='highs')\n    if not res2.success:\n        return np.zeros(N)\n\n    r_base = res2.x\n    # Scale radii to satisfy stricter pairwise with eps_pair\n    denom = r_base[:, None] + r_base[None, :]\n    with np.errstate(divide='ignore', invalid='ignore'):\n        ratio = np.where(denom > 0, (d - eps_pair) / denom, np.inf)\n    iu = np.triu_indices(N, 1)\n    if ratio[iu].size > 0:\n        s_pairs = np.min(ratio[iu])\n        if not np.isfinite(s_pairs):\n            s_pairs = 1.0\n        s = min(1.0, s_pairs)\n    else:\n        s = 1.0\n    s = max(0.0, s)\n    r = s * r_base\n    return r\n\ndef _verify(centers, radii, tol_pair=1e-6):\n    N = len(radii)\n    x = centers[:, 0]\n    y = centers[:, 1]\n    if np.any(radii < -1e-12):\n        return False\n    if np.any(x - radii < -1e-9):\n        return False\n    if np.any(y - radii < -1e-9):\n        return False\n    if np.any(x + radii > 1.0 + 1e-9):\n        return False\n    if np.any(y + radii > 1.0 + 1e-9):\n        return False\n    d, _, _ = _pairwise_distances(centers)\n    iu = np.triu_indices(N, 1)\n    lhs = radii[:, None] + radii[None, :]\n    viol = lhs[iu] - d[iu] + tol_pair\n    if np.any(viol > 1e-12):\n        return False\n    return True\n\ndef _hex_initial_centers(N, margin=0.04, jitter_scale=0.04, seed=0):\n    # Fixed row pattern to sum 26\n    rows = [6, 5, 6, 5, 4]\n    assert sum(rows) == 26\n    max_cols = max(rows)\n    nrows = len(rows)\n    dx = (1.0 - 2*margin) / (max_cols - 1)\n    dy = (1.0 - 2*margin) / (nrows - 1) if nrows > 1 else 0.0\n    centers = []\n    rng = np.random.default_rng(seed)\n    for ri, cols in enumerate(rows):\n        y = margin + ri * dy\n        if cols == max_cols:\n            startx = margin\n        else:\n            startx = margin + 0.5 * dx\n        for k in range(cols):\n            x = startx + k * dx\n            centers.append([x, y])\n    centers = np.array(centers)\n    # Jitter\n    jitter = rng.normal(scale=jitter_scale, size=centers.shape)\n    centers = np.clip(centers + jitter, 1e-3, 1 - 1e-3)\n    return centers\n\ndef _multi_start_inits(N):\n    inits = []\n    params = [\n        (0.03, 0.02, 0),\n        (0.05, 0.03, 1),\n        (0.07, 0.03, 2),\n        (0.04, 0.04, 3),\n        (0.06, 0.02, 4),\n        (0.05, 0.04, 5),\n        (0.03, 0.03, 6),\n        (0.07, 0.02, 7),\n    ]\n    for m, j, s in params:\n        inits.append(_hex_initial_centers(N, margin=m, jitter_scale=j, seed=s))\n    return inits\n\ndef construct_packing():\n    N = 26\n    best = {'centers': None, 'radii': None, 'sum': -np.inf}\n\n    # Optimization settings\n    P_schedule = [10.0, 100.0, 1000.0, 5000.0]\n    maxiter_stage = 120\n    bounds = [(0.0, 1.0)] * (2*N) + [(0.0, 0.6)] * N\n\n    # Initializations\n    inits = _multi_start_inits(N)\n\n    for centers0 in inits:\n        # Initial radii small, within boundary\n        x0 = centers0[:, 0]\n        y0 = centers0[:, 1]\n        r0 = 0.05 * np.minimum.reduce([x0, y0, 1 - x0, 1 - y0])\n        z = np.concatenate([x0, y0, r0])\n\n        # Annealing optimization with LP tightening\n        for P in P_schedule:\n            res = minimize(\n                fun=lambda zz: _objective_and_grad(zz, N, P),\n                x0=z,\n                method='L-BFGS-B',\n                jac=True,\n                bounds=bounds,\n                options={'maxiter': maxiter_stage, 'ftol': 1e-12, 'gtol': 1e-8, 'maxcor': 20}\n            )\n            z = res.x\n            centers_tmp = np.stack([z[:N], z[N:2*N]], axis=1)\n            # Conservative LP update, cap by slight expansion over current radii\n            r_cur = np.clip(z[2*N:], 0.0, 0.5)\n            r_cap = 1.05 * r_cur\n            r_lp = _lp_feasibilize(centers_tmp, r_max=r_cap, eps_pair=1e-6)\n            z[2*N:] = np.clip(r_lp, 0.0, 0.5)\n\n        # Final LP without cap to fully utilize geometry\n        centers_cur = np.stack([z[:N], z[N:2*N]], axis=1)\n        r_final = _lp_feasibilize(centers_cur, r_max=None, eps_pair=1e-6)\n\n        # Verify and update best\n        if _verify(centers_cur, r_final, tol_pair=1e-6):\n            s = float(np.sum(r_final))\n            if s > best['sum']:\n                best['centers'] = centers_cur.copy()\n                best['radii'] = r_final.copy()\n                best['sum'] = s\n        else:\n            # Try slight shrink and re-LP to fix tiny violations\n            d, _, _ = _pairwise_distances(centers_cur)\n            denom = r_final[:, None] + r_final[None, :]\n            with np.errstate(divide='ignore', invalid='ignore'):\n                ratio = np.where(denom > 0, (d - 1.0e-6) / denom, np.inf)\n            iu = np.triu_indices(N, 1)\n            s_pairs = np.min(ratio[iu]) if ratio[iu].size > 0 else 1.0\n            if not np.isfinite(s_pairs):\n                s_pairs = 1.0\n            s_pairs = max(0.0, min(1.0, s_pairs))\n            r_try = s_pairs * r_final\n            # Boundaries scale check\n            bx = np.minimum.reduce([centers_cur[:,0], centers_cur[:,1], 1-centers_cur[:,0], 1-centers_cur[:,1]])\n            scale_b = np.min(np.where(r_try > 0, bx / r_try, 1.0))\n            scale_b = min(1.0, scale_b if np.isfinite(scale_b) else 1.0)\n            r_try = scale_b * r_try\n            if _verify(centers_cur, r_try, tol_pair=1e-6):\n                s = float(np.sum(r_try))\n                if s > best['sum']:\n                    best['centers'] = centers_cur.copy()\n                    best['radii'] = r_try.copy()\n                    best['sum'] = s\n\n    # Fallback if nothing verified\n    if best['centers'] is None:\n        centers_fb = _hex_initial_centers(N, margin=0.06, jitter_scale=0.02, seed=42)\n        r_fb = _lp_feasibilize(centers_fb, r_max=None, eps_pair=1e-6)\n        if not _verify(centers_fb, r_fb, tol_pair=1e-6):\n            r_fb = 0.95 * r_fb\n        best['centers'] = centers_fb\n        best['radii'] = r_fb\n        best['sum'] = float(np.sum(r_fb))\n\n    # Ensure strict feasibility and final verify\n    radii_strict = _lp_feasibilize(best['centers'], r_max=None, eps_pair=1.1e-6)\n    if _verify(best['centers'], radii_strict, tol_pair=1e-6):\n        radii = radii_strict\n    else:\n        radii = best['radii']\n\n    centers = best['centers']\n    # Final hardening: if any violation persists, scale radii conservatively\n    if not _verify(centers, radii, tol_pair=1e-6):\n        d, _, _ = _pairwise_distances(centers)\n        denom = radii[:, None] + radii[None, :]\n        with np.errstate(divide='ignore', invalid='ignore'):\n            ratio = np.where(denom > 0, (d - 1e-6) / denom, np.inf)\n        iu = np.triu_indices(N, 1)\n        s_pairs = np.min(ratio[iu]) if ratio[iu].size > 0 else 1.0\n        s_pairs = min(1.0, s_pairs if np.isfinite(s_pairs) else 1.0)\n        s_pairs = max(0.0, s_pairs)\n        radii = s_pairs * radii\n        bx = np.minimum.reduce([centers[:,0], centers[:,1], 1-centers[:,0], 1-centers[:,1]])\n        scale_b = np.min(np.where(radii > 0, bx / radii, 1.0))\n        scale_b = min(1.0, scale_b if np.isfinite(scale_b) else 1.0)\n        radii = scale_b * radii\n\n    # Clip tiny negatives and ensure within [0,0.5]\n    radii = np.clip(radii, 0.0, 0.5)\n    sum_radii = float(np.sum(radii))\n    return centers.astype(float), radii.astype(float), sum_radii\n\ndef run_packing():\n    centers, radii, sum_radii = construct_packing()\n    return centers, radii, sum_radii\n",
  "response": "import numpy as np\nfrom scipy.optimize import minimize, linprog\n\ndef _pairwise_distances(centers):\n    diff = centers[:, None, :] - centers[None, :, :]\n    d = np.sqrt(np.sum(diff**2, axis=2))\n    return d, diff[..., 0], diff[..., 1]\n\ndef _objective_and_grad(z, N, w_over=1.0, w_b=1.0):\n    x = z[:N]\n    y = z[N:2*N]\n    r = z[2*N:]\n    eps = 1e-12\n\n    # Boundary hinges\n    s1 = r - x\n    s2 = r - y\n    s3 = r + x - 1.0\n    s4 = r + y - 1.0\n\n    h1 = np.maximum(0.0, s1)\n    h2 = np.maximum(0.0, s2)\n    h3 = np.maximum(0.0, s3)\n    h4 = np.maximum(0.0, s4)\n\n    pen_b = 0.5 * np.sum(h1*h1 + h2*h2 + h3*h3 + h4*h4)\n\n    # Overlap hinges\n    centers = np.stack([x, y], axis=1)\n    d, dx_mat, dy_mat = _pairwise_distances(centers)\n    denom = d + eps\n    hinge_full = r[:, None] + r[None, :] - d\n    H = np.triu(np.maximum(0.0, hinge_full), 1)\n    pen_o = 0.5 * np.sum(H*H)\n\n    f = -np.sum(r) + w_b * pen_b + w_over * pen_o\n\n    # Gradients for boundary penalties (0.5*h^2 -> h * grad(arg))\n    dr_b = h1 + h2 + h3 + h4\n    dx_b = -h1 + h3\n    dy_b = -h2 + h4\n\n    # Overlap gradients\n    Qx = H * (dx_mat / denom)\n    Qy = H * (dy_mat / denom)\n    dx_o = -np.sum(Qx, axis=1) + np.sum(Qx, axis=0)\n    dy_o = -np.sum(Qy, axis=1) + np.sum(Qy, axis=0)\n    dr_o = np.sum(H, axis=1) + np.sum(H, axis=0)\n\n    dx_total = w_b * dx_b + w_over * dx_o\n    dy_total = w_b * dy_b + w_over * dy_o\n    dr_total = -1.0 + w_b * dr_b + w_over * dr_o\n\n    grad = np.concatenate([dx_total, dy_total, dr_total])\n    return f, grad\n\ndef _lp_feasibilize(centers, r_max=None, eps_pair=1e-6):\n    N = centers.shape[0]\n    x = centers[:, 0].astype(float)\n    y = centers[:, 1].astype(float)\n\n    A_rows = []\n    b_vals = []\n\n    # Boundary constraints\n    for i in range(N):\n        e = np.zeros(N, dtype=float); e[i] = 1.0\n        A_rows.append(e.copy()); b_vals.append(x[i])\n        A_rows.append(e.copy()); b_vals.append(y[i])\n        A_rows.append(e.copy()); b_vals.append(1.0 - x[i])\n        A_rows.append(e.copy()); b_vals.append(1.0 - y[i])\n\n    # Pairwise constraints\n    d, _, _ = _pairwise_distances(centers)\n    for i in range(N):\n        for j in range(i+1, N):\n            e = np.zeros(N, dtype=float); e[i] = 1.0; e[j] = 1.0\n            A_rows.append(e); b_vals.append(d[i, j] - eps_pair)\n\n    # Optional upper bounds r_i <= r_max[i]\n    if r_max is not None:\n        for i in range(N):\n            e = np.zeros(N, dtype=float); e[i] = 1.0\n            A_rows.append(e); b_vals.append(float(r_max[i]))\n\n    A_ub = np.array(A_rows, dtype=float)\n    b_ub = np.array(b_vals, dtype=float)\n    bounds = [(0.0, 0.6) for _ in range(N)]\n    c = -np.ones(N, dtype=float)\n\n    res = linprog(c, A_ub=A_ub, b_ub=b_ub, bounds=bounds, method='highs')\n    if res.success:\n        return np.clip(res.x, 0.0, 0.6)\n\n    # Retry without eps_pair, then scale to satisfy eps_pair\n    A_rows2 = []\n    b_vals2 = []\n    for i in range(N):\n        e = np.zeros(N, dtype=float); e[i] = 1.0\n        A_rows2.append(e.copy()); b_vals2.append(x[i])\n        A_rows2.append(e.copy()); b_vals2.append(y[i])\n        A_rows2.append(e.copy()); b_vals2.append(1.0 - x[i])\n        A_rows2.append(e.copy()); b_vals2.append(1.0 - y[i])\n    for i in range(N):\n        for j in range(i+1, N):\n            e = np.zeros(N, dtype=float); e[i] = 1.0; e[j] = 1.0\n            A_rows2.append(e); b_vals2.append(d[i, j])\n    if r_max is not None:\n        for i in range(N):\n            e = np.zeros(N, dtype=float); e[i] = 1.0\n            A_rows2.append(e); b_vals2.append(float(r_max[i]))\n\n    A_ub2 = np.array(A_rows2, dtype=float)\n    b_ub2 = np.array(b_vals2, dtype=float)\n\n    res2 = linprog(c, A_ub=A_ub2, b_ub=b_ub2, bounds=bounds, method='highs')\n    if not res2.success:\n        # Fallback small radii\n        r_try = 0.1 * np.minimum.reduce([x, y, 1 - x, 1 - y])\n        denom = r_try[:, None] + r_try[None, :]\n        with np.errstate(divide='ignore', invalid='ignore'):\n            ratio = np.where(denom > 0, (d - eps_pair) / denom, np.inf)\n        iu = np.triu_indices(N, 1)\n        s_pairs = np.min(ratio[iu]) if ratio[iu].size > 0 else 1.0\n        s_pairs = max(0.0, min(1.0, s_pairs if np.isfinite(s_pairs) else 1.0))\n        return np.clip(s_pairs * r_try, 0.0, 0.6)\n\n    r_base = np.clip(res2.x, 0.0, 0.6)\n    denom = r_base[:, None] + r_base[None, :]\n    with np.errstate(divide='ignore', invalid='ignore'):\n        ratio = np.where(denom > 0, (d - eps_pair) / denom, np.inf)\n    iu = np.triu_indices(N, 1)\n    s_pairs = np.min(ratio[iu]) if ratio[iu].size > 0 else 1.0\n    s_pairs = max(0.0, min(1.0, s_pairs if np.isfinite(s_pairs) else 1.0))\n    return np.clip(s_pairs * r_base, 0.0, 0.6)\n\ndef _verify(centers, radii, tol_pair=1e-6):\n    N = len(radii)\n    x = centers[:, 0]\n    y = centers[:, 1]\n    if centers.shape != (N, 2):\n        return False\n    if np.any(radii < -1e-12):\n        return False\n    if np.any(x - radii < -1e-9):\n        return False\n    if np.any(y - radii < -1e-9):\n        return False\n    if np.any(x + radii > 1.0 + 1e-9):\n        return False\n    if np.any(y + radii > 1.0 + 1e-9):\n        return False\n    d, _, _ = _pairwise_distances(centers)\n    iu = np.triu_indices(N, 1)\n    lhs = radii[:, None] + radii[None, :]\n    if lhs[iu].size > 0:\n        viol = lhs[iu] - d[iu] + tol_pair\n        if np.any(viol > 1e-12):\n            return False\n    return True\n\ndef _hex_seed(N, margin=0.04, jitter=0.03, seed=0):\n    rows = [6, 5, 6, 5, 4]\n    assert sum(rows) == N\n    max_cols = max(rows)\n    nrows = len(rows)\n    dx = (1.0 - 2 * margin) / (max_cols - 1) if max_cols > 1 else 0.0\n    dy = (1.0 - 2 * margin) / (nrows - 1) if nrows > 1 else 0.0\n    centers = []\n    for ri, cols in enumerate(rows):\n        y = margin + ri * dy\n        startx = margin if cols == max_cols else margin + 0.5 * dx\n        if cols == 1:\n            centers.append([0.5, y])\n        else:\n            for k in range(cols):\n                centers.append([startx + k * dx, y])\n    centers = np.array(centers, dtype=float)\n    rng = np.random.default_rng(seed)\n    centers = centers + rng.normal(scale=jitter, size=centers.shape)\n    centers = np.clip(centers, 1e-3, 1 - 1e-3)\n    return centers\n\ndef _edge_ring_seed(N, margin=0.05):\n    # 7 bottom, 7 top, 6 left (excluding corners), 6 right (excluding corners) = 26\n    xs7 = np.linspace(margin, 1 - margin, 7)\n    ys6 = np.linspace(margin, 1 - margin, 8)[1:-1]  # 6 points excluding endpoints\n    pts = []\n    for x in xs7:\n        pts.append([x, margin])       # bottom\n    for x in xs7:\n        pts.append([x, 1 - margin])   # top\n    for y in ys6:\n        pts.append([margin, y])       # left (excluding corners)\n    for y in ys6:\n        pts.append([1 - margin, y])   # right (excluding corners)\n    centers = np.array(pts, dtype=float)\n    if centers.shape[0] > N:\n        centers = centers[:N]\n    return centers\n\ndef _lhs_seed(N, seed=0):\n    rng = np.random.default_rng(seed)\n    xs = (np.arange(N) + rng.random(N)) / N\n    ys = (np.arange(N) + rng.random(N)) / N\n    rng.shuffle(xs)\n    rng.shuffle(ys)\n    centers = np.vstack([xs, ys]).T\n    centers = np.clip(centers, 0.02, 0.98)\n    return centers\n\ndef _center_cluster_seed(N, seed=0):\n    rng = np.random.default_rng(seed)\n    centers = np.clip(0.5 + 0.22 * rng.standard_normal((N, 2)), 0.02, 0.98)\n    return centers\n\ndef construct_packing():\n    N = 26\n    bounds = [(0.0, 1.0)] * (2 * N) + [(0.0, 0.6)] * N\n    weight_schedule = [(50.0, 50.0), (200.0, 200.0), (1000.0, 1000.0), (4000.0, 4000.0)]\n    maxiter_stage = 180\n\n    seeds = []\n    seeds.append(_hex_seed(N, margin=0.04, jitter=0.03, seed=0))\n    seeds.append(_hex_seed(N, margin=0.06, jitter=0.02, seed=1))\n    seeds.append(_edge_ring_seed(N, margin=0.05))\n    seeds.append(_lhs_seed(N, seed=2))\n    seeds.append(_center_cluster_seed(N, seed=3))\n\n    best_sum = -np.inf\n    best_centers = None\n\n    for centers0 in seeds:\n        x0 = centers0[:, 0]\n        y0 = centers0[:, 1]\n        r0 = 0.05 * np.minimum.reduce([x0, y0, 1 - x0, 1 - y0])\n        z = np.concatenate([x0, y0, r0])\n\n        for (w_over, w_b) in weight_schedule:\n            res = minimize(\n                fun=lambda zz: _objective_and_grad(zz, N, w_over=w_over, w_b=w_b),\n                x0=z,\n                method='L-BFGS-B',\n                jac=True,\n                bounds=bounds,\n                options={'maxiter': maxiter_stage, 'ftol': 1e-12, 'gtol': 1e-8, 'maxcor': 25}\n            )\n            z = res.x\n\n        centers_end = np.stack([z[:N], z[N:2*N]], axis=1)\n        r_lp = _lp_feasibilize(centers_end, r_max=None, eps_pair=1.1e-6)\n        s = float(np.sum(r_lp))\n        if s > best_sum:\n            best_sum = s\n            best_centers = centers_end.copy()\n\n    # Final LP on best centers\n    radii = _lp_feasibilize(best_centers, r_max=None, eps_pair=1.1e-6)\n    # Ensure strict feasibility; if any issues, shrink slightly and re-LP\n    if not _verify(best_centers, radii, tol_pair=1e-6):\n        radii = _lp_feasibilize(best_centers, r_max=0.999 * radii, eps_pair=1.2e-6)\n        if not _verify(best_centers, radii, tol_pair=1e-6):\n            d, _, _ = _pairwise_distances(best_centers)\n            denom = radii[:, None] + radii[None, :]\n            with np.errstate(divide='ignore', invalid='ignore'):\n                ratio = np.where(denom > 0, (d - 1.2e-6) / denom, np.inf)\n            iu = np.triu_indices(N, 1)\n            s_pairs = np.min(ratio[iu]) if ratio[iu].size > 0 else 1.0\n            s_pairs = max(0.0, min(1.0, s_pairs if np.isfinite(s_pairs) else 1.0))\n            radii = np.clip(s_pairs * radii, 0.0, 0.6)\n            # Boundary scaling\n            bx = np.minimum.reduce([best_centers[:, 0], best_centers[:, 1], 1 - best_centers[:, 0], 1 - best_centers[:, 1]])\n            with np.errstate(divide='ignore', invalid='ignore'):\n                scale_b = np.min(np.where(radii > 0, bx / radii, 1.0))\n            if not np.isfinite(scale_b):\n                scale_b = 1.0\n            radii = np.clip(scale_b * radii, 0.0, 0.6)\n\n    radii = np.clip(radii, 0.0, 0.6)\n    sum_radii = float(np.sum(radii))\n    return best_centers.astype(float), radii.astype(float), sum_radii\n\ndef run_packing():\n    return construct_packing()",
  "reasoning": "import numpy as np\nfrom scipy.optimize import minimize, linprog\n\ndef _pairwise_distances(centers):\n    diff = centers[:, None, :] - centers[None, :, :]\n    d = np.sqrt(np.sum(diff**2, axis=2))\n    return d, diff[..., 0], diff[..., 1]\n\ndef _objective_and_grad(z, N, w_over=1.0, w_b=1.0):\n    x = z[:N]\n    y = z[N:2*N]\n    r = z[2*N:]\n    eps = 1e-12\n\n    # Boundary hinges\n    s1 = r - x\n    s2 = r - y\n    s3 = r + x - 1.0\n    s4 = r + y - 1.0\n\n    h1 = np.maximum(0.0, s1)\n    h2 = np.maximum(0.0, s2)\n    h3 = np.maximum(0.0, s3)\n    h4 = np.maximum(0.0, s4)\n\n    pen_b = 0.5 * np.sum(h1*h1 + h2*h2 + h3*h3 + h4*h4)\n\n    # Overlap hinges\n    centers = np.stack([x, y], axis=1)\n    d, dx_mat, dy_mat = _pairwise_distances(centers)\n    denom = d + eps\n    hinge_full = r[:, None] + r[None, :] - d\n    H = np.triu(np.maximum(0.0, hinge_full), 1)\n    pen_o = 0.5 * np.sum(H*H)\n\n    f = -np.sum(r) + w_b * pen_b + w_over * pen_o\n\n    # Gradients for boundary penalties (0.5*h^2 -> h * grad(arg))\n    dr_b = h1 + h2 + h3 + h4\n    dx_b = -h1 + h3\n    dy_b = -h2 + h4\n\n    # Overlap gradients\n    Qx = H * (dx_mat / denom)\n    Qy = H * (dy_mat / denom)\n    dx_o = -np.sum(Qx, axis=1) + np.sum(Qx, axis=0)\n    dy_o = -np.sum(Qy, axis=1) + np.sum(Qy, axis=0)\n    dr_o = np.sum(H, axis=1) + np.sum(H, axis=0)\n\n    dx_total = w_b * dx_b + w_over * dx_o\n    dy_total = w_b * dy_b + w_over * dy_o\n    dr_total = -1.0 + w_b * dr_b + w_over * dr_o\n\n    grad = np.concatenate([dx_total, dy_total, dr_total])\n    return f, grad\n\ndef _lp_feasibilize(centers, r_max=None, eps_pair=1e-6):\n    N = centers.shape[0]\n    x = centers[:, 0].astype(float)\n    y = centers[:, 1].astype(float)\n\n    A_rows = []\n    b_vals = []\n\n    # Boundary constraints\n    for i in range(N):\n        e = np.zeros(N, dtype=float); e[i] = 1.0\n        A_rows.append(e.copy()); b_vals.append(x[i])\n        A_rows.append(e.copy()); b_vals.append(y[i])\n        A_rows.append(e.copy()); b_vals.append(1.0 - x[i])\n        A_rows.append(e.copy()); b_vals.append(1.0 - y[i])\n\n    # Pairwise constraints\n    d, _, _ = _pairwise_distances(centers)\n    for i in range(N):\n        for j in range(i+1, N):\n            e = np.zeros(N, dtype=float); e[i] = 1.0; e[j] = 1.0\n            A_rows.append(e); b_vals.append(d[i, j] - eps_pair)\n\n    # Optional upper bounds r_i <= r_max[i]\n    if r_max is not None:\n        for i in range(N):\n            e = np.zeros(N, dtype=float); e[i] = 1.0\n            A_rows.append(e); b_vals.append(float(r_max[i]))\n\n    A_ub = np.array(A_rows, dtype=float)\n    b_ub = np.array(b_vals, dtype=float)\n    bounds = [(0.0, 0.6) for _ in range(N)]\n    c = -np.ones(N, dtype=float)\n\n    res = linprog(c, A_ub=A_ub, b_ub=b_ub, bounds=bounds, method='highs')\n    if res.success:\n        return np.clip(res.x, 0.0, 0.6)\n\n    # Retry without eps_pair, then scale to satisfy eps_pair\n    A_rows2 = []\n    b_vals2 = []\n    for i in range(N):\n        e = np.zeros(N, dtype=float); e[i] = 1.0\n        A_rows2.append(e.copy()); b_vals2.append(x[i])\n        A_rows2.append(e.copy()); b_vals2.append(y[i])\n        A_rows2.append(e.copy()); b_vals2.append(1.0 - x[i])\n        A_rows2.append(e.copy()); b_vals2.append(1.0 - y[i])\n    for i in range(N):\n        for j in range(i+1, N):\n            e = np.zeros(N, dtype=float); e[i] = 1.0; e[j] = 1.0\n            A_rows2.append(e); b_vals2.append(d[i, j])\n    if r_max is not None:\n        for i in range(N):\n            e = np.zeros(N, dtype=float); e[i] = 1.0\n            A_rows2.append(e); b_vals2.append(float(r_max[i]))\n\n    A_ub2 = np.array(A_rows2, dtype=float)\n    b_ub2 = np.array(b_vals2, dtype=float)\n\n    res2 = linprog(c, A_ub=A_ub2, b_ub=b_ub2, bounds=bounds, method='highs')\n    if not res2.success:\n        # Fallback small radii\n        r_try = 0.1 * np.minimum.reduce([x, y, 1 - x, 1 - y])\n        denom = r_try[:, None] + r_try[None, :]\n        with np.errstate(divide='ignore', invalid='ignore'):\n            ratio = np.where(denom > 0, (d - eps_pair) / denom, np.inf)\n        iu = np.triu_indices(N, 1)\n        s_pairs = np.min(ratio[iu]) if ratio[iu].size > 0 else 1.0\n        s_pairs = max(0.0, min(1.0, s_pairs if np.isfinite(s_pairs) else 1.0))\n        return np.clip(s_pairs * r_try, 0.0, 0.6)\n\n    r_base = np.clip(res2.x, 0.0, 0.6)\n    denom = r_base[:, None] + r_base[None, :]\n    with np.errstate(divide='ignore', invalid='ignore'):\n        ratio = np.where(denom > 0, (d - eps_pair) / denom, np.inf)\n    iu = np.triu_indices(N, 1)\n    s_pairs = np.min(ratio[iu]) if ratio[iu].size > 0 else 1.0\n    s_pairs = max(0.0, min(1.0, s_pairs if np.isfinite(s_pairs) else 1.0))\n    return np.clip(s_pairs * r_base, 0.0, 0.6)\n\ndef _verify(centers, radii, tol_pair=1e-6):\n    N = len(radii)\n    x = centers[:, 0]\n    y = centers[:, 1]\n    if centers.shape != (N, 2):\n        return False\n    if np.any(radii < -1e-12):\n        return False\n    if np.any(x - radii < -1e-9):\n        return False\n    if np.any(y - radii < -1e-9):\n        return False\n    if np.any(x + radii > 1.0 + 1e-9):\n        return False\n    if np.any(y + radii > 1.0 + 1e-9):\n        return False\n    d, _, _ = _pairwise_distances(centers)\n    iu = np.triu_indices(N, 1)\n    lhs = radii[:, None] + radii[None, :]\n    if lhs[iu].size > 0:\n        viol = lhs[iu] - d[iu] + tol_pair\n        if np.any(viol > 1e-12):\n            return False\n    return True\n\ndef _hex_seed(N, margin=0.04, jitter=0.03, seed=0):\n    rows = [6, 5, 6, 5, 4]\n    assert sum(rows) == N\n    max_cols = max(rows)\n    nrows = len(rows)\n    dx = (1.0 - 2 * margin) / (max_cols - 1) if max_cols > 1 else 0.0\n    dy = (1.0 - 2 * margin) / (nrows - 1) if nrows > 1 else 0.0\n    centers = []\n    for ri, cols in enumerate(rows):\n        y = margin + ri * dy\n        startx = margin if cols == max_cols else margin + 0.5 * dx\n        if cols == 1:\n            centers.append([0.5, y])\n        else:\n            for k in range(cols):\n                centers.append([startx + k * dx, y])\n    centers = np.array(centers, dtype=float)\n    rng = np.random.default_rng(seed)\n    centers = centers + rng.normal(scale=jitter, size=centers.shape)\n    centers = np.clip(centers, 1e-3, 1 - 1e-3)\n    return centers\n\ndef _edge_ring_seed(N, margin=0.05):\n    # 7 bottom, 7 top, 6 left (excluding corners), 6 right (excluding corners) = 26\n    xs7 = np.linspace(margin, 1 - margin, 7)\n    ys6 = np.linspace(margin, 1 - margin, 8)[1:-1]  # 6 points excluding endpoints\n    pts = []\n    for x in xs7:\n        pts.append([x, margin])       # bottom\n    for x in xs7:\n        pts.append([x, 1 - margin])   # top\n    for y in ys6:\n        pts.append([margin, y])       # left (excluding corners)\n    for y in ys6:\n        pts.append([1 - margin, y])   # right (excluding corners)\n    centers = np.array(pts, dtype=float)\n    if centers.shape[0] > N:\n        centers = centers[:N]\n    return centers\n\ndef _lhs_seed(N, seed=0):\n    rng = np.random.default_rng(seed)\n    xs = (np.arange(N) + rng.random(N)) / N\n    ys = (np.arange(N) + rng.random(N)) / N\n    rng.shuffle(xs)\n    rng.shuffle(ys)\n    centers = np.vstack([xs, ys]).T\n    centers = np.clip(centers, 0.02, 0.98)\n    return centers\n\ndef _center_cluster_seed(N, seed=0):\n    rng = np.random.default_rng(seed)\n    centers = np.clip(0.5 + 0.22 * rng.standard_normal((N, 2)), 0.02, 0.98)\n    return centers\n\ndef construct_packing():\n    N = 26\n    bounds = [(0.0, 1.0)] * (2 * N) + [(0.0, 0.6)] * N\n    weight_schedule = [(50.0, 50.0), (200.0, 200.0), (1000.0, 1000.0), (4000.0, 4000.0)]\n    maxiter_stage = 180\n\n    seeds = []\n    seeds.append(_hex_seed(N, margin=0.04, jitter=0.03, seed=0))\n    seeds.append(_hex_seed(N, margin=0.06, jitter=0.02, seed=1))\n    seeds.append(_edge_ring_seed(N, margin=0.05))\n    seeds.append(_lhs_seed(N, seed=2))\n    seeds.append(_center_cluster_seed(N, seed=3))\n\n    best_sum = -np.inf\n    best_centers = None\n\n    for centers0 in seeds:\n        x0 = centers0[:, 0]\n        y0 = centers0[:, 1]\n        r0 = 0.05 * np.minimum.reduce([x0, y0, 1 - x0, 1 - y0])\n        z = np.concatenate([x0, y0, r0])\n\n        for (w_over, w_b) in weight_schedule:\n            res = minimize(\n                fun=lambda zz: _objective_and_grad(zz, N, w_over=w_over, w_b=w_b),\n                x0=z,\n                method='L-BFGS-B',\n                jac=True,\n                bounds=bounds,\n                options={'maxiter': maxiter_stage, 'ftol': 1e-12, 'gtol': 1e-8, 'maxcor': 25}\n            )\n            z = res.x\n\n        centers_end = np.stack([z[:N], z[N:2*N]], axis=1)\n        r_lp = _lp_feasibilize(centers_end, r_max=None, eps_pair=1.1e-6)\n        s = float(np.sum(r_lp))\n        if s > best_sum:\n            best_sum = s\n            best_centers = centers_end.copy()\n\n    # Final LP on best centers\n    radii = _lp_feasibilize(best_centers, r_max=None, eps_pair=1.1e-6)\n    # Ensure strict feasibility; if any issues, shrink slightly and re-LP\n    if not _verify(best_centers, radii, tol_pair=1e-6):\n        radii = _lp_feasibilize(best_centers, r_max=0.999 * radii, eps_pair=1.2e-6)\n        if not _verify(best_centers, radii, tol_pair=1e-6):\n            d, _, _ = _pairwise_distances(best_centers)\n            denom = radii[:, None] + radii[None, :]\n            with np.errstate(divide='ignore', invalid='ignore'):\n                ratio = np.where(denom > 0, (d - 1.2e-6) / denom, np.inf)\n            iu = np.triu_indices(N, 1)\n            s_pairs = np.min(ratio[iu]) if ratio[iu].size > 0 else 1.0\n            s_pairs = max(0.0, min(1.0, s_pairs if np.isfinite(s_pairs) else 1.0))\n            radii = np.clip(s_pairs * radii, 0.0, 0.6)\n            # Boundary scaling\n            bx = np.minimum.reduce([best_centers[:, 0], best_centers[:, 1], 1 - best_centers[:, 0], 1 - best_centers[:, 1]])\n            with np.errstate(divide='ignore', invalid='ignore'):\n                scale_b = np.min(np.where(radii > 0, bx / radii, 1.0))\n            if not np.isfinite(scale_b):\n                scale_b = 1.0\n            radii = np.clip(scale_b * radii, 0.0, 0.6)\n\n    radii = np.clip(radii, 0.0, 0.6)\n    sum_radii = float(np.sum(radii))\n    return best_centers.astype(float), radii.astype(float), sum_radii\n\ndef run_packing():\n    return construct_packing()",
  "timestamp": "2026-02-11T01:36:20.282095",
  "cost_data": null,
  "model_config": {
    "model": "openai/gpt-5",
    "temperature": 0.25
  },
  "llm_call_duration_s": 111.63261988700833,
  "eval_duration_s": 0.0
}