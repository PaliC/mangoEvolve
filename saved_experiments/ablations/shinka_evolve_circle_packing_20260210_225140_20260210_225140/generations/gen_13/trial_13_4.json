{
  "trial_id": "trial_13_4",
  "generation": 13,
  "parent_id": "trial_7_7",
  "code": "import numpy as np\nfrom scipy.optimize import minimize, linprog\n\ndef construct_packing():\n    \"\"\"\n    Return (centers, radii, sum_radii) with:\n        - centers: np.array shape (26, 2)\n        - radii:   np.array shape (26,)\n        - sum_radii: float, strictly feasible sum of radii\n    Strategy:\n      - Generate multiple deterministic hex-like initial center layouts.\n      - Parameterize centers via unconstrained variables u; centers = sigmoid(u).\n      - Optimize centers-only with a smooth repulsive objective:\n          sum_{i<j} exp(-k * d_ij) + sum_i exp(-k * b_i)\n        where b_i is a smooth-min distance to the four boundaries.\n      - After each local optimization, solve an LP to maximize radii for fixed centers\n        with strict eps_pair = 1e-6 to ensure non-overlap and boundary containment.\n      - Keep the best strictly feasible packing found.\n    \"\"\"\n    # ---------------------------\n    # Helper utilities\n    # ---------------------------\n    def _sigmoid(u):\n        # Numerically stable sigmoid\n        # Clip inputs to avoid overflow in exp\n        uu = np.clip(u, -60.0, 60.0)\n        s = 1.0 / (1.0 + np.exp(-uu))\n        # Keep strictly inside (1e-9, 1-1e-9) to avoid degenerate LP bounds\n        return np.clip(s, 1e-9, 1 - 1e-9)\n\n    def _sigmoid_grad(u):\n        s = _sigmoid(u)\n        return s * (1.0 - s)\n\n    def _pairwise_distances(centers):\n        diff = centers[:, None, :] - centers[None, :, :]\n        d = np.sqrt(np.sum(diff ** 2, axis=2))\n        return d, diff[..., 0], diff[..., 1]\n\n    def _hex_initial_centers_from_rows(rows, margin=0.05, jitter_scale=0.015, seed=0):\n        \"\"\"\n        Deterministic hex-like pattern with small jitter.\n        rows: list of ints summing to 26\n        \"\"\"\n        assert sum(rows) == 26\n        max_cols = max(rows)\n        nrows = len(rows)\n        dx_base = (1.0 - 2 * margin) / (max_cols - 1) if max_cols > 1 else 0.0\n        dy_base = (1.0 - 2 * margin) / (nrows - 1) if nrows > 1 else 0.0\n        centers = []\n        rng = np.random.default_rng(seed)\n        for ri, cols in enumerate(rows):\n            y = margin + ri * dy_base if nrows > 1 else 0.5\n            if cols > 1:\n                row_width = (cols - 1) * dx_base\n                startx = 0.5 - row_width / 2.0\n                if ri % 2 == 1:\n                    startx += dx_base / 2.0\n            else:\n                startx = 0.5\n            for k in range(cols):\n                x = startx + (k * dx_base if cols > 1 else 0.0)\n                centers.append([x, y])\n        centers = np.array(centers, dtype=float)\n        # Add small deterministic jitter using RNG but seed fixed\n        jitter = rng.normal(scale=jitter_scale, size=centers.shape)\n        centers = centers + jitter\n        centers = np.clip(centers, 1e-6, 1 - 1e-6)\n        return centers\n\n    def _multi_start_inits():\n        patterns = [\n            [6, 5, 6, 5, 4], [6, 5, 5, 5, 5], [5, 6, 5, 5, 5],\n            [5, 5, 6, 5, 5], [6, 6, 5, 5, 4], [5, 6, 6, 5, 4],\n            [6, 5, 6, 4, 5], [5, 5, 5, 5, 6], [4, 5, 6, 5, 6],\n            [5, 4, 5, 6, 6], [5, 6, 4, 6, 5],\n        ]\n        params = [\n            (0.04, 0.012), (0.05, 0.015), (0.06, 0.018),\n            (0.045, 0.02), (0.035, 0.01), (0.055, 0.014)\n        ]\n        inits = []\n        seed = 1\n        for pat in patterns:\n            for (m, j) in params:\n                inits.append(_hex_initial_centers_from_rows(pat, margin=m, jitter_scale=j, seed=seed))\n                seed += 1\n        # Ensure at least some additional structured edge-corner biased initialization\n        # corners + edges + interior small circles as a fallback arrangement\n        if len(inits) < 16:\n            extra = _hex_initial_centers_from_rows([6,5,6,5,4], margin=0.07, jitter_scale=0.01, seed=999)\n            inits.append(extra)\n        return inits\n\n    def _smooth_min_four(x, y):\n        # Returns b = smooth-min(x, y, 1-x, 1-y) and gradients wrt x,y\n        # b = -1/gamma * log(sum_j exp(-gamma * t_j))\n        gamma = 80.0\n        t1 = x\n        t2 = y\n        t3 = 1.0 - x\n        t4 = 1.0 - y\n        T = np.stack([ -gamma * t1, -gamma * t2, -gamma * t3, -gamma * t4 ], axis=0)\n        # To stabilize log-sum-exp:\n        m = np.max(T, axis=0)\n        S = np.exp(T - m)\n        denom = np.sum(S, axis=0)\n        logsum = np.log(denom) + m\n        b = - (1.0 / gamma) * logsum\n        # probabilities for each term p_j = exp(-gamma t_j) / sum\n        p = S / denom\n        # derivative db/dx = sum_j p_j * dt_j/dx\n        # dt/dx = [1, 0, -1, 0] for [x, y, 1-x, 1-y]\n        db_dx = p[0] * 1.0 + p[2] * (-1.0)\n        db_dy = p[1] * 1.0 + p[3] * (-1.0)\n        return b, db_dx, db_dy\n\n    def _objective_and_grad_u(u):\n        \"\"\"\n        centers parameterized by u: c = sigmoid(u)\n        Objective: sum_{i<j} exp(-k * d_ij) + sum_i exp(-k * b_i)\n        Minimization of this objective encourages spread and staying away from boundaries.\n        Returns scalar f and gradient wrt u (len 2N).\n        \"\"\"\n        N = 26\n        centers = _sigmoid(u.reshape((-1, 2)))\n        x = centers[:, 0]\n        y = centers[:, 1]\n        # Pairwise distances and diff\n        d, dx_mat, dy_mat = _pairwise_distances(centers)\n        iu = np.triu_indices(N, 1)\n        dab = d[iu]\n        dxab = dx_mat[iu]\n        dyab = dy_mat[iu]\n        # Parameters\n        k = 28.0  # repulsion strength\n        tiny = 1e-12\n        # Pairwise repulsive terms: exp(-k * d)\n        exp_pairs = np.exp(-k * dab)\n        f_pairs = np.sum(exp_pairs)\n        # Gradients wrt centers for pairwise\n        # d/ x_a derivative: (x_a - x_b)/d = -dxab/d (since dxab = xa - xb)\n        d_safe = np.maximum(dab, tiny)\n        coeff = -k * exp_pairs / d_safe  # scalar per pair multiply (xa-xb)\n        gx = np.zeros(N, dtype=float)\n        gy = np.zeros(N, dtype=float)\n        # For each pair (i,j) at positions iu[0], iu[1]:\n        iidx = iu[0]; jidx = iu[1]\n        # contribution to grad_x_i: coeff * dxab\n        np.add.at(gx, iidx, coeff * dxab)\n        np.add.at(gx, jidx, -coeff * dxab)\n        np.add.at(gy, iidx, coeff * dyab)\n        np.add.at(gy, jidx, -coeff * dyab)\n        # Boundary smooth-min b_i and repulsion exp(-k*b_i)\n        b, db_dx, db_dy = _smooth_min_four(x, y)\n        exp_bound = np.exp(-k * b)\n        f_bound = np.sum(exp_bound)\n        # grad wrt x,y from boundary term: -k * exp(-k*b) * db_dx\n        gx += (-k * exp_bound) * db_dx\n        gy += (-k * exp_bound) * db_dy\n        f = f_pairs + f_bound\n        # Chain rule to u via sigmoid derivative\n        s_grad = _sigmoid_grad(u.reshape((-1, 2)))\n        gu = np.empty_like(u).reshape((-1, 2))\n        gu[:, 0] = gx * s_grad[:, 0]\n        gu[:, 1] = gy * s_grad[:, 1]\n        return float(f), gu.ravel()\n\n    def _lp_feasibilize(centers, r_max=None, eps_pair=1e-6):\n        \"\"\"\n        LP maximize sum r subject to:\n          0 <= r_i <= 0.5\n          r_i <= x_i - eps_pair\n          r_i <= y_i - eps_pair\n          r_i <= 1-x_i - eps_pair\n          r_i <= 1-y_i - eps_pair\n          r_i + r_j <= dist(c_i,c_j) - eps_pair\n        If infeasible with eps_pair, fallback to eps=0 then scale down radii to satisfy eps_pair.\n        \"\"\"\n        N = centers.shape[0]\n        x = centers[:, 0]\n        y = centers[:, 1]\n        # Build constraints\n        A_rows = []\n        b_vals = []\n        # boundary constraints\n        for i in range(N):\n            e = np.zeros(N, dtype=float); e[i] = 1.0\n            A_rows.append(e.copy()); b_vals.append(x[i] - eps_pair)\n            A_rows.append(e.copy()); b_vals.append(y[i] - eps_pair)\n            A_rows.append(e.copy()); b_vals.append(1.0 - x[i] - eps_pair)\n            A_rows.append(e.copy()); b_vals.append(1.0 - y[i] - eps_pair)\n        # pairwise\n        d, _, _ = _pairwise_distances(centers)\n        for i in range(N):\n            for j in range(i+1, N):\n                e = np.zeros(N, dtype=float); e[i] = 1.0; e[j] = 1.0\n                A_rows.append(e); b_vals.append(d[i, j] - eps_pair)\n        if r_max is not None:\n            for i in range(N):\n                e = np.zeros(N, dtype=float); e[i] = 1.0\n                A_rows.append(e); b_vals.append(min(0.5, float(r_max[i])))\n        A_ub = np.array(A_rows, dtype=float)\n        b_ub = np.array(b_vals, dtype=float)\n        bounds = [(0.0, 0.5) for _ in range(N)]\n        c = -np.ones(N, dtype=float)  # minimize c^T r = -sum r => maximize sum r\n        res = linprog(c, A_ub=A_ub, b_ub=b_ub, bounds=bounds, method='highs')\n        if res.success:\n            return np.clip(res.x, 0.0, 0.5)\n        # Fallback: try without eps_pair\n        A_rows2 = []\n        b_vals2 = []\n        for i in range(N):\n            e = np.zeros(N, dtype=float); e[i] = 1.0\n            A_rows2.append(e.copy()); b_vals2.append(x[i])\n            A_rows2.append(e.copy()); b_vals2.append(y[i])\n            A_rows2.append(e.copy()); b_vals2.append(1.0 - x[i])\n            A_rows2.append(e.copy()); b_vals2.append(1.0 - y[i])\n        for i in range(N):\n            for j in range(i+1, N):\n                e = np.zeros(N, dtype=float); e[i] = 1.0; e[j] = 1.0\n                A_rows2.append(e); b_vals2.append(d[i, j])\n        if r_max is not None:\n            for i in range(N):\n                e = np.zeros(N, dtype=float); e[i] = 1.0\n                A_rows2.append(e); b_vals2.append(min(0.5, float(r_max[i])))\n        A_ub2 = np.array(A_rows2, dtype=float)\n        b_ub2 = np.array(b_vals2, dtype=float)\n        res2 = linprog(c, A_ub=A_ub2, b_ub=b_ub2, bounds=bounds, method='highs')\n        if not res2.success:\n            return np.zeros(N, dtype=float)\n        r_base = np.clip(res2.x, 0.0, 0.5)\n        # Scale down r_base to satisfy eps_pair\n        denom_pairs = r_base[:, None] + r_base[None, :]\n        with np.errstate(divide='ignore', invalid='ignore'):\n            ratio_pairs = np.where(denom_pairs > 1e-12, (d - eps_pair) / denom_pairs, np.inf)\n        iu = np.triu_indices(N, 1)\n        s_pairs = np.min(ratio_pairs[iu]) if ratio_pairs[iu].size > 0 else 1.0\n        if not np.isfinite(s_pairs):\n            s_pairs = 1.0\n        s_pairs = max(0.0, min(1.0, s_pairs))\n        # boundary\n        with np.errstate(divide='ignore', invalid='ignore'):\n            s_x_min = np.where(r_base > 1e-12, (x - eps_pair) / r_base, np.inf)\n            s_y_min = np.where(r_base > 1e-12, (y - eps_pair) / r_base, np.inf)\n            s_x_max = np.where(r_base > 1e-12, (1.0 - x - eps_pair) / r_base, np.inf)\n            s_y_max = np.where(r_base > 1e-12, (1.0 - y - eps_pair) / r_base, np.inf)\n        s_boundary = np.min([s_x_min, s_y_min, s_x_max, s_y_max])\n        if not np.isfinite(s_boundary):\n            s_boundary = 1.0\n        s_boundary = max(0.0, min(1.0, s_boundary))\n        s = min(s_pairs, s_boundary)\n        r_scaled = s * r_base\n        return np.clip(r_scaled, 0.0, 0.5)\n\n    def _verify(centers, radii, tol_pair=1e-6):\n        N = len(radii)\n        if centers.shape != (N, 2):\n            return False\n        if np.any(radii < -1e-12):\n            return False\n        x = centers[:, 0]; y = centers[:, 1]\n        # boundary: x - r >= tol_pair, etc\n        if np.any(x - radii < tol_pair):\n            return False\n        if np.any(y - radii < tol_pair):\n            return False\n        if np.any(x + radii > 1.0 - tol_pair):\n            return False\n        if np.any(y + radii > 1.0 - tol_pair):\n            return False\n        d, _, _ = _pairwise_distances(centers)\n        iu = np.triu_indices(N, 1)\n        lhs = radii[:, None] + radii[None, :]\n        viol = lhs[iu] - d[iu] + tol_pair\n        if np.any(viol > 1e-12):\n            return False\n        return True\n\n    # ---------------------------\n    # Main multi-start optimization\n    # ---------------------------\n    N = 26\n    best = {'centers': None, 'radii': None, 'sum': -1.0}\n    inits = _multi_start_inits()\n    # Limit number of starts for runtime constraints:\n    max_starts = min(len(inits), 16)\n    starts = inits[:max_starts]\n    # Minimizer options\n    options = {'maxiter': 300, 'ftol': 1e-10}\n    seed_inc = 0\n    for idx, centers0 in enumerate(starts):\n        # parameterize initial centers via logit\n        # Avoid exact 0/1 by clipping\n        centers0 = np.clip(centers0, 1e-6, 1 - 1e-6)\n        u0 = np.log(centers0 / (1.0 - centers0))  # logit\n        # Add small deterministic perturbation in u-space based on idx\n        rng = np.random.default_rng(1000 + idx)\n        perturb = rng.normal(scale=0.15, size=u0.shape)\n        u0p = u0 + perturb\n        # Flatten u\n        u0_flat = u0p.ravel()\n        # Run L-BFGS-B unconstrained on u (we provide jac)\n        res = minimize(fun=lambda uu: _objective_and_grad_u(uu),\n                       x0=u0_flat,\n                       method='L-BFGS-B',\n                       jac=True,\n                       bounds=[(-60.0, 60.0)] * (2 * N),\n                       options=options)\n        u_opt = res.x\n        centers_opt = _sigmoid(u_opt.reshape((-1, 2)))\n        # Solve LP to maximize radii for these centers\n        r_lp = _lp_feasibilize(centers_opt, r_max=None, eps_pair=1e-6)\n        if not _verify(centers_opt, r_lp, tol_pair=1e-6):\n            # try slight shrink scaling fallback\n            r_try = 0.98 * r_lp\n            if _verify(centers_opt, r_try, tol_pair=1e-6):\n                r_lp = r_try\n        if _verify(centers_opt, r_lp, tol_pair=1e-6):\n            s = float(np.sum(r_lp))\n            if s > best['sum'] + 1e-12:\n                best['centers'] = centers_opt.copy()\n                best['radii'] = r_lp.copy()\n                best['sum'] = s\n        else:\n            # If LP failed to produce strictly feasible solution, attempt a scaled LP with cap\n            # Use LP fallback with cap from small base radii proportional to boundary\n            base_bound = np.minimum.reduce([centers_opt[:, 0], centers_opt[:, 1],\n                                           1.0 - centers_opt[:, 0], 1.0 - centers_opt[:, 1]])\n            r_guess = 0.2 * base_bound\n            r_cap = np.clip(1.2 * r_guess, 0.0, 0.5)\n            r_lp2 = _lp_feasibilize(centers_opt, r_max=r_cap, eps_pair=1e-6)\n            if _verify(centers_opt, r_lp2, tol_pair=1e-6):\n                s = float(np.sum(r_lp2))\n                if s > best['sum'] + 1e-12:\n                    best['centers'] = centers_opt.copy()\n                    best['radii'] = r_lp2.copy()\n                    best['sum'] = s\n        seed_inc += 1\n\n    # If none found strictly feasible, fallback to a baseline hex layout and LP\n    if best['centers'] is None:\n        centers_fb = _hex_initial_centers_from_rows([6, 5, 6, 5, 4], margin=0.06, jitter_scale=0.0, seed=42)\n        r_fb = _lp_feasibilize(centers_fb, r_max=None, eps_pair=1e-6)\n        if not _verify(centers_fb, r_fb, tol_pair=1e-6):\n            # try scaling a bit down\n            r_fb2 = 0.98 * r_fb\n            if _verify(centers_fb, r_fb2, tol_pair=1e-6):\n                r_fb = r_fb2\n            else:\n                # last resort: very small radii\n                r_fb = np.minimum(r_fb, 0.01)\n                if not _verify(centers_fb, r_fb, tol_pair=1e-6):\n                    # As absolute fallback, place 26 tiny circles in a grid\n                    coords = []\n                    g = int(np.ceil(np.sqrt(26)))\n                    xs = np.linspace(0.05, 0.95, g)\n                    ys = np.linspace(0.05, 0.95, g)\n                    for xi in xs:\n                        for yi in ys:\n                            coords.append([xi, yi])\n                    centers_fb = np.array(coords[:26])\n                    r_fb = np.full(26, 0.01)\n        best['centers'] = centers_fb\n        best['radii'] = r_fb\n        best['sum'] = float(np.sum(r_fb))\n\n    # Final strict LP hardening: attempt slightly stricter eps to guarantee strictness\n    final_r = _lp_feasibilize(best['centers'], r_max=None, eps_pair=1.1e-6)\n    if _verify(best['centers'], final_r, tol_pair=1e-6):\n        radii = final_r\n    else:\n        # If stricter LP failed, fall back to previous and scale down if necessary\n        radii = best['radii']\n        if not _verify(best['centers'], radii, tol_pair=1e-6):\n            # compute maximal scaling to satisfy constraints\n            d, _, _ = _pairwise_distances(best['centers'])\n            denom = radii[:, None] + radii[None, :]\n            with np.errstate(divide='ignore', invalid='ignore'):\n                ratio_pairs = np.where(denom > 1e-12, (d - 1e-6) / denom, np.inf)\n            iu = np.triu_indices(N, 1)\n            s_pairs = np.min(ratio_pairs[iu]) if ratio_pairs[iu].size > 0 else 1.0\n            if not np.isfinite(s_pairs):\n                s_pairs = 1.0\n            s_pairs = max(0.0, min(1.0, s_pairs))\n            bx = np.minimum.reduce([best['centers'][:, 0], best['centers'][:, 1],\n                                    1.0 - best['centers'][:, 0], 1.0 - best['centers'][:, 1]])\n            with np.errstate(divide='ignore', invalid='ignore'):\n                scale_b = np.min(np.where(radii > 1e-12, (bx - 1e-6) / radii, 1.0))\n            if not np.isfinite(scale_b):\n                scale_b = 1.0\n            scale_b = max(0.0, min(1.0, scale_b))\n            s = min(s_pairs, scale_b)\n            radii = np.clip(s * radii, 0.0, 0.5)\n\n    centers = best['centers'].astype(float)\n    radii = np.clip(radii, 0.0, 0.5).astype(float)\n    sum_radii = float(np.sum(radii))\n\n    # Final verification; if still not strictly feasible attempt to shrink slightly until feasible\n    if not _verify(centers, radii, tol_pair=1e-6):\n        # iterative shrink\n        shrink = 0.995\n        tries = 0\n        while tries < 40 and (not _verify(centers, radii, tol_pair=1e-6)):\n            radii *= shrink\n            tries += 1\n        sum_radii = float(np.sum(radii))\n\n    return centers, radii, sum_radii",
  "metrics": {
    "valid": true,
    "score": 2.1901174945129,
    "eval_time": 0.6368703842163086,
    "error": null
  },
  "prompt": "\nYou are an expert numerical optimizer. Produce ONLY a single Python code block with the function:\n\ndef construct_packing():\n    \"\"\"Return (centers, radii, sum_radii) with:\n        - centers: np.array shape (26, 2)\n        - radii:   np.array shape (26,)\n        - sum_radii: float, strictly feasible sum of radii\n    \"\"\"\n    ...\n\nRules:\n- Exactly 26 circles\n- Circles inside [0,1]^2\n- No overlaps: r_i + r_j <= dist(c_i,c_j) - 1e-6 (strict feasibility)\n- All radii non-negative\n- Use numpy, scipy only\n- No printing, no plots, no I/O\n- Runtime <= 300s\n- Return strictly feasible packing\n- Use a final LP feasibilization step to maximize sum(r) given fixed centers:\n  Maximize sum(r_i) subject to:\n    r_i >= 0\n    r_i <= x_i, y_i, 1-x_i, 1-y_i\n    r_i + r_j <= ||c_i - c_j||_2 - 1e-6 for all i<j\n- Vectorized math; stabilize divisions using eps ONLY in denominators (do not bias penalties).\n- Prefer L-BFGS-B or trust-constr/SLSQP with analytic gradients/Jacobians where applicable.\n- Seed with deterministic edge/corner-biased and hex-like layouts plus small jitter.\n- Multi-start (moderate count) and finalize with LP ensures strict feasibility.\n\nTitle: Unconstrained reparam: sigmoid for centers; softplus surrogate; L-BFGS-B + LP\n\n# Reference unconstrained style:\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.optimize import linprog\n\ndef _pairwise_distances(centers):\n    \"\"\"\n    Calculates pairwise Euclidean distances between centers.\n    Returns distance matrix, and matrices of dx and dy.\n    \"\"\"\n    diff = centers[:, None, :] - centers[None, :, :]\n    d = np.sqrt(np.sum(diff**2, axis=2))\n    return d, diff[..., 0], diff[..., 1]\n\ndef _objective_and_grad(z, N, P, w_b=1.0, w_o=1.0):\n    \"\"\"\n    Objective function and its gradient for joint optimization of centers (x,y) and radii (r).\n    z = [x_0..x_N-1, y_0..y_N-1, r_0..r_N-1]\n    Objective: -sum(r) + P * (w_b * boundary_penalty + w_o * overlap_penalty)\n    Penalties are squared hinge functions.\n    \"\"\"\n    x = z[:N]\n    y = z[N:2*N]\n    r = z[2*N:]\n    tiny = 1e-12 # Small value to prevent division by zero\n\n    # Boundary penalties: hinges for r - x, r - y, r + x - 1, r + y - 1\n    s1 = r - x\n    s2 = r - y\n    s3 = r + x - 1.0\n    s4 = r + y - 1.0\n\n    h1 = np.maximum(0.0, s1)\n    h2 = np.maximum(0.0, s2)\n    h3 = np.maximum(0.0, s3)\n    h4 = np.maximum(0.0, s4)\n\n    pen_b = 0.5 * np.sum(h1*h1 + h2*h2 + h3*h3 + h4*h4)\n\n    # Overlap penalties: hinges for r_i + r_j - d_ij\n    centers = np.stack([x, y], axis=1)\n    d, dx_mat, dy_mat = _pairwise_distances(centers)\n    iu = np.triu_indices(N, 1) # Upper triangle indices for unique pairs (i, j where i < j)\n    dab = d[iu] # Distances for unique pairs\n    dxab = dx_mat[iu]\n    dyab = dy_mat[iu]\n    d_safe = np.maximum(dab, tiny) # Use tiny to prevent division by zero in gradients\n    hij = r[iu[0]] + r[iu[1]] - dab # Overlap amount for each pair\n    hpos = np.maximum(0.0, hij) # Positive part of overlap (hinge)\n    pen_o = 0.5 * np.sum(hpos*hpos)\n\n    # Total objective function value\n    f = -np.sum(r) + P * (w_b * pen_b + w_o * pen_o)\n\n    # Gradients for boundary penalties\n    dr_b = h1 + h2 + h3 + h4\n    dx_b = -h1 + h3\n    dy_b = -h2 + h4\n\n    # Overlap gradients (accumulate with np.add.at for efficiency)\n    dx_o = np.zeros(N, dtype=float)\n    dy_o = np.zeros(N, dtype=float)\n    dr_o = np.zeros(N, dtype=float)\n\n    # Gradient contribution for centers (x, y)\n    # For a pair (a, b), the force on 'a' is in direction (c_b - c_a), force on 'b' is (c_a - c_b)\n    # grad_x_a += -h * (dxab/d), grad_x_b += +h * (dxab/d)\n    coeff = hpos / d_safe # h / d\n    cx = coeff * dxab\n    cy = coeff * dyab\n    np.add.at(dx_o, iu[0], -cx) # For circle i\n    np.add.at(dx_o, iu[1], +cx) # For circle j\n    np.add.at(dy_o, iu[0], -cy)\n    np.add.at(dy_o, iu[1], +cy)\n    \n    # Gradient contribution for radii (r)\n    # grad_r_a += h, grad_r_b += h\n    np.add.at(dr_o, iu[0], hpos)\n    np.add.at(dr_o, iu[1], hpos)\n\n    # Combine all gradients\n    dx_total = P * (w_b * dx_b + w_o * dx_o)\n    dy_total = P * (w_b * dy_b + w_o * dy_o)\n    dr_total = -1.0 + P * (w_b * dr_b + w_o * dr_o) # -1.0 from -sum(r)\n\n    grad = np.concatenate([dx_total, dy_total, dr_total])\n    return f, grad\n\ndef _objective_centers_only(z_xy, r_fixed, P, w_b=1.0, w_o=1.0):\n    \"\"\"\n    Objective function and its gradient for optimizing centers (x,y) only, with fixed radii.\n    z_xy = [x_0..x_N-1, y_0..y_N-1]\n    Objective: P * (w_b * boundary_penalty + w_o * overlap_penalty)\n    \"\"\"\n    N = r_fixed.shape[0]\n    x = z_xy[:N]\n    y = z_xy[N:2*N]\n    r = r_fixed\n    tiny = 1e-12\n\n    # Boundary hinges\n    s1 = r - x\n    s2 = r - y\n    s3 = r + x - 1.0\n    s4 = r + y - 1.0\n    h1 = np.maximum(0.0, s1)\n    h2 = np.maximum(0.0, s2)\n    h3 = np.maximum(0.0, s3)\n    h4 = np.maximum(0.0, s4)\n    pen_b = 0.5 * np.sum(h1*h1 + h2*h2 + h3*h3 + h4*h4)\n\n    # Overlap hinges\n    centers = np.stack([x, y], axis=1)\n    d, dx_mat, dy_mat = _pairwise_distances(centers)\n    iu = np.triu_indices(N, 1)\n    dab = d[iu]\n    dxab = dx_mat[iu]\n    dyab = dy_mat[iu]\n    d_safe = np.maximum(dab, tiny)\n    hij = r[iu[0]] + r[iu[1]] - dab\n    hpos = np.maximum(0.0, hij)\n    pen_o = 0.5 * np.sum(hpos*hpos)\n\n    f = P * (w_b * pen_b + w_o * pen_o)\n\n    # Gradients wrt x,y only\n    dx_b = -h1 + h3\n    dy_b = -h2 + h4\n\n    dx_o = np.zeros(N, dtype=float)\n    dy_o = np.zeros(N, dtype=float)\n    coeff = hpos / d_safe\n    cx = coeff * dxab\n    cy = coeff * dyab\n    np.add.at(dx_o, iu[0], -cx)\n    np.add.at(dx_o, iu[1], +cx)\n    np.add.at(dy_o, iu[0], -cy)\n    np.add.at(dy_o, iu[1], +cy)\n\n    dx_total = P * (w_b * dx_b + w_o * dx_o)\n    dy_total = P * (w_b * dy_b + w_o * dy_o)\n    g = np.concatenate([dx_total, dy_total])\n    return f, g\n\ndef _lp_feasibilize(centers, r_max=None, eps_pair=1e-6):\n    \"\"\"\n    Maximizes sum(r) for given centers using linear programming, ensuring feasibility.\n    Constraints:\n      r_i >= 0\n      r_i <= r_max[i] (if provided)\n      r_i <= x_i - eps_pair, r_i <= y_i - eps_pair, r_i <= 1-x_i - eps_pair, r_i <= 1-y_i - eps_pair\n      r_i + r_j <= dist(c_i,c_j) - eps_pair\n    Includes a retry mechanism if the initial LP with eps_pair is infeasible.\n    \"\"\"\n    N = centers.shape[0]\n    x = centers[:, 0]\n    y = centers[:, 1]\n\n    A_rows = []\n    b_vals = []\n\n    # Boundary constraints: r_i <= c_k - eps_pair\n    for i in range(N):\n        e = np.zeros(N); e[i] = 1.0\n        A_rows.append(e.copy()); b_vals.append(x[i] - eps_pair)\n        A_rows.append(e.copy()); b_vals.append(y[i] - eps_pair)\n        A_rows.append(e.copy()); b_vals.append(1.0 - x[i] - eps_pair)\n        A_rows.append(e.copy()); b_vals.append(1.0 - y[i] - eps_pair)\n\n    # Pairwise overlap constraints: r_i + r_j <= dist(c_i,c_j) - eps_pair\n    d, _, _ = _pairwise_distances(centers)\n    for i in range(N):\n        for j in range(i+1, N):\n            e = np.zeros(N); e[i] = 1.0; e[j] = 1.0\n            A_rows.append(e); b_vals.append(d[i, j] - eps_pair)\n\n    # Optional upper bounds r_i <= r_max[i]\n    if r_max is not None:\n        for i in range(N):\n            e = np.zeros(N); e[i] = 1.0\n            A_rows.append(e); b_vals.append(r_max[i])\n\n    A_ub = np.array(A_rows)\n    b_ub = np.array(b_vals)\n\n    # Radii bounds: [0, 0.5] (max possible radius in unit square)\n    bounds = [(0.0, 0.5) for _ in range(N)]\n    c = -np.ones(N) # Objective for linprog is to minimize c.T @ x, so -sum(r) means maximize sum(r)\n\n    res = linprog(c, A_ub=A_ub, b_ub=b_ub, bounds=bounds, method='highs')\n    if res.success:\n        return np.clip(res.x, 0.0, 0.5)\n\n    # Fallback: If LP with eps_pair is infeasible, try with eps_pair = 0 to get a base solution,\n    # then scale radii down to satisfy the eps_pair.\n    b_vals2 = []\n    A_rows2 = []\n    for i in range(N):\n        e = np.zeros(N); e[i] = 1.0\n        A_rows2.append(e.copy()); b_vals2.append(x[i])\n        A_rows2.append(e.copy()); b_vals2.append(y[i])\n        A_rows2.append(e.copy()); b_vals2.append(1.0 - x[i])\n        A_rows2.append(e.copy()); b_vals2.append(1.0 - y[i])\n\n    for i in range(N):\n        for j in range(i+1, N):\n            e = np.zeros(N); e[i] = 1.0; e[j] = 1.0\n            A_rows2.append(e); b_vals2.append(d[i, j])\n\n    if r_max is not None:\n        for i in range(N):\n            e = np.zeros(N); e[i] = 1.0\n            A_rows2.append(e); b_vals2.append(r_max[i])\n\n    A_ub2 = np.array(A_rows2)\n    b_ub2 = np.array(b_vals2)\n\n    res2 = linprog(c, A_ub=A_ub2, b_ub=b_ub2, bounds=bounds, method='highs')\n    if not res2.success:\n        return np.zeros(N) # If even without epsilon, no solution, return zero radii\n\n    r_base = np.clip(res2.x, 0.0, 0.5)\n    \n    # Scale r_base to satisfy eps_pair for both pairwise and boundary constraints\n    d, _, _ = _pairwise_distances(centers)\n    \n    # Scaling factor for pairwise overlaps: s * (r_i + r_j) <= d_ij - eps_pair\n    denom_pairs = r_base[:, None] + r_base[None, :]\n    with np.errstate(divide='ignore', invalid='ignore'):\n        ratio_pairs = np.where(denom_pairs > 1e-9, (d - eps_pair) / denom_pairs, np.inf)\n    iu = np.triu_indices(N, 1)\n    s_pairs = np.min(ratio_pairs[iu]) if ratio_pairs[iu].size > 0 else 1.0\n    if not np.isfinite(s_pairs): s_pairs = 1.0\n    s_pairs = max(0.0, min(1.0, s_pairs))\n\n    # Scaling factor for boundary constraints: s * r_i <= c_k - eps_pair\n    with np.errstate(divide='ignore', invalid='ignore'):\n        s_boundary_x_min = np.where(r_base > 1e-9, (x - eps_pair) / r_base, np.inf)\n        s_boundary_y_min = np.where(r_base > 1e-9, (y - eps_pair) / r_base, np.inf)\n        s_boundary_x_max = np.where(r_base > 1e-9, (1.0 - x - eps_pair) / r_base, np.inf)\n        s_boundary_y_max = np.where(r_base > 1e-9, (1.0 - y - eps_pair) / r_base, np.inf)\n\n    s_boundary = np.min([s_boundary_x_min, s_boundary_y_min, s_boundary_x_max, s_boundary_y_max])\n    if not np.isfinite(s_boundary): s_boundary = 1.0\n    s_boundary = max(0.0, min(1.0, s_boundary))\n\n    s = min(s_pairs, s_boundary)\n    r = s * r_base\n    return np.clip(r, 0.0, 0.5)\n\ndef _verify(centers, radii, tol_pair=1e-6):\n    \"\"\"\n    Verifies if a given packing is strictly feasible within the unit square\n    and without overlaps, considering a tolerance.\n    \"\"\"\n    N = len(radii)\n    x = centers[:, 0]\n    y = centers[:, 1]\n    \n    if centers.shape != (N, 2): return False\n    if np.any(radii < -1e-12): return False # Radii must be non-negative\n\n    # Check boundary constraints: r_i <= c_k - tol_pair\n    if np.any(x - radii < tol_pair): return False # x_i - r_i >= tol_pair\n    if np.any(y - radii < tol_pair): return False # y_i - r_i >= tol_pair\n    if np.any(x + radii > 1.0 - tol_pair): return False # x_i + r_i <= 1 - tol_pair\n    if np.any(y + radii > 1.0 - tol_pair): return False # y_i + r_i <= 1 - tol_pair\n    \n    # Check pairwise overlap constraints: r_i + r_j <= dist(c_i,c_j) - tol_pair\n    d, _, _ = _pairwise_distances(centers)\n    iu = np.triu_indices(N, 1)\n    lhs = radii[:, None] + radii[None, :]\n    viol = lhs[iu] - d[iu] + tol_pair # Should be <= 0 for no overlap\n    if np.any(viol > 1e-12): return False # Allow a tiny buffer for float precision\n    \n    return True\n\ndef _hex_initial_centers_from_rows(rows, margin=0.05, jitter_scale=0.03, seed=0):\n    \"\"\"\n    Generates initial centers in a hexagonal-like grid pattern.\n    Rows is a list of circle counts per row, e.g., [6,5,6,5,4].\n    \"\"\"\n    assert sum(rows) == 26\n    max_cols = max(rows)\n    nrows = len(rows)\n    \n    # Calculate base spacing based on max columns and rows\n    dx_base = (1.0 - 2*margin) / (max_cols - 1) if max_cols > 1 else 0.0\n    dy_base = (1.0 - 2*margin) / (nrows - 1) if nrows > 1 else 0.0\n\n    centers = []\n    rng = np.random.default_rng(seed)\n    for ri, cols in enumerate(rows):\n        y = margin + ri * dy_base if nrows > 1 else 0.5\n        \n        # Calculate starting x to center the row, applying hex offset for alternating rows\n        if cols > 1:\n            row_width = (cols - 1) * dx_base\n            startx = 0.5 - row_width / 2.0\n            if ri % 2 == 1: # Apply hex offset\n                startx += dx_base / 2.0\n        else: # Single circle in row, center it\n            startx = 0.5\n\n        for k in range(cols):\n            x = startx + (k * dx_base if cols > 1 else 0.0)\n            centers.append([x, y])\n    centers = np.array(centers)\n    \n    # Add jitter and clip to ensure centers are within bounds\n    jitter = rng.normal(scale=jitter_scale, size=centers.shape)\n    centers = centers + jitter\n    centers = np.clip(centers, 1e-3, 1 - 1e-3) # Keep centers slightly away from boundary\n    return centers\n\ndef _multi_start_inits(N):\n    \"\"\"\n    Generates a list of diverse initial center configurations using various\n    hexagonal patterns, margins, and jitter scales.\n    \"\"\"\n    # Various row patterns summing to 26\n    patterns = [\n        [6, 5, 6, 5, 4], [6, 5, 5, 5, 5], [5, 6, 5, 5, 5],\n        [5, 5, 6, 5, 5], [6, 6, 5, 5, 4], [5, 6, 6, 5, 4],\n        [6, 5, 6, 4, 5], [5, 5, 5, 5, 6], [4, 5, 6, 5, 6],\n        [5, 4, 5, 6, 6], [5, 6, 4, 6, 5],\n    ]\n    # Various margin and jitter scale parameters\n    params = [\n        (0.03, 0.02), (0.04, 0.03), (0.05, 0.03),\n        (0.06, 0.02), (0.07, 0.03), (0.05, 0.04),\n        (0.04, 0.025), (0.055, 0.035), (0.065, 0.025)\n    ]\n    inits = []\n    seed = 0\n    for pat in patterns:\n        for (m, j) in params:\n            inits.append(_hex_initial_centers_from_rows(pat, margin=m, jitter_scale=j, seed=seed))\n            seed += 1\n    return inits\n\ndef _center_only_relax(centers, r_fixed, P=2000.0, w_b=1.0, w_o=1.0, iters=80):\n    \"\"\"\n    Performs a short optimization run on centers only, with fixed radii,\n    to relax the configuration and potentially open up space.\n    \"\"\"\n    N = centers.shape[0]\n    z0 = np.concatenate([centers[:, 0], centers[:, 1]])\n    bounds_xy = [(0.0, 1.0)] * (2 * N) # Centers must stay within [0,1]\n    res = minimize(\n        fun=lambda zz: _objective_centers_only(zz, r_fixed, P, w_b, w_o),\n        x0=z0,\n        method='L-BFGS-B',\n        jac=True,\n        bounds=bounds_xy,\n        options={'maxiter': iters, 'ftol': 1e-12, 'gtol': 1e-8, 'maxcor': 20}\n    )\n    z = res.x\n    centers_new = np.stack([z[:N], z[N:2*N]], axis=1)\n    return centers_new\n\ndef construct_packing():\n    N = 26\n    best = {'centers': None, 'radii': None, 'sum': -np.inf}\n\n    # Penalty annealing schedule and max iterations per stage\n    P_schedule = [8.0, 80.0, 800.0, 3000.0, 10000.0, 20000.0] # Increasing penalty\n    maxiter_stage = [90, 100, 110, 120, 130, 140] # Iterations per stage\n    bounds = [(0.0, 1.0)] * (2*N) + [(0.0, 0.5)] * N # Bounds for x, y, r\n\n    w_b = 1.0 # Weight for boundary penalty\n    w_o = 1.0 # Weight for overlap penalty\n\n    # Generate multiple initial configurations\n    inits = _multi_start_inits(N)\n    if not inits: # Fallback if initializations list is empty\n        inits = [_hex_initial_centers_from_rows([6,5,6,5,4], margin=0.05, jitter_scale=0.03, seed=42)]\n\n    for init_idx, centers0 in enumerate(inits):\n        x0 = centers0[:, 0]\n        y0 = centers0[:, 1]\n        # Initialize radii to a small value, respecting boundary constraints\n        base_bound = np.minimum.reduce([x0, y0, 1 - x0, 1 - y0])\n        r0 = 0.05 * base_bound\n        z = np.concatenate([x0, y0, r0])\n\n        # Annealing optimization loop\n        for si, P in enumerate(P_schedule):\n            # Stage 1: Joint optimization of (x, y, r) using penalty method\n            res = minimize(\n                fun=lambda zz: _objective_and_grad(zz, N, P, w_b, w_o),\n                x0=z,\n                method='L-BFGS-B',\n                jac=True,\n                bounds=bounds,\n                options={'maxiter': maxiter_stage[si], 'ftol': 1e-12, 'gtol': 1e-8, 'maxcor': 30}\n            )\n            z = res.x\n            centers_tmp = np.stack([z[:N], z[N:2*N]], axis=1)\n            r_cur = np.clip(z[2*N:], 0.0, 0.5)\n            \n            # Stage 2: LP Feasibilization with a radius cap\n            # The cap prevents radii from growing too aggressively too early, allowing centers to adjust.\n            cap_factor = 1.08 if si < len(P_schedule) - 1 else 1.15\n            r_cap = np.minimum(0.5, cap_factor * np.maximum(1e-9, r_cur))\n            r_lp = _lp_feasibilize(centers_tmp, r_max=r_cap, eps_pair=1e-6)\n            \n            # Stage 3: Center-only relaxation\n            # Move centers slightly with fixed (LP-derived) radii to open up space.\n            centers_relaxed = _center_only_relax(centers_tmp, r_lp, P=max(1000.0, P), w_b=w_b, w_o=w_o, iters=70)\n            \n            # Stage 4: Final LP for this stage (no cap)\n            # Maximize radii again for the relaxed centers.\n            r_lp2 = _lp_feasibilize(centers_relaxed, r_max=None, eps_pair=1e-6)\n            \n            # Update the state (z) for the next annealing stage\n            z = np.concatenate([centers_relaxed[:, 0], centers_relaxed[:, 1], np.clip(r_lp2, 0.0, 0.5)])\n\n        # After all annealing stages, perform a final LP to get the best possible radii\n        centers_final_opt = np.stack([z[:N], z[N:2*N]], axis=1)\n        r_final = _lp_feasibilize(centers_final_opt, r_max=None, eps_pair=1e-6)\n\n        # Verify the solution and update the best found packing\n        if _verify(centers_final_opt, r_final, tol_pair=1e-6):\n            s = float(np.sum(r_final))\n            if s > best['sum']:\n                best['centers'] = centers_final_opt.copy()\n                best['radii'] = r_final.copy()\n                best['sum'] = s\n        else:\n            # If the solution is not strictly feasible, try to scale down radii\n            # to achieve feasibility. This is a \"hardening\" step.\n            d, _, _ = _pairwise_distances(centers_final_opt)\n            denom = r_final[:, None] + r_final[None, :]\n            with np.errstate(divide='ignore', invalid='ignore'):\n                ratio_pairs = np.where(denom > 1e-9, (d - 1.0e-6) / denom, np.inf)\n            iu = np.triu_indices(N, 1)\n            s_pairs = np.min(ratio_pairs[iu]) if ratio_pairs[iu].size > 0 else 1.0\n            if not np.isfinite(s_pairs): s_pairs = 1.0\n            s_pairs = max(0.0, min(1.0, s_pairs))\n            \n            bx = np.minimum.reduce([centers_final_opt[:,0], centers_final_opt[:,1], 1-centers_final_opt[:,0], 1-centers_final_opt[:,1]])\n            with np.errstate(divide='ignore', invalid='ignore'):\n                scale_b = np.min(np.where(r_final > 1e-9, (bx - 1e-6) / r_final, 1.0))\n            scale_b = min(1.0, scale_b if np.isfinite(scale_b) else 1.0)\n            scale_b = max(0.0, scale_b)\n\n            r_try = min(s_pairs, scale_b) * r_final\n            \n            if _verify(centers_final_opt, r_try, tol_pair=1e-6):\n                s = float(np.sum(r_try))\n                if s > best['sum']:\n                    best['centers'] = centers_final_opt.copy()\n                    best['radii'] = r_try.copy()\n                    best['sum'] = s\n\n    # Fallback mechanism: if no valid solution was found after all multi-starts,\n    # generate a simple hexagonal pattern and run LP on it.\n    if best['centers'] is None:\n        centers_fb = _hex_initial_centers_from_rows([6,5,6,5,4], margin=0.06, jitter_scale=0.02, seed=123)\n        r_fb = _lp_feasibilize(centers_fb, r_max=None, eps_pair=1e-6)\n        if not _verify(centers_fb, r_fb, tol_pair=1e-6):\n            r_fb = 0.95 * r_fb # Shrink a bit more if still not feasible\n        best['centers'] = centers_fb\n        best['radii'] = np.clip(r_fb, 0.0, 0.5)\n        best['sum'] = float(np.sum(best['radii']))\n\n    # Final hardening step to guarantee strict feasibility for the returned solution.\n    # Re-run LP with a slightly stricter epsilon.\n    radii_strict = _lp_feasibilize(best['centers'], r_max=None, eps_pair=1.1e-6)\n    if _verify(best['centers'], radii_strict, tol_pair=1e-6):\n        radii = radii_strict\n    else:\n        # If stricter LP fails, use the best found radii and apply a final scaling if needed.\n        radii = best['radii']\n        if not _verify(best['centers'], radii, tol_pair=1e-6):\n            d, _, _ = _pairwise_distances(best['centers'])\n            denom = radii[:, None] + radii[None, :]\n            with np.errstate(divide='ignore', invalid='ignore'):\n                ratio_pairs = np.where(denom > 1e-9, (d - 1e-6) / denom, np.inf)\n            iu = np.triu_indices(N, 1)\n            s_pairs = np.min(ratio_pairs[iu]) if ratio_pairs[iu].size > 0 else 1.0\n            s_pairs = min(1.0, s_pairs if np.isfinite(s_pairs) else 1.0)\n            s_pairs = max(0.0, s_pairs)\n            \n            bx = np.minimum.reduce([best['centers'][:,0], best['centers'][:,1], 1-best['centers'][:,0], 1-best['centers'][:,1]])\n            with np.errstate(divide='ignore', invalid='ignore'):\n                scale_b = np.min(np.where(radii > 1e-9, (bx - 1e-6) / radii, 1.0))\n            scale_b = min(1.0, scale_b if np.isfinite(scale_b) else 1.0)\n            scale_b = max(0.0, scale_b)\n            \n            radii = min(s_pairs, scale_b) * radii\n\n    centers = best['centers']\n    radii = np.clip(radii, 0.0, 0.5) # Final clip to ensure radii are non-negative and within max possible\n    sum_radii = float(np.sum(radii))\n    return centers.astype(float), radii.astype(float), sum_radii\n\ndef run_packing():\n    \"\"\"\n    Entry point for internal testing or direct execution.\n    \"\"\"\n    centers, radii, sum_radii = construct_packing()\n    return centers, radii, sum_radii\n\n# Implement as follows:\n\nImplementation guidance:\n- Penalized objective for centers (and optionally radii) with:\n  * Overlap penalty: sum(max(0, r_i+r_j - d_ij)^2), where d_ij = ||c_i-c_j||, eps only in denom.\n  * Boundary penalty: sum(max(0, r_i - b_i)^2) where b_i = min(x_i, y_i, 1-x_i, 1-y_i).\n- Prefer center-only optimization; always finalize radii via LP.\n- Adaptive/density weights: weight pairs by local density (e.g., based on k-NN distances).\n- Anneal penalties over stages; warm-start best centers between stages.\n- Use neighbor masks to reduce O(n^2) cost where safe (still maintain correctness).\n- Ensure strict feasibility via LP with -1e-6 margins on pair constraints.\n\n\n- Parameterize centers via sigmoid to keep them in (0,1); optimize only centers (radii via LP).\n- Use robust smooth penalties; eps only in denominators; anneal stages.\n- 16 multi-starts; choose best via LP.\n- Return strictly feasible LP result.\n",
  "response": "```python\nimport numpy as np\nfrom scipy.optimize import minimize, linprog\n\ndef construct_packing():\n    \"\"\"\n    Return (centers, radii, sum_radii) with:\n        - centers: np.array shape (26, 2)\n        - radii:   np.array shape (26,)\n        - sum_radii: float, strictly feasible sum of radii\n    Strategy:\n      - Generate multiple deterministic hex-like initial center layouts.\n      - Parameterize centers via unconstrained variables u; centers = sigmoid(u).\n      - Optimize centers-only with a smooth repulsive objective:\n          sum_{i<j} exp(-k * d_ij) + sum_i exp(-k * b_i)\n        where b_i is a smooth-min distance to the four boundaries.\n      - After each local optimization, solve an LP to maximize radii for fixed centers\n        with strict eps_pair = 1e-6 to ensure non-overlap and boundary containment.\n      - Keep the best strictly feasible packing found.\n    \"\"\"\n    # ---------------------------\n    # Helper utilities\n    # ---------------------------\n    def _sigmoid(u):\n        # Numerically stable sigmoid\n        # Clip inputs to avoid overflow in exp\n        uu = np.clip(u, -60.0, 60.0)\n        s = 1.0 / (1.0 + np.exp(-uu))\n        # Keep strictly inside (1e-9, 1-1e-9) to avoid degenerate LP bounds\n        return np.clip(s, 1e-9, 1 - 1e-9)\n\n    def _sigmoid_grad(u):\n        s = _sigmoid(u)\n        return s * (1.0 - s)\n\n    def _pairwise_distances(centers):\n        diff = centers[:, None, :] - centers[None, :, :]\n        d = np.sqrt(np.sum(diff ** 2, axis=2))\n        return d, diff[..., 0], diff[..., 1]\n\n    def _hex_initial_centers_from_rows(rows, margin=0.05, jitter_scale=0.015, seed=0):\n        \"\"\"\n        Deterministic hex-like pattern with small jitter.\n        rows: list of ints summing to 26\n        \"\"\"\n        assert sum(rows) == 26\n        max_cols = max(rows)\n        nrows = len(rows)\n        dx_base = (1.0 - 2 * margin) / (max_cols - 1) if max_cols > 1 else 0.0\n        dy_base = (1.0 - 2 * margin) / (nrows - 1) if nrows > 1 else 0.0\n        centers = []\n        rng = np.random.default_rng(seed)\n        for ri, cols in enumerate(rows):\n            y = margin + ri * dy_base if nrows > 1 else 0.5\n            if cols > 1:\n                row_width = (cols - 1) * dx_base\n                startx = 0.5 - row_width / 2.0\n                if ri % 2 == 1:\n                    startx += dx_base / 2.0\n            else:\n                startx = 0.5\n            for k in range(cols):\n                x = startx + (k * dx_base if cols > 1 else 0.0)\n                centers.append([x, y])\n        centers = np.array(centers, dtype=float)\n        # Add small deterministic jitter using RNG but seed fixed\n        jitter = rng.normal(scale=jitter_scale, size=centers.shape)\n        centers = centers + jitter\n        centers = np.clip(centers, 1e-6, 1 - 1e-6)\n        return centers\n\n    def _multi_start_inits():\n        patterns = [\n            [6, 5, 6, 5, 4], [6, 5, 5, 5, 5], [5, 6, 5, 5, 5],\n            [5, 5, 6, 5, 5], [6, 6, 5, 5, 4], [5, 6, 6, 5, 4],\n            [6, 5, 6, 4, 5], [5, 5, 5, 5, 6], [4, 5, 6, 5, 6],\n            [5, 4, 5, 6, 6], [5, 6, 4, 6, 5],\n        ]\n        params = [\n            (0.04, 0.012), (0.05, 0.015), (0.06, 0.018),\n            (0.045, 0.02), (0.035, 0.01), (0.055, 0.014)\n        ]\n        inits = []\n        seed = 1\n        for pat in patterns:\n            for (m, j) in params:\n                inits.append(_hex_initial_centers_from_rows(pat, margin=m, jitter_scale=j, seed=seed))\n                seed += 1\n        # Ensure at least some additional structured edge-corner biased initialization\n        # corners + edges + interior small circles as a fallback arrangement\n        if len(inits) < 16:\n            extra = _hex_initial_centers_from_rows([6,5,6,5,4], margin=0.07, jitter_scale=0.01, seed=999)\n            inits.append(extra)\n        return inits\n\n    def _smooth_min_four(x, y):\n        # Returns b = smooth-min(x, y, 1-x, 1-y) and gradients wrt x,y\n        # b = -1/gamma * log(sum_j exp(-gamma * t_j))\n        gamma = 80.0\n        t1 = x\n        t2 = y\n        t3 = 1.0 - x\n        t4 = 1.0 - y\n        T = np.stack([ -gamma * t1, -gamma * t2, -gamma * t3, -gamma * t4 ], axis=0)\n        # To stabilize log-sum-exp:\n        m = np.max(T, axis=0)\n        S = np.exp(T - m)\n        denom = np.sum(S, axis=0)\n        logsum = np.log(denom) + m\n        b = - (1.0 / gamma) * logsum\n        # probabilities for each term p_j = exp(-gamma t_j) / sum\n        p = S / denom\n        # derivative db/dx = sum_j p_j * dt_j/dx\n        # dt/dx = [1, 0, -1, 0] for [x, y, 1-x, 1-y]\n        db_dx = p[0] * 1.0 + p[2] * (-1.0)\n        db_dy = p[1] * 1.0 + p[3] * (-1.0)\n        return b, db_dx, db_dy\n\n    def _objective_and_grad_u(u):\n        \"\"\"\n        centers parameterized by u: c = sigmoid(u)\n        Objective: sum_{i<j} exp(-k * d_ij) + sum_i exp(-k * b_i)\n        Minimization of this objective encourages spread and staying away from boundaries.\n        Returns scalar f and gradient wrt u (len 2N).\n        \"\"\"\n        N = 26\n        centers = _sigmoid(u.reshape((-1, 2)))\n        x = centers[:, 0]\n        y = centers[:, 1]\n        # Pairwise distances and diff\n        d, dx_mat, dy_mat = _pairwise_distances(centers)\n        iu = np.triu_indices(N, 1)\n        dab = d[iu]\n        dxab = dx_mat[iu]\n        dyab = dy_mat[iu]\n        # Parameters\n        k = 28.0  # repulsion strength\n        tiny = 1e-12\n        # Pairwise repulsive terms: exp(-k * d)\n        exp_pairs = np.exp(-k * dab)\n        f_pairs = np.sum(exp_pairs)\n        # Gradients wrt centers for pairwise\n        # d/ x_a derivative: (x_a - x_b)/d = -dxab/d (since dxab = xa - xb)\n        d_safe = np.maximum(dab, tiny)\n        coeff = -k * exp_pairs / d_safe  # scalar per pair multiply (xa-xb)\n        gx = np.zeros(N, dtype=float)\n        gy = np.zeros(N, dtype=float)\n        # For each pair (i,j) at positions iu[0], iu[1]:\n        iidx = iu[0]; jidx = iu[1]\n        # contribution to grad_x_i: coeff * dxab\n        np.add.at(gx, iidx, coeff * dxab)\n        np.add.at(gx, jidx, -coeff * dxab)\n        np.add.at(gy, iidx, coeff * dyab)\n        np.add.at(gy, jidx, -coeff * dyab)\n        # Boundary smooth-min b_i and repulsion exp(-k*b_i)\n        b, db_dx, db_dy = _smooth_min_four(x, y)\n        exp_bound = np.exp(-k * b)\n        f_bound = np.sum(exp_bound)\n        # grad wrt x,y from boundary term: -k * exp(-k*b) * db_dx\n        gx += (-k * exp_bound) * db_dx\n        gy += (-k * exp_bound) * db_dy\n        f = f_pairs + f_bound\n        # Chain rule to u via sigmoid derivative\n        s_grad = _sigmoid_grad(u.reshape((-1, 2)))\n        gu = np.empty_like(u).reshape((-1, 2))\n        gu[:, 0] = gx * s_grad[:, 0]\n        gu[:, 1] = gy * s_grad[:, 1]\n        return float(f), gu.ravel()\n\n    def _lp_feasibilize(centers, r_max=None, eps_pair=1e-6):\n        \"\"\"\n        LP maximize sum r subject to:\n          0 <= r_i <= 0.5\n          r_i <= x_i - eps_pair\n          r_i <= y_i - eps_pair\n          r_i <= 1-x_i - eps_pair\n          r_i <= 1-y_i - eps_pair\n          r_i + r_j <= dist(c_i,c_j) - eps_pair\n        If infeasible with eps_pair, fallback to eps=0 then scale down radii to satisfy eps_pair.\n        \"\"\"\n        N = centers.shape[0]\n        x = centers[:, 0]\n        y = centers[:, 1]\n        # Build constraints\n        A_rows = []\n        b_vals = []\n        # boundary constraints\n        for i in range(N):\n            e = np.zeros(N, dtype=float); e[i] = 1.0\n            A_rows.append(e.copy()); b_vals.append(x[i] - eps_pair)\n            A_rows.append(e.copy()); b_vals.append(y[i] - eps_pair)\n            A_rows.append(e.copy()); b_vals.append(1.0 - x[i] - eps_pair)\n            A_rows.append(e.copy()); b_vals.append(1.0 - y[i] - eps_pair)\n        # pairwise\n        d, _, _ = _pairwise_distances(centers)\n        for i in range(N):\n            for j in range(i+1, N):\n                e = np.zeros(N, dtype=float); e[i] = 1.0; e[j] = 1.0\n                A_rows.append(e); b_vals.append(d[i, j] - eps_pair)\n        if r_max is not None:\n            for i in range(N):\n                e = np.zeros(N, dtype=float); e[i] = 1.0\n                A_rows.append(e); b_vals.append(min(0.5, float(r_max[i])))\n        A_ub = np.array(A_rows, dtype=float)\n        b_ub = np.array(b_vals, dtype=float)\n        bounds = [(0.0, 0.5) for _ in range(N)]\n        c = -np.ones(N, dtype=float)  # minimize c^T r = -sum r => maximize sum r\n        res = linprog(c, A_ub=A_ub, b_ub=b_ub, bounds=bounds, method='highs')\n        if res.success:\n            return np.clip(res.x, 0.0, 0.5)\n        # Fallback: try without eps_pair\n        A_rows2 = []\n        b_vals2 = []\n        for i in range(N):\n            e = np.zeros(N, dtype=float); e[i] = 1.0\n            A_rows2.append(e.copy()); b_vals2.append(x[i])\n            A_rows2.append(e.copy()); b_vals2.append(y[i])\n            A_rows2.append(e.copy()); b_vals2.append(1.0 - x[i])\n            A_rows2.append(e.copy()); b_vals2.append(1.0 - y[i])\n        for i in range(N):\n            for j in range(i+1, N):\n                e = np.zeros(N, dtype=float); e[i] = 1.0; e[j] = 1.0\n                A_rows2.append(e); b_vals2.append(d[i, j])\n        if r_max is not None:\n            for i in range(N):\n                e = np.zeros(N, dtype=float); e[i] = 1.0\n                A_rows2.append(e); b_vals2.append(min(0.5, float(r_max[i])))\n        A_ub2 = np.array(A_rows2, dtype=float)\n        b_ub2 = np.array(b_vals2, dtype=float)\n        res2 = linprog(c, A_ub=A_ub2, b_ub=b_ub2, bounds=bounds, method='highs')\n        if not res2.success:\n            return np.zeros(N, dtype=float)\n        r_base = np.clip(res2.x, 0.0, 0.5)\n        # Scale down r_base to satisfy eps_pair\n        denom_pairs = r_base[:, None] + r_base[None, :]\n        with np.errstate(divide='ignore', invalid='ignore'):\n            ratio_pairs = np.where(denom_pairs > 1e-12, (d - eps_pair) / denom_pairs, np.inf)\n        iu = np.triu_indices(N, 1)\n        s_pairs = np.min(ratio_pairs[iu]) if ratio_pairs[iu].size > 0 else 1.0\n        if not np.isfinite(s_pairs):\n            s_pairs = 1.0\n        s_pairs = max(0.0, min(1.0, s_pairs))\n        # boundary\n        with np.errstate(divide='ignore', invalid='ignore'):\n            s_x_min = np.where(r_base > 1e-12, (x - eps_pair) / r_base, np.inf)\n            s_y_min = np.where(r_base > 1e-12, (y - eps_pair) / r_base, np.inf)\n            s_x_max = np.where(r_base > 1e-12, (1.0 - x - eps_pair) / r_base, np.inf)\n            s_y_max = np.where(r_base > 1e-12, (1.0 - y - eps_pair) / r_base, np.inf)\n        s_boundary = np.min([s_x_min, s_y_min, s_x_max, s_y_max])\n        if not np.isfinite(s_boundary):\n            s_boundary = 1.0\n        s_boundary = max(0.0, min(1.0, s_boundary))\n        s = min(s_pairs, s_boundary)\n        r_scaled = s * r_base\n        return np.clip(r_scaled, 0.0, 0.5)\n\n    def _verify(centers, radii, tol_pair=1e-6):\n        N = len(radii)\n        if centers.shape != (N, 2):\n            return False\n        if np.any(radii < -1e-12):\n            return False\n        x = centers[:, 0]; y = centers[:, 1]\n        # boundary: x - r >= tol_pair, etc\n        if np.any(x - radii < tol_pair):\n            return False\n        if np.any(y - radii < tol_pair):\n            return False\n        if np.any(x + radii > 1.0 - tol_pair):\n            return False\n        if np.any(y + radii > 1.0 - tol_pair):\n            return False\n        d, _, _ = _pairwise_distances(centers)\n        iu = np.triu_indices(N, 1)\n        lhs = radii[:, None] + radii[None, :]\n        viol = lhs[iu] - d[iu] + tol_pair\n        if np.any(viol > 1e-12):\n            return False\n        return True\n\n    # ---------------------------\n    # Main multi-start optimization\n    # ---------------------------\n    N = 26\n    best = {'centers': None, 'radii': None, 'sum': -1.0}\n    inits = _multi_start_inits()\n    # Limit number of starts for runtime constraints:\n    max_starts = min(len(inits), 16)\n    starts = inits[:max_starts]\n    # Minimizer options\n    options = {'maxiter': 300, 'ftol': 1e-10}\n    seed_inc = 0\n    for idx, centers0 in enumerate(starts):\n        # parameterize initial centers via logit\n        # Avoid exact 0/1 by clipping\n        centers0 = np.clip(centers0, 1e-6, 1 - 1e-6)\n        u0 = np.log(centers0 / (1.0 - centers0))  # logit\n        # Add small deterministic perturbation in u-space based on idx\n        rng = np.random.default_rng(1000 + idx)\n        perturb = rng.normal(scale=0.15, size=u0.shape)\n        u0p = u0 + perturb\n        # Flatten u\n        u0_flat = u0p.ravel()\n        # Run L-BFGS-B unconstrained on u (we provide jac)\n        res = minimize(fun=lambda uu: _objective_and_grad_u(uu),\n                       x0=u0_flat,\n                       method='L-BFGS-B',\n                       jac=True,\n                       bounds=[(-60.0, 60.0)] * (2 * N),\n                       options=options)\n        u_opt = res.x\n        centers_opt = _sigmoid(u_opt.reshape((-1, 2)))\n        # Solve LP to maximize radii for these centers\n        r_lp = _lp_feasibilize(centers_opt, r_max=None, eps_pair=1e-6)\n        if not _verify(centers_opt, r_lp, tol_pair=1e-6):\n            # try slight shrink scaling fallback\n            r_try = 0.98 * r_lp\n            if _verify(centers_opt, r_try, tol_pair=1e-6):\n                r_lp = r_try\n        if _verify(centers_opt, r_lp, tol_pair=1e-6):\n            s = float(np.sum(r_lp))\n            if s > best['sum'] + 1e-12:\n                best['centers'] = centers_opt.copy()\n                best['radii'] = r_lp.copy()\n                best['sum'] = s\n        else:\n            # If LP failed to produce strictly feasible solution, attempt a scaled LP with cap\n            # Use LP fallback with cap from small base radii proportional to boundary\n            base_bound = np.minimum.reduce([centers_opt[:, 0], centers_opt[:, 1],\n                                           1.0 - centers_opt[:, 0], 1.0 - centers_opt[:, 1]])\n            r_guess = 0.2 * base_bound\n            r_cap = np.clip(1.2 * r_guess, 0.0, 0.5)\n            r_lp2 = _lp_feasibilize(centers_opt, r_max=r_cap, eps_pair=1e-6)\n            if _verify(centers_opt, r_lp2, tol_pair=1e-6):\n                s = float(np.sum(r_lp2))\n                if s > best['sum'] + 1e-12:\n                    best['centers'] = centers_opt.copy()\n                    best['radii'] = r_lp2.copy()\n                    best['sum'] = s\n        seed_inc += 1\n\n    # If none found strictly feasible, fallback to a baseline hex layout and LP\n    if best['centers'] is None:\n        centers_fb = _hex_initial_centers_from_rows([6, 5, 6, 5, 4], margin=0.06, jitter_scale=0.0, seed=42)\n        r_fb = _lp_feasibilize(centers_fb, r_max=None, eps_pair=1e-6)\n        if not _verify(centers_fb, r_fb, tol_pair=1e-6):\n            # try scaling a bit down\n            r_fb2 = 0.98 * r_fb\n            if _verify(centers_fb, r_fb2, tol_pair=1e-6):\n                r_fb = r_fb2\n            else:\n                # last resort: very small radii\n                r_fb = np.minimum(r_fb, 0.01)\n                if not _verify(centers_fb, r_fb, tol_pair=1e-6):\n                    # As absolute fallback, place 26 tiny circles in a grid\n                    coords = []\n                    g = int(np.ceil(np.sqrt(26)))\n                    xs = np.linspace(0.05, 0.95, g)\n                    ys = np.linspace(0.05, 0.95, g)\n                    for xi in xs:\n                        for yi in ys:\n                            coords.append([xi, yi])\n                    centers_fb = np.array(coords[:26])\n                    r_fb = np.full(26, 0.01)\n        best['centers'] = centers_fb\n        best['radii'] = r_fb\n        best['sum'] = float(np.sum(r_fb))\n\n    # Final strict LP hardening: attempt slightly stricter eps to guarantee strictness\n    final_r = _lp_feasibilize(best['centers'], r_max=None, eps_pair=1.1e-6)\n    if _verify(best['centers'], final_r, tol_pair=1e-6):\n        radii = final_r\n    else:\n        # If stricter LP failed, fall back to previous and scale down if necessary\n        radii = best['radii']\n        if not _verify(best['centers'], radii, tol_pair=1e-6):\n            # compute maximal scaling to satisfy constraints\n            d, _, _ = _pairwise_distances(best['centers'])\n            denom = radii[:, None] + radii[None, :]\n            with np.errstate(divide='ignore', invalid='ignore'):\n                ratio_pairs = np.where(denom > 1e-12, (d - 1e-6) / denom, np.inf)\n            iu = np.triu_indices(N, 1)\n            s_pairs = np.min(ratio_pairs[iu]) if ratio_pairs[iu].size > 0 else 1.0\n            if not np.isfinite(s_pairs):\n                s_pairs = 1.0\n            s_pairs = max(0.0, min(1.0, s_pairs))\n            bx = np.minimum.reduce([best['centers'][:, 0], best['centers'][:, 1],\n                                    1.0 - best['centers'][:, 0], 1.0 - best['centers'][:, 1]])\n            with np.errstate(divide='ignore', invalid='ignore'):\n                scale_b = np.min(np.where(radii > 1e-12, (bx - 1e-6) / radii, 1.0))\n            if not np.isfinite(scale_b):\n                scale_b = 1.0\n            scale_b = max(0.0, min(1.0, scale_b))\n            s = min(s_pairs, scale_b)\n            radii = np.clip(s * radii, 0.0, 0.5)\n\n    centers = best['centers'].astype(float)\n    radii = np.clip(radii, 0.0, 0.5).astype(float)\n    sum_radii = float(np.sum(radii))\n\n    # Final verification; if still not strictly feasible attempt to shrink slightly until feasible\n    if not _verify(centers, radii, tol_pair=1e-6):\n        # iterative shrink\n        shrink = 0.995\n        tries = 0\n        while tries < 40 and (not _verify(centers, radii, tol_pair=1e-6)):\n            radii *= shrink\n            tries += 1\n        sum_radii = float(np.sum(radii))\n\n    return centers, radii, sum_radii\n```",
  "reasoning": "",
  "timestamp": "2026-02-11T02:55:41.743777",
  "cost_data": null,
  "model_config": {
    "model": "openai/gpt-5-mini",
    "temperature": 0.35
  },
  "llm_call_duration_s": 118.53338247805368,
  "eval_duration_s": 0.6368751950794831
}