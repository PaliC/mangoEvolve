{
  "trial_id": "trial_14_3",
  "generation": 14,
  "parent_id": null,
  "code": "import numpy as np\nfrom scipy.optimize import minimize, linprog\n\ndef construct_packing():\n    \"\"\"\n    Construct a packing of 26 circles in the unit square.\n    Returns (centers, radii, sum_radii)\n    - centers: np.array (26,2)\n    - radii: np.array (26,)\n    - sum_radii: float\n    \"\"\"\n    # Constants\n    N = 26\n    tiny = 1e-12\n    eps_pair_final = 1e-6\n\n    # Utilities\n    def sigmoid(z):\n        return 1.0 / (1.0 + np.exp(-z))\n\n    def logit(p):\n        p = np.clip(p, 1e-9, 1.0 - 1e-9)\n        return np.log(p / (1.0 - p))\n\n    def _pairwise_distances(centers):\n        diff = centers[:, None, :] - centers[None, :, :]\n        dx = diff[..., 0]\n        dy = diff[..., 1]\n        d = np.sqrt(np.maximum(dx * dx + dy * dy, 0.0))\n        return d, dx, dy\n\n    def _compute_dynamic_weights(centers, tiny=1e-12, gamma_density=0.8, gamma_edge=0.6):\n        \"\"\"\n        Compute per-circle weights w_i based on local density and edge proximity.\n        Returns w (N,) with mean ~1.\n        \"\"\"\n        Nloc = centers.shape[0]\n        x = centers[:, 0]\n        y = centers[:, 1]\n        d, dx_mat, dy_mat = _pairwise_distances(centers)\n        k = min(6, max(1, Nloc - 1))\n        if Nloc <= 1:\n            density = np.ones(Nloc)\n        else:\n            # ignore self by adding large diag\n            sorted_d = np.sort(d + np.eye(Nloc) * 1e9, axis=1)\n            knn_mean = np.mean(sorted_d[:, :k], axis=1)\n            density_raw = 1.0 / (knn_mean + tiny)\n            density = density_raw / (np.mean(density_raw) + tiny)\n        proximity = np.minimum.reduce([x, y, 1.0 - x, 1.0 - y])\n        edge_factor = 1.0 - np.clip(4.0 * proximity, 0.0, 1.0)\n        w = 1.0 + gamma_density * (density - 1.0) + gamma_edge * edge_factor\n        w = np.clip(w, 0.2, 5.0)\n        w = w / (np.mean(w) + tiny)\n        return w\n\n    def _hex_initial_centers_from_rows(rows, margin=0.05, jitter_scale=0.03, seed=0):\n        assert sum(rows) == N\n        max_cols = max(rows)\n        nrows = len(rows)\n        dx_base = (1.0 - 2.0 * margin) / (max_cols - 1) if max_cols > 1 else 0.0\n        dy_base = (1.0 - 2.0 * margin) / (nrows - 1) if nrows > 1 else 0.0\n\n        centers = []\n        rng = np.random.default_rng(seed)\n        for ri, cols in enumerate(rows):\n            y = margin + ri * dy_base if nrows > 1 else 0.5\n            if cols > 1:\n                row_width = (cols - 1) * dx_base\n                startx = 0.5 - row_width / 2.0\n                if ri % 2 == 1:\n                    startx += dx_base / 2.0\n            else:\n                startx = 0.5\n            for k in range(cols):\n                x = startx + (k * dx_base if cols > 1 else 0.0)\n                centers.append([x, y])\n        centers = np.array(centers)\n        jitter = rng.normal(scale=jitter_scale, size=centers.shape)\n        centers = centers + jitter\n        centers = np.clip(centers, 1e-6, 1.0 - 1e-6)\n        return centers\n\n    def _multi_start_inits(max_inits=24):\n        patterns = [\n            [6,5,6,5,4], [6,5,5,5,5], [5,6,5,5,5],\n            [5,5,6,5,5], [6,6,5,5,4], [5,6,6,5,4],\n            [6,5,6,4,5], [5,5,5,5,6], [4,5,6,5,6],\n            [5,4,5,6,6], [5,6,4,6,5]\n        ]\n        params = [\n            (0.03, 0.02), (0.04, 0.03), (0.05, 0.03),\n            (0.06, 0.02), (0.07, 0.03), (0.05, 0.04),\n            (0.04, 0.025), (0.055, 0.035), (0.065, 0.025)\n        ]\n        inits = []\n        seed = 1\n        for pat in patterns:\n            for (m, j) in params:\n                c = _hex_initial_centers_from_rows(pat, margin=m, jitter_scale=j, seed=seed)\n                inits.append(c)\n                seed += 1\n                if len(inits) >= max_inits:\n                    break\n            if len(inits) >= max_inits:\n                break\n        # Add an edge biased seed\n        if len(inits) < max_inits:\n            rng = np.random.default_rng(999)\n            edge_positions = [\n                [0.02, 0.02], [0.02, 0.5], [0.02, 0.98], [0.5, 0.02],\n                [0.5, 0.98], [0.98, 0.02], [0.98, 0.5], [0.98, 0.98]\n            ]\n            interior = _hex_initial_centers_from_rows([6,6,6], margin=0.2, jitter_scale=0.03, seed=1234)\n            if interior.shape[0] >= 18:\n                interior = interior[:18]\n            else:\n                extra = np.tile(interior, (int(np.ceil(18 / max(1, interior.shape[0]))), 1))[:18]\n                interior = extra\n            centers = np.vstack([np.array(edge_positions), interior])\n            centers += rng.normal(scale=0.01, size=centers.shape)\n            centers = np.clip(centers, 1e-6, 1 - 1e-6)\n            inits.append(centers)\n        # ensure shapes\n        final = []\n        for arr in inits:\n            if arr.shape[0] == N:\n                final.append(arr)\n        # fallback\n        while len(final) < 4:\n            final.append(_hex_initial_centers_from_rows([6,5,6,5,4], margin=0.05, jitter_scale=0.02, seed=42+len(final)))\n        return final[:max_inits]\n\n    def _lp_maximize_sum_r(centers, eps_pair=eps_pair_final):\n        \"\"\"\n        Maximize sum(r) subject to:\n          0 <= r_i <= bound_i = min(x_i, y_i, 1-x_i, 1-y_i)\n          r_i + r_j <= d_ij - eps_pair for i<j\n        \"\"\"\n        Nloc = centers.shape[0]\n        x = centers[:, 0].copy()\n        y = centers[:, 1].copy()\n        bounds_var = [(0.0, 0.5)] * Nloc\n\n        A_rows = []\n        b_vals = []\n        # per-variable upper bounds via A_ub\n        bounds_caps = np.minimum.reduce([x, y, 1.0 - x, 1.0 - y])\n        # pairwise distances\n        d, _, _ = _pairwise_distances(centers)\n\n        # pairwise constraints: r_i + r_j <= d_ij - eps_pair\n        for i in range(Nloc):\n            for j in range(i+1, Nloc):\n                rhs = d[i, j] - eps_pair\n                if rhs < 0.0:\n                    # infeasible at this centers; zero radii fallback will be produced by LP\n                    rhs = -1.0  # forces infeasible, but we'll let LP handle\n                e = np.zeros(Nloc)\n                e[i] = 1.0; e[j] = 1.0\n                A_rows.append(e); b_vals.append(rhs)\n        # bound constraints as linear inequalities r_i <= bound_i\n        for i in range(Nloc):\n            e = np.zeros(Nloc)\n            e[i] = 1.0\n            A_rows.append(e); b_vals.append(bounds_caps[i])\n\n        if len(A_rows) == 0:\n            A_ub = None; b_ub = None\n        else:\n            A_ub = np.vstack(A_rows)\n            b_ub = np.array(b_vals)\n\n        c = -np.ones(Nloc)  # maximize sum -> minimize -sum\n        try:\n            res = linprog(c, A_ub=A_ub, b_ub=b_ub, bounds=bounds_var, method='highs')\n        except Exception:\n            res = None\n\n        if res is not None and res.success:\n            r = np.clip(res.x, 0.0, 0.5)\n            return r\n        # fallback: return zeros\n        return np.zeros(Nloc)\n\n    def _verify(centers, radii, tol_pair=1e-6):\n        Nloc = len(radii)\n        if centers.shape != (Nloc, 2):\n            return False\n        if np.any(radii < -1e-12):\n            return False\n        x = centers[:, 0]\n        y = centers[:, 1]\n        if np.any(x - radii < tol_pair):\n            return False\n        if np.any(y - radii < tol_pair):\n            return False\n        if np.any(x + radii > 1.0 - tol_pair):\n            return False\n        if np.any(y + radii > 1.0 - tol_pair):\n            return False\n        d, _, _ = _pairwise_distances(centers)\n        iu = np.triu_indices(Nloc, 1)\n        lhs = radii[:, None] + radii[None, :]\n        viol = lhs[iu] - d[iu] + tol_pair\n        if np.any(viol > 1e-12):\n            return False\n        return True\n\n    # Objective for centers-only parameterized by sigmoid(z)\n    # Using target radius proxy r_i = alpha * min(x, y, 1-x, 1-y)\n    def _objective_centers_sigmoid(z, alpha, P, w_b=1.0, w_o=1.0):\n        \"\"\"\n        z: length 2N (unbounded). x = sigmoid(z[:N]), y = sigmoid(z[N:2N])\n        Returns f, grad_z where grad_z is analytic gradient of f wrt z.\n        f = -sum(r_proxy) + P*(w_b * pen_b + pen_o)\n        \"\"\"\n        zx = z[:N]\n        zy = z[N:2*N]\n        x = sigmoid(zx)\n        y = sigmoid(zy)\n        centers = np.stack([x, y], axis=1)\n\n        # compute c = min(x, y, 1-x, 1-y) and its derivative\n        arr = np.vstack([x, y, 1.0 - x, 1.0 - y])  # shape (4, N)\n        argmin = np.argmin(arr, axis=0)  # 0->x,1->y,2->1-x,3->1-y\n        c = arr[argmin, np.arange(N)]\n        r = alpha * c  # proxy radii\n\n        # dynamic weights (we treat w_vec as constant for gradient to simplify)\n        w_vec = _compute_dynamic_weights(centers, tiny=tiny, gamma_density=0.8, gamma_edge=0.65)\n        # boundary weight per circle\n        prox = np.minimum.reduce([x, y, 1.0 - x, 1.0 - y])\n        w_b_vec = 1.0 + 0.4 * (1.0 - np.clip(4.0 * prox, 0.0, 1.0))\n        w_b_vec = w_b_vec * w_vec\n\n        # boundary hinge terms\n        s1 = r - x\n        s2 = r - y\n        s3 = r + x - 1.0\n        s4 = r + y - 1.0\n        h1 = np.maximum(0.0, s1)\n        h2 = np.maximum(0.0, s2)\n        h3 = np.maximum(0.0, s3)\n        h4 = np.maximum(0.0, s4)\n        pen_b_terms = 0.5 * (h1*h1 + h2*h2 + h3*h3 + h4*h4) * w_b_vec\n        pen_b = np.sum(pen_b_terms)\n\n        # pairwise overlaps\n        d, dx_mat, dy_mat = _pairwise_distances(centers)\n        iu = np.triu_indices(N, 1)\n        dab = d[iu]\n        dxab = dx_mat[iu]\n        dyab = dy_mat[iu]\n        d_safe = np.maximum(dab, tiny)\n\n        hij = r[iu[0]] + r[iu[1]] - dab\n        hpos = np.maximum(0.0, hij)\n        w_i = w_vec[iu[0]]\n        w_j = w_vec[iu[1]]\n        w_pair = w_o * (w_i * w_j)\n        pen_o = 0.5 * np.sum(w_pair * (hpos * hpos))\n\n        # total objective\n        f = -np.sum(r) + P * (w_b * pen_b + pen_o)\n\n        # Gradients:\n        # dr/dx and dr/dy from r = alpha * c\n        # dc/dx = 1 if argmin==0, -1 if argmin==2, else 0\n        # dc/dy = 1 if argmin==1, -1 if argmin==3, else 0\n        dc_dx = np.zeros(N)\n        dc_dy = np.zeros(N)\n        mask0 = (argmin == 0)\n        mask1 = (argmin == 1)\n        mask2 = (argmin == 2)\n        mask3 = (argmin == 3)\n        dc_dx[mask0] = 1.0\n        dc_dy[mask1] = 1.0\n        dc_dx[mask2] = -1.0\n        dc_dy[mask3] = -1.0\n        dr_dx = alpha * dc_dx\n        dr_dy = alpha * dc_dy\n\n        # Boundary gradients (w.r.t r, x, y) (we ignore gradient of w_b_vec)\n        dr_b = (h1 + h2 + h3 + h4) * w_b_vec  # derivative of penalty wrt r_i\n        dx_b = (-h1 + h3) * w_b_vec\n        dy_b = (-h2 + h4) * w_b_vec\n\n        # Overlap gradients\n        dx_o = np.zeros(N, dtype=float)\n        dy_o = np.zeros(N, dtype=float)\n        dr_o = np.zeros(N, dtype=float)\n        # coeff = hpos / d_safe\n        coeff = np.zeros_like(hpos)\n        mask_pos = (hpos > 0.0)\n        if np.any(mask_pos):\n            coeff[mask_pos] = (hpos[mask_pos] / d_safe[mask_pos])\n            coeff_w = coeff * w_pair\n            cx = coeff_w * dxab\n            cy = coeff_w * dyab\n            # center contributions\n            np.add.at(dx_o, iu[0], -cx)\n            np.add.at(dx_o, iu[1], +cx)\n            np.add.at(dy_o, iu[0], -cy)\n            np.add.at(dy_o, iu[1], +cy)\n            # radii contributions\n            np.add.at(dr_o, iu[0], w_pair * hpos)\n            np.add.at(dr_o, iu[1], w_pair * hpos)\n\n        # Total derivatives wrt x and y from explicit dependence in penalties\n        dx_total_pen = P * (w_b * dx_b + dx_o)\n        dy_total_pen = P * (w_b * dy_b + dy_o)\n        # derivative wrt r (for chain rule)\n        dr_total = -1.0 + P * (w_b * dr_b + dr_o)  # df/dr_i\n\n        # include chain contributions via r(x,y)\n        dx_total = dx_total_pen + dr_total * dr_dx\n        dy_total = dy_total_pen + dr_total * dr_dy\n\n        # Now chain through sigmoid: dx/dzx = x*(1-x)\n        dzx = dx_total * (x * (1.0 - x))\n        dzy = dy_total * (y * (1.0 - y))\n\n        grad = np.concatenate([dzx, dzy])\n        return float(f), grad\n\n    # Initialization seeds\n    inits = _multi_start_inits(max_inits=20)\n\n    # Stage schedules\n    alpha_schedule = [0.70, 0.80, 0.86, 0.92, 0.95]\n    P_schedule = [12.0, 80.0, 600.0, 2400.0, 6000.0]  # increasing penalties\n    maxiter_stage = [80, 100, 110, 120, 140]\n\n    best = {'centers': None, 'radii': None, 'sum': -1e9}\n\n    # Limit number of inits to control runtime\n    max_trials = min(12, len(inits))\n    for init_idx in range(max_trials):\n        centers0 = inits[init_idx].copy()\n        # convert to z initial\n        zx0 = logit(np.clip(centers0[:, 0], 1e-9, 1.0 - 1e-9))\n        zy0 = logit(np.clip(centers0[:, 1], 1e-9, 1.0 - 1e-9))\n        z = np.concatenate([zx0, zy0])  # length 2N\n\n        # annealing stages\n        for stage_idx, (alpha, P, maxit) in enumerate(zip(alpha_schedule, P_schedule, maxiter_stage)):\n            try:\n                res = minimize(\n                    fun=lambda zz: _objective_centers_sigmoid(zz, alpha=alpha, P=P, w_b=1.0, w_o=1.0),\n                    x0=z,\n                    method='L-BFGS-B',\n                    jac=True,\n                    options={'maxiter': maxit, 'ftol': 1e-12, 'gtol': 1e-8, 'maxcor': 20}\n                )\n                z = res.x\n            except Exception:\n                # ignore failures and continue\n                pass\n\n            # After stage, map to centers and run LP to get true radii\n            x = sigmoid(z[:N]); y = sigmoid(z[N:2*N])\n            centers_tmp = np.stack([x, y], axis=1)\n            centers_tmp = np.clip(centers_tmp, 1e-9, 1.0 - 1e-9)\n            r_lp = _lp_maximize_sum_r(centers_tmp, eps_pair=1e-6)\n\n            # Optionally perform a short centers-only relaxation keeping LP radii fixed:\n            # We'll do a single quick L-BFGS stage refining centers to reduce penalties for the LP radii.\n            def _obj_centers_for_fixed_r(zz):\n                zx = zz[:N]; zy = zz[N:2*N]\n                x = sigmoid(zx)\n                y = sigmoid(zy)\n                centers = np.stack([x, y], axis=1)\n                # compute simple penalty: boundary hinge and overlaps wrt fixed r_lp\n                r = r_lp\n                s1 = r - x\n                s2 = r - y\n                s3 = r + x - 1.0\n                s4 = r + y - 1.0\n                h1 = np.maximum(0.0, s1)\n                h2 = np.maximum(0.0, s2)\n                h3 = np.maximum(0.0, s3)\n                h4 = np.maximum(0.0, s4)\n                w_vec = _compute_dynamic_weights(centers, tiny=tiny, gamma_density=0.8, gamma_edge=0.65)\n                prox = np.minimum.reduce([x, y, 1.0 - x, 1.0 - y])\n                w_b_vec = 1.0 + 0.4 * (1.0 - np.clip(4.0 * prox, 0.0, 1.0))\n                w_b_vec = w_b_vec * w_vec\n                pen_b = 0.5 * np.sum((h1*h1 + h2*h2 + h3*h3 + h4*h4) * w_b_vec)\n\n                d, dx_mat, dy_mat = _pairwise_distances(centers)\n                iu = np.triu_indices(N, 1)\n                dab = d[iu]\n                hij = r[iu[0]] + r[iu[1]] - dab\n                hpos = np.maximum(0.0, hij)\n                w_i = w_vec[iu[0]]\n                w_j = w_vec[iu[1]]\n                w_pair = (w_i * w_j)\n                pen_o = 0.5 * np.sum(w_pair * (hpos * hpos))\n\n                f = P * (pen_b + pen_o)\n                # gradient (approximate analytic, but ignore gradient of w_vec for speed)\n                # boundary grads\n                dx_b = (-h1 + h3) * w_b_vec\n                dy_b = (-h2 + h4) * w_b_vec\n                dx_o = np.zeros(N); dy_o = np.zeros(N)\n                d_safe = np.maximum(d[iu], tiny)\n                mask_pos = (hpos > 0.0)\n                if np.any(mask_pos):\n                    coeff = np.zeros_like(hpos); coeff[mask_pos] = hpos[mask_pos] / d_safe[mask_pos]\n                    coeff_w = coeff * w_pair\n                    cx = coeff_w * dx_mat[iu]\n                    cy = coeff_w * dy_mat[iu]\n                    np.add.at(dx_o, iu[0], -cx); np.add.at(dx_o, iu[1], +cx)\n                    np.add.at(dy_o, iu[0], -cy); np.add.at(dy_o, iu[1], +cy)\n                dx_total = P * (dx_b + dx_o)\n                dy_total = P * (dy_b + dy_o)\n                dzx = dx_total * (x * (1.0 - x))\n                dzy = dy_total * (y * (1.0 - y))\n                grad = np.concatenate([dzx, dzy])\n                return float(f), grad\n\n            # run short relaxation\n            try:\n                res2 = minimize(\n                    fun=_obj_centers_for_fixed_r,\n                    x0=z,\n                    method='L-BFGS-B',\n                    jac=True,\n                    options={'maxiter': 40, 'ftol': 1e-10, 'gtol': 1e-8}\n                )\n                z = res2.x\n            except Exception:\n                pass\n\n        # After all stages, produce final centers and final LP\n        x = sigmoid(z[:N]); y = sigmoid(z[N:2*N])\n        centers_final = np.stack([x, y], axis=1)\n        centers_final = np.clip(centers_final, 1e-9, 1.0 - 1e-9)\n        r_final = _lp_maximize_sum_r(centers_final, eps_pair=eps_pair_final)\n\n        # verify, if not feasible try uniform shrink\n        if not _verify(centers_final, r_final, tol_pair=eps_pair_final):\n            # Try shrinking radii slightly\n            radii_try = r_final.copy()\n            factor = 0.9995\n            for _ in range(2000):\n                radii_try *= factor\n                if _verify(centers_final, radii_try, tol_pair=eps_pair_final):\n                    r_final = radii_try\n                    break\n\n        # Accept if feasible\n        if _verify(centers_final, r_final, tol_pair=eps_pair_final):\n            s = float(np.sum(r_final))\n            if s > best['sum']:\n                best['centers'] = centers_final.copy()\n                best['radii'] = r_final.copy()\n                best['sum'] = s\n\n    # If none found, fallback deterministic hex and LP\n    if best['centers'] is None:\n        centers_fb = _hex_initial_centers_from_rows([6,5,6,5,4], margin=0.06, jitter_scale=0.02, seed=123)\n        r_fb = _lp_maximize_sum_r(centers_fb, eps_pair=eps_pair_final)\n        if not _verify(centers_fb, r_fb, tol_pair=eps_pair_final):\n            # scale down until feasible\n            fac = 0.995\n            for _ in range(2000):\n                r_fb *= fac\n                if _verify(centers_fb, r_fb, tol_pair=eps_pair_final):\n                    break\n        best['centers'] = centers_fb\n        best['radii'] = r_fb\n        best['sum'] = float(np.sum(r_fb))\n\n    # Final strict LP enforcement once more and final verification\n    centers = best['centers'].astype(float)\n    radii = _lp_maximize_sum_r(centers, eps_pair=eps_pair_final)\n    if not _verify(centers, radii, tol_pair=eps_pair_final):\n        # try scaling down radii slightly\n        fac = 0.9995\n        for _ in range(5000):\n            radii = radii * fac\n            if _verify(centers, radii, tol_pair=eps_pair_final):\n                break\n    radii = np.clip(radii, 0.0, 0.5)\n    sum_radii = float(np.sum(radii))\n\n    # final safety: if still infeasible, zero out and return fallback\n    if not _verify(centers, radii, tol_pair=eps_pair_final):\n        centers = _hex_initial_centers_from_rows([6,5,6,5,4], margin=0.08, jitter_scale=0.01, seed=321)\n        radii = _lp_maximize_sum_r(centers, eps_pair=eps_pair_final)\n        if not _verify(centers, radii, tol_pair=eps_pair_final):\n            # ultimate fallback uniform shrink until feasible\n            radii = radii.copy()\n            fac = 0.995\n            for _ in range(10000):\n                radii *= fac\n                if _verify(centers, radii, tol_pair=eps_pair_final):\n                    break\n        sum_radii = float(np.sum(radii))\n\n    return centers, radii, float(sum_radii)",
  "metrics": {
    "valid": true,
    "score": 2.1666731131572625,
    "eval_time": 2.5697405338287354,
    "error": null
  },
  "prompt": "\nOptimize unconstrained vars z where centers = sigmoid(z_xy) and optional radius vars handled implicitly by LP (so optimize centers only). Objective: smooth penalty over overlaps and boundary slack of a target radius proxy r_i = alpha * min(x_i, y_i, 1-x_i, 1-y_i), with alpha slowly increased across stages (0.7 -> 0.95). Use L-BFGS-B (on z) with analytic gradients; eps only in denominators. After each stage, run LP to get true radii. Multiple seeds (hex, edge-biased), keep best.\n\nTake patterns from:\n# Ref (trial_11_7):\nimport numpy as np\nfrom scipy.optimize import minimize, linprog\n\ndef _pairwise_distances(centers):\n    diff = centers[:, None, :] - centers[None, :, :]\n    d = np.sqrt(np.sum(diff**2, axis=2))\n    return d, diff[..., 0], diff[..., 1]\n\ndef _compute_dynamic_weights(centers, tiny=1e-12, gamma_density=0.8, gamma_edge=0.6):\n    \"\"\"\n    Compute per-circle weights w_i based on local density and edge proximity.\n    Returns w (N,) with mean ~1.\n    \"\"\"\n    N = centers.shape[0]\n    x = centers[:, 0]\n    y = centers[:, 1]\n    d, dx_mat, dy_mat = _pairwise_distances(centers)\n    # For density: use mean distance to k nearest neighbors (k = min(6, N-1))\n    k = min(6, N-1)\n    if N <= 1:\n        density = np.ones(N)\n    else:\n        sorted_d = np.sort(d + np.eye(N) * 1e9, axis=1)  # ignore self\n        knn_mean = np.mean(sorted_d[:, :k], axis=1)\n        density_raw = 1.0 / (knn_mean + tiny)\n        # normalize to mean 1\n        density = density_raw / (np.mean(density_raw) + tiny)\n    # Edge proximity: smaller proximity => larger edge factor\n    proximity = np.minimum.reduce([x, y, 1.0 - x, 1.0 - y])\n    edge_factor = 1.0 - np.clip(4.0 * proximity, 0.0, 1.0)  # near center -> 0, near edge-> up to 1\n    w = 1.0 + gamma_density * (density - 1.0) + gamma_edge * edge_factor\n    # Clip to reasonable bounds\n    w = np.clip(w, 0.2, 5.0)\n    # Normalize to mean 1 to keep penalty scale stable\n    w = w / (np.mean(w) + tiny)\n    return w\n\ndef _objective_and_grad(z, N, P, w_b=1.0, w_o=1.0):\n    \"\"\"\n    Joint objective and gradient for (x,y,r) using dynamic per-circle weights.\n    Returns (f, grad) where grad is the gradient vector.\n    \"\"\"\n    x = z[:N]\n    y = z[N:2*N]\n    r = z[2*N:]\n    tiny = 1e-12\n\n    centers = np.stack([x, y], axis=1)\n    # dynamic per-circle weights\n    w_vec = _compute_dynamic_weights(centers, tiny=tiny, gamma_density=0.8, gamma_edge=0.65)  # tuned\n    # Boundary penalties: hinges for r - x, r - y, r + x - 1, r + y - 1\n    s1 = r - x\n    s2 = r - y\n    s3 = r + x - 1.0\n    s4 = r + y - 1.0\n    h1 = np.maximum(0.0, s1)\n    h2 = np.maximum(0.0, s2)\n    h3 = np.maximum(0.0, s3)\n    h4 = np.maximum(0.0, s4)\n    # per-circle boundary weight\n    w_b_vec = 1.0 + 0.4 * (1.0 - np.clip(4.0 * np.minimum.reduce([x, y, 1.0 - x, 1.0 - y]), 0.0, 1.0))\n    w_b_vec = w_b_vec * w_vec\n    pen_b_terms = 0.5 * (h1*h1 + h2*h2 + h3*h3 + h4*h4) * w_b_vec\n    pen_b = np.sum(pen_b_terms)\n\n    # Overlap penalties\n    d, dx_mat, dy_mat = _pairwise_distances(centers)\n    iu = np.triu_indices(N, 1)\n    dab = d[iu]\n    dxab = dx_mat[iu]\n    dyab = dy_mat[iu]\n    d_safe = np.maximum(dab, tiny)\n    hij = r[iu[0]] + r[iu[1]] - dab\n    hpos = np.maximum(0.0, hij)\n    # pairwise weight w_ij = w_i * w_j scaled by global w_o\n    w_i = w_vec[iu[0]]\n    w_j = w_vec[iu[1]]\n    w_pair = w_o * (w_i * w_j)\n    pen_o = 0.5 * np.sum(w_pair * (hpos * hpos))\n\n    f = -np.sum(r) + P * (w_b * pen_b + pen_o)\n\n    # Gradients\n    # Boundary gradients\n    dr_b = (h1 + h2 + h3 + h4) * w_b_vec\n    dx_b = (-h1 + h3) * w_b_vec\n    dy_b = (-h2 + h4) * w_b_vec\n\n    # Overlap gradients\n    dx_o = np.zeros(N, dtype=float)\n    dy_o = np.zeros(N, dtype=float)\n    dr_o = np.zeros(N, dtype=float)\n\n    coeff = hpos / d_safe  # h / d\n    # Multiply coeff by pairwise weight and by 1 (for squared hinge derivative)\n    coeff_w = coeff * w_pair\n    cx = coeff_w * dxab\n    cy = coeff_w * dyab\n    # center contributions\n    np.add.at(dx_o, iu[0], -cx)\n    np.add.at(dx_o, iu[1], +cx)\n    np.add.at(dy_o, iu[0], -cy)\n    np.add.at(dy_o, iu[1], +cy)\n    # radii contributions: derivative of 0.5 * w_pair * h^2 wrt r_a is w_pair * hpos\n    np.add.at(dr_o, iu[0], w_pair * hpos)\n    np.add.at(dr_o, iu[1], w_pair * hpos)\n\n    dx_total = P * (w_b * dx_b + dx_o)\n    dy_total = P * (w_b * dy_b + dy_o)\n    dr_total = -1.0 + P * (w_b * dr_b + dr_o)\n\n    grad = np.concatenate([dx_total, dy_total, dr_total])\n    return float(f), grad\n\ndef _objective_centers_only(z_xy, r_fixed, P, w_b=1.0, w_o=1.0):\n    \"\"\"\n    Objective and gradient for centers only with fixed radii.\n    Returns (f, grad_xy).\n    \"\"\"\n    N = r_fixed.shape[0]\n    x = z_xy[:N]\n    y = z_xy[N:2*N]\n    r = r_fixed\n    tiny = 1e-12\n\n    centers = np.stack([x, y], axis=1)\n    w_vec = _compute_dynamic_weights(centers, tiny=tiny, gamma_density=0.8, gamma_edge=0.65)\n    # Boundary\n    s1 = r - x\n    s2 = r - y\n    s3 = r + x - 1.0\n    s4 = r + y - 1.0\n    h1 = np.maximum(0.0, s1)\n    h2 = np.maximum(0.0, s2)\n    h3 = np.maximum(0.0, s3)\n    h4 = np.maximum(0.0, s4)\n    w_b_vec = 1.0 + 0.4 * (1.0 - np.clip(4.0 * np.minimum.reduce([x, y, 1.0 - x, 1.0 - y]), 0.0, 1.0))\n    w_b_vec = w_b_vec * w_vec\n    pen_b_terms = 0.5 * (h1*h1 + h2*h2 + h3*h3 + h4*h4) * w_b_vec\n    pen_b = np.sum(pen_b_terms)\n\n    # Overlap\n    d, dx_mat, dy_mat = _pairwise_distances(centers)\n    iu = np.triu_indices(N, 1)\n    dab = d[iu]\n    dxab = dx_mat[iu]\n    dyab = dy_mat[iu]\n    d_safe = np.maximum(dab, tiny)\n    hij = r[iu[0]] + r[iu[1]] - dab\n    hpos = np.maximum(0.0, hij)\n    w_i = w_vec[iu[0]]\n    w_j = w_vec[iu[1]]\n    w_pair = w_o * (w_i * w_j)\n    pen_o = 0.5 * np.sum(w_pair * (hpos * hpos))\n\n    f = P * (w_b * pen_b + pen_o)\n\n    # Gradients wrt centers\n    dx_b = (-h1 + h3) * w_b_vec\n    dy_b = (-h2 + h4) * w_b_vec\n\n    dx_o = np.zeros(N, dtype=float)\n    dy_o = np.zeros(N, dtype=float)\n    coeff = hpos / d_safe\n    coeff_w = coeff * w_pair\n    cx = coeff_w * dxab\n    cy = coeff_w * dyab\n    np.add.at(dx_o, iu[0], -cx)\n    np.add.at(dx_o, iu[1], +cx)\n    np.add.at(dy_o, iu[0], -cy)\n    np.add.at(dy_o, iu[1], +cy)\n\n    dx_total = P * (w_b * dx_b + dx_o)\n    dy_total = P * (w_b * dy_b + dy_o)\n    g = np.concatenate([dx_total, dy_total])\n    return float(f), g\n\ndef _lp_feasibilize(centers, r_max=None, eps_pair=1e-6):\n    \"\"\"\n    Maximize sum(r) for fixed centers using linear programming.\n    \"\"\"\n    N = centers.shape[0]\n    x = centers[:, 0]\n    y = centers[:, 1]\n\n    A_rows = []\n    b_vals = []\n\n    # Boundary constraints: r_i <= x_i - eps_pair, etc.\n    for i in range(N):\n        e = np.zeros(N)\n        e[i] = 1.0\n        A_rows.append(e.copy()); b_vals.append(x[i] - eps_pair)\n        A_rows.append(e.copy()); b_vals.append(y[i] - eps_pair)\n        A_rows.append(e.copy()); b_vals.append(1.0 - x[i] - eps_pair)\n        A_rows.append(e.copy()); b_vals.append(1.0 - y[i] - eps_pair)\n\n    # Pairwise overlap constraints\n    d, _, _ = _pairwise_distances(centers)\n    for i in range(N):\n        for j in range(i+1, N):\n            e = np.zeros(N)\n            e[i] = 1.0; e[j] = 1.0\n            A_rows.append(e); b_vals.append(d[i, j] - eps_pair)\n\n    # Optional per-variable caps\n    if r_max is not None:\n        for i in range(N):\n            e = np.zeros(N); e[i] = 1.0\n            A_rows.append(e); b_vals.append(r_max[i])\n\n    if len(A_rows) == 0:\n        A_ub = None\n        b_ub = None\n    else:\n        A_ub = np.vstack(A_rows)\n        b_ub = np.array(b_vals)\n\n    bounds = [(0.0, 0.5)] * N\n    c = -np.ones(N)  # maximize sum(r) -> minimize -sum(r)\n    try:\n        res = linprog(c, A_ub=A_ub, b_ub=b_ub, bounds=bounds, method='highs')\n    except Exception:\n        res = None\n\n    if res is not None and res.success:\n        return np.clip(res.x, 0.0, 0.5)\n\n    # Fallback: try without eps_pair (less strict)\n    A_rows2 = []\n    b_vals2 = []\n    for i in range(N):\n        e = np.zeros(N)\n        e[i] = 1.0\n        A_rows2.append(e.copy()); b_vals2.append(x[i])\n        A_rows2.append(e.copy()); b_vals2.append(y[i])\n        A_rows2.append(e.copy()); b_vals2.append(1.0 - x[i])\n        A_rows2.append(e.copy()); b_vals2.append(1.0 - y[i])\n\n    for i in range(N):\n        for j in range(i+1, N):\n            e = np.zeros(N); e[i] = 1.0; e[j] = 1.0\n            A_rows2.append(e); b_vals2.append(d[i, j])\n\n    if r_max is not None:\n        for i in range(N):\n            e = np.zeros(N); e[i] = 1.0\n            A_rows2.append(e); b_vals2.append(r_max[i])\n\n    if len(A_rows2) == 0:\n        A_ub2 = None\n        b_ub2 = None\n    else:\n        A_ub2 = np.vstack(A_rows2)\n        b_ub2 = np.array(b_vals2)\n\n    try:\n        res2 = linprog(c, A_ub=A_ub2, b_ub=b_ub2, bounds=bounds, method='highs')\n    except Exception:\n        res2 = None\n\n    if res2 is None or not res2.success:\n        return np.zeros(N)\n\n    r_base = np.clip(res2.x, 0.0, 0.5)\n    # scale down to satisfy eps_pair\n    d, _, _ = _pairwise_distances(centers)\n    denom_pairs = r_base[:, None] + r_base[None, :]\n    with np.errstate(divide='ignore', invalid='ignore'):\n        ratio_pairs = np.where(denom_pairs > 1e-9, (d - eps_pair) / denom_pairs, np.inf)\n    iu = np.triu_indices(N, 1)\n    s_pairs = np.min(ratio_pairs[iu]) if ratio_pairs[iu].size > 0 else 1.0\n    if not np.isfinite(s_pairs):\n        s_pairs = 1.0\n    s_pairs = max(0.0, min(1.0, s_pairs))\n    # boundary scaling\n    with np.errstate(divide='ignore', invalid='ignore'):\n        s_x_min = np.where(r_base > 1e-9, (x - eps_pair) / r_base, np.inf)\n        s_y_min = np.where(r_base > 1e-9, (y - eps_pair) / r_base, np.inf)\n        s_x_max = np.where(r_base > 1e-9, (1.0 - x - eps_pair) / r_base, np.inf)\n        s_y_max = np.where(r_base > 1e-9, (1.0 - y - eps_pair) / r_base, np.inf)\n    s_boundary = np.min([np.min(s_x_min), np.min(s_y_min), np.min(s_x_max), np.min(s_y_max)])\n    if not np.isfinite(s_boundary):\n        s_boundary = 1.0\n    s_boundary = max(0.0, min(1.0, s_boundary))\n    s = min(s_pairs, s_boundary)\n    r = s * r_base\n    return np.clip(r, 0.0, 0.5)\n\ndef _verify(centers, radii, tol_pair=1e-6):\n    N = len(radii)\n    if centers.shape != (N, 2):\n        return False\n    if np.any(radii < -1e-12):\n        return False\n    x = centers[:, 0]\n    y = centers[:, 1]\n    # boundary: x - r >= tol_pair, etc.\n    if np.any(x - radii < tol_pair):\n        return False\n    if np.any(y - radii < tol_pair):\n        return False\n    if np.any(x + radii > 1.0 - tol_pair):\n        return False\n    if np.any(y + radii > 1.0 - tol_pair):\n        return False\n    # pairwise\n    d, _, _ = _pairwise_distances(centers)\n    iu = np.triu_indices(N, 1)\n    lhs = radii[:, None] + radii[None, :]\n    viol = lhs[iu] - d[iu] + tol_pair\n    if np.any(viol > 1e-12):\n        return False\n    return True\n\ndef _hex_initial_centers_from_rows(rows, margin=0.05, jitter_scale=0.03, seed=0):\n    assert sum(rows) == 26\n    max_cols = max(rows)\n    nrows = len(rows)\n    dx_base = (1.0 - 2.0 * margin) / (max_cols - 1) if max_cols > 1 else 0.0\n    dy_base = (1.0 - 2.0 * margin) / (nrows - 1) if nrows > 1 else 0.0\n\n    centers = []\n    rng = np.random.default_rng(seed)\n    for ri, cols in enumerate(rows):\n        y = margin + ri * dy_base if nrows > 1 else 0.5\n        if cols > 1:\n            row_width = (cols - 1) * dx_base\n            startx = 0.5 - row_width / 2.0\n            if ri % 2 == 1:\n                startx += dx_base / 2.0\n        else:\n            startx = 0.5\n        for k in range(cols):\n            x = startx + (k * dx_base if cols > 1 else 0.0)\n            centers.append([x, y])\n    centers = np.array(centers)\n    jitter = rng.normal(scale=jitter_scale, size=centers.shape)\n    centers = centers + jitter\n    centers = np.clip(centers, 1e-3, 1.0 - 1e-3)\n    return centers\n\ndef _multi_start_inits(N, max_inits=32):\n    patterns = [\n        [6,5,6,5,4], [6,5,5,5,5], [5,6,5,5,5],\n        [5,5,6,5,5], [6,6,5,5,4], [5,6,6,5,4],\n        [6,5,6,4,5], [5,5,5,5,6], [4,5,6,5,6],\n        [5,4,5,6,6], [5,6,4,6,5]\n    ]\n    params = [\n        (0.03, 0.02), (0.04, 0.03), (0.05, 0.03),\n        (0.06, 0.02), (0.07, 0.03), (0.05, 0.04),\n        (0.04, 0.025), (0.055, 0.035), (0.065, 0.025)\n    ]\n    inits = []\n    seed = 1\n    for pat in patterns:\n        for (m, j) in params:\n            inits.append(_hex_initial_centers_from_rows(pat, margin=m, jitter_scale=j, seed=seed))\n            seed += 1\n            if len(inits) >= max_inits:\n                break\n        if len(inits) >= max_inits:\n            break\n    # Add some edge/corner heavy seeds: 8 near corners/edges + interior\n    if len(inits) < max_inits:\n        rng = np.random.default_rng(999)\n        # Construct 8 near corners/edges\n        edge_positions = [\n            [0.02, 0.02], [0.02, 0.5], [0.02, 0.98], [0.5, 0.02],\n            [0.5, 0.98], [0.98, 0.02], [0.98, 0.5], [0.98, 0.98]\n        ]\n        # remaining 18 as hex grid in interior\n        interior = _hex_initial_centers_from_rows([6,6,6], margin=0.2, jitter_scale=0.03, seed=1234)\n        # pick 18 from interior (it gives 18)\n        if interior.shape[0] >= 18:\n            interior = interior[:18]\n        else:\n            # replicate some if needed (unlikely)\n            extra = np.tile(interior, (int(np.ceil(18 / interior.shape[0])), 1))[:18]\n            interior = extra\n        centers = np.vstack([np.array(edge_positions), interior])\n        centers += rng.normal(scale=0.01, size=centers.shape)\n        centers = np.clip(centers, 1e-3, 1 - 1e-3)\n        inits.append(centers)\n    # Ensure all inits have N points\n    final_inits = []\n    for arr in inits[:max_inits]:\n        if arr.shape[0] != N:\n            continue\n        final_inits.append(arr)\n    # If none or less than desired, add deterministic fallback\n    while len(final_inits) < 4:\n        final_inits.append(_hex_initial_centers_from_rows([6,5,6,5,4], margin=0.05, jitter_scale=0.02, seed=42+len(final_inits)))\n    return final_inits\n\ndef _center_only_relax(centers, r_fixed, P=2000.0, w_b=1.0, w_o=1.0, iters=80):\n    N = centers.shape[0]\n    z0 = np.concatenate([centers[:, 0], centers[:, 1]])\n    bounds_xy = [(0.0, 1.0)] * (2 * N)\n    res = minimize(\n        fun=lambda zz: _objective_centers_only(zz, r_fixed, P, w_b, w_o),\n        x0=z0,\n        method='L-BFGS-B',\n        jac=True,\n        bounds=bounds_xy,\n        options={'maxiter': iters, 'ftol': 1e-12, 'gtol': 1e-8, 'maxcor': 20}\n    )\n    z = res.x\n    centers_new = np.stack([z[:N], z[N:2*N]], axis=1)\n    centers_new = np.clip(centers_new, 1e-6, 1.0 - 1e-6)\n    return centers_new\n\ndef construct_packing():\n    N = 26\n    best = {'centers': None, 'radii': None, 'sum': -np.inf}\n\n    P_schedule = [8.0, 80.0, 800.0, 3000.0]  # moderate schedule to limit runtime\n    maxiter_stage = [90, 100, 110, 120]\n    bounds = [(0.0, 1.0)] * (2 * N) + [(0.0, 0.5)] * N\n\n    inits = _multi_start_inits(N, max_inits=32)\n    if not inits:\n        inits = [_hex_initial_centers_from_rows([6,5,6,5,4], margin=0.05, jitter_scale=0.03, seed=42)]\n\n    for init_idx, centers0 in enumerate(inits):\n        x0 = centers0[:, 0].copy()\n        y0 = centers0[:, 1].copy()\n        base_bound = np.minimum.reduce([x0, y0, 1 - x0, 1 - y0])\n        # initialize radii as fraction of local boundary-clearance\n        r0 = 0.08 * base_bound\n        # small randomization for radii to diversify\n        rng = np.random.default_rng(1000 + init_idx)\n        r0 = r0 * (1.0 + 0.05 * rng.normal(size=r0.shape))\n        r0 = np.clip(r0, 1e-6, 0.5)\n        z = np.concatenate([x0, y0, r0])\n\n        # Annealing optimization loop\n        for si, P in enumerate(P_schedule):\n            # Joint optimization of centers & radii\n            try:\n                res = minimize(\n                    fun=lambda zz: _objective_and_grad(zz, N, P, 1.0, 1.0),\n                    x0=z,\n                    method='L-BFGS-B',\n                    jac=True,\n                    bounds=bounds,\n                    options={'maxiter': maxiter_stage[si], 'ftol': 1e-12, 'gtol': 1e-8, 'maxcor': 25}\n                )\n                z = res.x\n            except Exception:\n                # If optimizer fails, keep previous z\n                pass\n\n            centers_tmp = np.stack([z[:N], z[N:2*N]], axis=1)\n            centers_tmp = np.clip(centers_tmp, 1e-6, 1.0 - 1e-6)\n            r_cur = np.clip(z[2*N:], 0.0, 0.5)\n\n            # LP Feasibilization with cap to avoid overgrowth\n            cap_factor = 1.07 if si < len(P_schedule) - 1 else 1.12\n            r_cap = np.minimum(0.5, cap_factor * np.maximum(1e-9, r_cur))\n            r_lp = _lp_feasibilize(centers_tmp, r_max=r_cap, eps_pair=1e-6)\n\n            # Center-only relaxation with fixed LP radii\n            centers_relaxed = _center_only_relax(centers_tmp, r_lp, P=max(1000.0, P), w_b=1.0, w_o=1.0, iters=70)\n\n            # Final LP for this stage without cap\n            r_lp2 = _lp_feasibilize(centers_relaxed, r_max=None, eps_pair=1e-6)\n\n            z = np.concatenate([centers_relaxed[:, 0], centers_relaxed[:, 1], np.clip(r_lp2, 0.0, 0.5)])\n\n        # Final LP after annealing\n        centers_final_opt = np.stack([z[:N], z[N:2*N]], axis=1)\n        centers_final_opt = np.clip(centers_final_opt, 1e-6, 1.0 - 1e-6)\n        r_final = _lp_feasibilize(centers_final_opt, r_max=None, eps_pair=1e-6)\n\n        if _verify(centers_final_opt, r_final, tol_pair=1e-6):\n            s = float(np.sum(r_final))\n            if s > best['sum']:\n                best['centers'] = centers_final_opt.copy()\n                best['radii'] = r_final.copy()\n                best['sum'] = s\n        else:\n            # Try scaling down radii to achieve strict feasibility\n            d, _, _ = _pairwise_distances(centers_final_opt)\n            denom = r_final[:, None] + r_final[None, :]\n            with np.errstate(divide='ignore', invalid='ignore'):\n                ratio_pairs = np.where(denom > 1e-9, (d - 1.0e-6) / denom, np.inf)\n            iu = np.triu_indices(N, 1)\n            s_pairs = np.min(ratio_pairs[iu]) if ratio_pairs[iu].size > 0 else 1.0\n            if not np.isfinite(s_pairs):\n                s_pairs = 1.0\n            s_pairs = max(0.0, min(1.0, s_pairs))\n            bx = np.minimum.reduce([centers_final_opt[:,0], centers_final_opt[:,1], 1-centers_final_opt[:,0], 1-centers_final_opt[:,1]])\n            with np.errstate(divide='ignore', invalid='ignore'):\n                scale_b = np.min(np.where(r_final > 1e-9, (bx - 1e-6) / r_final, 1.0))\n            scale_b = min(1.0, scale_b if np.isfinite(scale_b) else 1.0)\n            scale_b = max(0.0, scale_b)\n            r_try = min(s_pairs, scale_b) * r_final\n            r_try = np.clip(r_try, 0.0, 0.5)\n            if _verify(centers_final_opt, r_try, tol_pair=1e-6):\n                s = float(np.sum(r_try))\n                if s > best['sum']:\n                    best['centers'] = centers_final_opt.copy()\n                    best['radii'] = r_try.copy()\n                    best['sum'] = s\n\n    # Fallback if none found\n    if best['centers'] is None:\n        centers_fb = _hex_initial_centers_from_rows([6,5,6,5,4], margin=0.06, jitter_scale=0.02, seed=123)\n        r_fb = _lp_feasibilize(centers_fb, r_max=None, eps_pair=1e-6)\n        if not _verify(centers_fb, r_fb, tol_pair=1e-6):\n            r_fb = 0.95 * r_fb\n        best['centers'] = centers_fb\n        best['radii'] = np.clip(r_fb, 0.0, 0.5)\n        best['sum'] = float(np.sum(best['radii']))\n\n    # Final strict LP to harden solution\n    radii_strict = _lp_feasibilize(best['centers'], r_max=None, eps_pair=1.1e-6)\n    if _verify(best['centers'], radii_strict, tol_pair=1e-6):\n        radii = radii_strict\n    else:\n        radii = best['radii']\n        if not _verify(best['centers'], radii, tol_pair=1e-6):\n            d, _, _ = _pairwise_distances(best['centers'])\n            denom = radii[:, None] + radii[None, :]\n            with np.errstate(divide='ignore', invalid='ignore'):\n                ratio_pairs = np.where(denom > 1e-9, (d - 1e-6) / denom, np.inf)\n            iu = np.triu_indices(N, 1)\n            s_pairs = np.min(ratio_pairs[iu]) if ratio_pairs[iu].size > 0 else 1.0\n            s_pairs = min(1.0, s_pairs if np.isfinite(s_pairs) else 1.0)\n            s_pairs = max(0.0, s_pairs)\n            bx = np.minimum.reduce([best['centers'][:,0], best['centers'][:,1], 1-best['centers'][:,0], 1-best['centers'][:,1]])\n            with np.errstate(divide='ignore', invalid='ignore'):\n                scale_b = np.min(np.where(radii > 1e-9, (bx - 1e-6) / radii, 1.0))\n            scale_b = min(1.0, scale_b if np.isfinite(scale_b) else 1.0)\n            scale_b = max(0.0, scale_b)\n            radii = min(s_pairs, scale_b) * radii\n            radii = np.clip(radii, 0.0, 0.5)\n\n    centers = best['centers'].astype(float)\n    radii = np.clip(radii.astype(float), 0.0, 0.5)\n    sum_radii = float(np.sum(radii))\n    # Final verification; if it fails, shrink uniformly until it passes (very small loop)\n    if not _verify(centers, radii, tol_pair=1e-6):\n        factor = 0.999\n        for _ in range(2000):\n            radii = radii * factor\n            if _verify(centers, radii, tol_pair=1e-6):\n                break\n    return centers, radii, float(np.sum(radii))\n\ndef run_packing():\n    centers, radii, sum_radii = construct_packing()\n    return centers, radii, sum_radii\n\n\nRequirements:\n- Single Python code block only. No prose. Implement construct_packing() as entry point.\n- Return (centers, radii, sum_radii) strictly feasible for 26 circles in [0,1]^2.\n- Use numpy, scipy only. No prints. Runtime <= 300s.\n- Use analytic, vectorized gradients if optimizing; apply eps only in divisions (not in penalties).\n- Finalize with an LP: maximize sum(r) s.t.\n  0 <= r_i <= bound_i where bound_i = min(x_i, y_i, 1-x_i, 1-y_i), and\n  r_i + r_j <= dist_ij - 1e-6 for all i<j.\n- Shapes: centers (26,2), radii (26,), sum_radii float.\n\nImplementation tips:\n- Multi-start on centers; seeds: hex grid, edge/corner-biased jitter, and refined best-known-like layouts.\n- Penalized objective for centers (optional radii vars) with annealed penalties; adaptive pairwise weights by local density.\n- Stable distances: use d = sqrt(||\u0394||^2 + eps^2); hinge = relu(overlap).\n- Cache pairwise indices; optionally k-NN masks for speed; widen mask over annealing.\n- LP: build A_ub once; warm-start bounds from current radii; use HiGHS method.\n\nFeasibility:\n- Always LP at end; clip tiny negatives to 0; verify non-overlap with 1e-6 tolerance.\n\n",
  "response": "```python\nimport numpy as np\nfrom scipy.optimize import minimize, linprog\n\ndef construct_packing():\n    \"\"\"\n    Construct a packing of 26 circles in the unit square.\n    Returns (centers, radii, sum_radii)\n    - centers: np.array (26,2)\n    - radii: np.array (26,)\n    - sum_radii: float\n    \"\"\"\n    # Constants\n    N = 26\n    tiny = 1e-12\n    eps_pair_final = 1e-6\n\n    # Utilities\n    def sigmoid(z):\n        return 1.0 / (1.0 + np.exp(-z))\n\n    def logit(p):\n        p = np.clip(p, 1e-9, 1.0 - 1e-9)\n        return np.log(p / (1.0 - p))\n\n    def _pairwise_distances(centers):\n        diff = centers[:, None, :] - centers[None, :, :]\n        dx = diff[..., 0]\n        dy = diff[..., 1]\n        d = np.sqrt(np.maximum(dx * dx + dy * dy, 0.0))\n        return d, dx, dy\n\n    def _compute_dynamic_weights(centers, tiny=1e-12, gamma_density=0.8, gamma_edge=0.6):\n        \"\"\"\n        Compute per-circle weights w_i based on local density and edge proximity.\n        Returns w (N,) with mean ~1.\n        \"\"\"\n        Nloc = centers.shape[0]\n        x = centers[:, 0]\n        y = centers[:, 1]\n        d, dx_mat, dy_mat = _pairwise_distances(centers)\n        k = min(6, max(1, Nloc - 1))\n        if Nloc <= 1:\n            density = np.ones(Nloc)\n        else:\n            # ignore self by adding large diag\n            sorted_d = np.sort(d + np.eye(Nloc) * 1e9, axis=1)\n            knn_mean = np.mean(sorted_d[:, :k], axis=1)\n            density_raw = 1.0 / (knn_mean + tiny)\n            density = density_raw / (np.mean(density_raw) + tiny)\n        proximity = np.minimum.reduce([x, y, 1.0 - x, 1.0 - y])\n        edge_factor = 1.0 - np.clip(4.0 * proximity, 0.0, 1.0)\n        w = 1.0 + gamma_density * (density - 1.0) + gamma_edge * edge_factor\n        w = np.clip(w, 0.2, 5.0)\n        w = w / (np.mean(w) + tiny)\n        return w\n\n    def _hex_initial_centers_from_rows(rows, margin=0.05, jitter_scale=0.03, seed=0):\n        assert sum(rows) == N\n        max_cols = max(rows)\n        nrows = len(rows)\n        dx_base = (1.0 - 2.0 * margin) / (max_cols - 1) if max_cols > 1 else 0.0\n        dy_base = (1.0 - 2.0 * margin) / (nrows - 1) if nrows > 1 else 0.0\n\n        centers = []\n        rng = np.random.default_rng(seed)\n        for ri, cols in enumerate(rows):\n            y = margin + ri * dy_base if nrows > 1 else 0.5\n            if cols > 1:\n                row_width = (cols - 1) * dx_base\n                startx = 0.5 - row_width / 2.0\n                if ri % 2 == 1:\n                    startx += dx_base / 2.0\n            else:\n                startx = 0.5\n            for k in range(cols):\n                x = startx + (k * dx_base if cols > 1 else 0.0)\n                centers.append([x, y])\n        centers = np.array(centers)\n        jitter = rng.normal(scale=jitter_scale, size=centers.shape)\n        centers = centers + jitter\n        centers = np.clip(centers, 1e-6, 1.0 - 1e-6)\n        return centers\n\n    def _multi_start_inits(max_inits=24):\n        patterns = [\n            [6,5,6,5,4], [6,5,5,5,5], [5,6,5,5,5],\n            [5,5,6,5,5], [6,6,5,5,4], [5,6,6,5,4],\n            [6,5,6,4,5], [5,5,5,5,6], [4,5,6,5,6],\n            [5,4,5,6,6], [5,6,4,6,5]\n        ]\n        params = [\n            (0.03, 0.02), (0.04, 0.03), (0.05, 0.03),\n            (0.06, 0.02), (0.07, 0.03), (0.05, 0.04),\n            (0.04, 0.025), (0.055, 0.035), (0.065, 0.025)\n        ]\n        inits = []\n        seed = 1\n        for pat in patterns:\n            for (m, j) in params:\n                c = _hex_initial_centers_from_rows(pat, margin=m, jitter_scale=j, seed=seed)\n                inits.append(c)\n                seed += 1\n                if len(inits) >= max_inits:\n                    break\n            if len(inits) >= max_inits:\n                break\n        # Add an edge biased seed\n        if len(inits) < max_inits:\n            rng = np.random.default_rng(999)\n            edge_positions = [\n                [0.02, 0.02], [0.02, 0.5], [0.02, 0.98], [0.5, 0.02],\n                [0.5, 0.98], [0.98, 0.02], [0.98, 0.5], [0.98, 0.98]\n            ]\n            interior = _hex_initial_centers_from_rows([6,6,6], margin=0.2, jitter_scale=0.03, seed=1234)\n            if interior.shape[0] >= 18:\n                interior = interior[:18]\n            else:\n                extra = np.tile(interior, (int(np.ceil(18 / max(1, interior.shape[0]))), 1))[:18]\n                interior = extra\n            centers = np.vstack([np.array(edge_positions), interior])\n            centers += rng.normal(scale=0.01, size=centers.shape)\n            centers = np.clip(centers, 1e-6, 1 - 1e-6)\n            inits.append(centers)\n        # ensure shapes\n        final = []\n        for arr in inits:\n            if arr.shape[0] == N:\n                final.append(arr)\n        # fallback\n        while len(final) < 4:\n            final.append(_hex_initial_centers_from_rows([6,5,6,5,4], margin=0.05, jitter_scale=0.02, seed=42+len(final)))\n        return final[:max_inits]\n\n    def _lp_maximize_sum_r(centers, eps_pair=eps_pair_final):\n        \"\"\"\n        Maximize sum(r) subject to:\n          0 <= r_i <= bound_i = min(x_i, y_i, 1-x_i, 1-y_i)\n          r_i + r_j <= d_ij - eps_pair for i<j\n        \"\"\"\n        Nloc = centers.shape[0]\n        x = centers[:, 0].copy()\n        y = centers[:, 1].copy()\n        bounds_var = [(0.0, 0.5)] * Nloc\n\n        A_rows = []\n        b_vals = []\n        # per-variable upper bounds via A_ub\n        bounds_caps = np.minimum.reduce([x, y, 1.0 - x, 1.0 - y])\n        # pairwise distances\n        d, _, _ = _pairwise_distances(centers)\n\n        # pairwise constraints: r_i + r_j <= d_ij - eps_pair\n        for i in range(Nloc):\n            for j in range(i+1, Nloc):\n                rhs = d[i, j] - eps_pair\n                if rhs < 0.0:\n                    # infeasible at this centers; zero radii fallback will be produced by LP\n                    rhs = -1.0  # forces infeasible, but we'll let LP handle\n                e = np.zeros(Nloc)\n                e[i] = 1.0; e[j] = 1.0\n                A_rows.append(e); b_vals.append(rhs)\n        # bound constraints as linear inequalities r_i <= bound_i\n        for i in range(Nloc):\n            e = np.zeros(Nloc)\n            e[i] = 1.0\n            A_rows.append(e); b_vals.append(bounds_caps[i])\n\n        if len(A_rows) == 0:\n            A_ub = None; b_ub = None\n        else:\n            A_ub = np.vstack(A_rows)\n            b_ub = np.array(b_vals)\n\n        c = -np.ones(Nloc)  # maximize sum -> minimize -sum\n        try:\n            res = linprog(c, A_ub=A_ub, b_ub=b_ub, bounds=bounds_var, method='highs')\n        except Exception:\n            res = None\n\n        if res is not None and res.success:\n            r = np.clip(res.x, 0.0, 0.5)\n            return r\n        # fallback: return zeros\n        return np.zeros(Nloc)\n\n    def _verify(centers, radii, tol_pair=1e-6):\n        Nloc = len(radii)\n        if centers.shape != (Nloc, 2):\n            return False\n        if np.any(radii < -1e-12):\n            return False\n        x = centers[:, 0]\n        y = centers[:, 1]\n        if np.any(x - radii < tol_pair):\n            return False\n        if np.any(y - radii < tol_pair):\n            return False\n        if np.any(x + radii > 1.0 - tol_pair):\n            return False\n        if np.any(y + radii > 1.0 - tol_pair):\n            return False\n        d, _, _ = _pairwise_distances(centers)\n        iu = np.triu_indices(Nloc, 1)\n        lhs = radii[:, None] + radii[None, :]\n        viol = lhs[iu] - d[iu] + tol_pair\n        if np.any(viol > 1e-12):\n            return False\n        return True\n\n    # Objective for centers-only parameterized by sigmoid(z)\n    # Using target radius proxy r_i = alpha * min(x, y, 1-x, 1-y)\n    def _objective_centers_sigmoid(z, alpha, P, w_b=1.0, w_o=1.0):\n        \"\"\"\n        z: length 2N (unbounded). x = sigmoid(z[:N]), y = sigmoid(z[N:2N])\n        Returns f, grad_z where grad_z is analytic gradient of f wrt z.\n        f = -sum(r_proxy) + P*(w_b * pen_b + pen_o)\n        \"\"\"\n        zx = z[:N]\n        zy = z[N:2*N]\n        x = sigmoid(zx)\n        y = sigmoid(zy)\n        centers = np.stack([x, y], axis=1)\n\n        # compute c = min(x, y, 1-x, 1-y) and its derivative\n        arr = np.vstack([x, y, 1.0 - x, 1.0 - y])  # shape (4, N)\n        argmin = np.argmin(arr, axis=0)  # 0->x,1->y,2->1-x,3->1-y\n        c = arr[argmin, np.arange(N)]\n        r = alpha * c  # proxy radii\n\n        # dynamic weights (we treat w_vec as constant for gradient to simplify)\n        w_vec = _compute_dynamic_weights(centers, tiny=tiny, gamma_density=0.8, gamma_edge=0.65)\n        # boundary weight per circle\n        prox = np.minimum.reduce([x, y, 1.0 - x, 1.0 - y])\n        w_b_vec = 1.0 + 0.4 * (1.0 - np.clip(4.0 * prox, 0.0, 1.0))\n        w_b_vec = w_b_vec * w_vec\n\n        # boundary hinge terms\n        s1 = r - x\n        s2 = r - y\n        s3 = r + x - 1.0\n        s4 = r + y - 1.0\n        h1 = np.maximum(0.0, s1)\n        h2 = np.maximum(0.0, s2)\n        h3 = np.maximum(0.0, s3)\n        h4 = np.maximum(0.0, s4)\n        pen_b_terms = 0.5 * (h1*h1 + h2*h2 + h3*h3 + h4*h4) * w_b_vec\n        pen_b = np.sum(pen_b_terms)\n\n        # pairwise overlaps\n        d, dx_mat, dy_mat = _pairwise_distances(centers)\n        iu = np.triu_indices(N, 1)\n        dab = d[iu]\n        dxab = dx_mat[iu]\n        dyab = dy_mat[iu]\n        d_safe = np.maximum(dab, tiny)\n\n        hij = r[iu[0]] + r[iu[1]] - dab\n        hpos = np.maximum(0.0, hij)\n        w_i = w_vec[iu[0]]\n        w_j = w_vec[iu[1]]\n        w_pair = w_o * (w_i * w_j)\n        pen_o = 0.5 * np.sum(w_pair * (hpos * hpos))\n\n        # total objective\n        f = -np.sum(r) + P * (w_b * pen_b + pen_o)\n\n        # Gradients:\n        # dr/dx and dr/dy from r = alpha * c\n        # dc/dx = 1 if argmin==0, -1 if argmin==2, else 0\n        # dc/dy = 1 if argmin==1, -1 if argmin==3, else 0\n        dc_dx = np.zeros(N)\n        dc_dy = np.zeros(N)\n        mask0 = (argmin == 0)\n        mask1 = (argmin == 1)\n        mask2 = (argmin == 2)\n        mask3 = (argmin == 3)\n        dc_dx[mask0] = 1.0\n        dc_dy[mask1] = 1.0\n        dc_dx[mask2] = -1.0\n        dc_dy[mask3] = -1.0\n        dr_dx = alpha * dc_dx\n        dr_dy = alpha * dc_dy\n\n        # Boundary gradients (w.r.t r, x, y) (we ignore gradient of w_b_vec)\n        dr_b = (h1 + h2 + h3 + h4) * w_b_vec  # derivative of penalty wrt r_i\n        dx_b = (-h1 + h3) * w_b_vec\n        dy_b = (-h2 + h4) * w_b_vec\n\n        # Overlap gradients\n        dx_o = np.zeros(N, dtype=float)\n        dy_o = np.zeros(N, dtype=float)\n        dr_o = np.zeros(N, dtype=float)\n        # coeff = hpos / d_safe\n        coeff = np.zeros_like(hpos)\n        mask_pos = (hpos > 0.0)\n        if np.any(mask_pos):\n            coeff[mask_pos] = (hpos[mask_pos] / d_safe[mask_pos])\n            coeff_w = coeff * w_pair\n            cx = coeff_w * dxab\n            cy = coeff_w * dyab\n            # center contributions\n            np.add.at(dx_o, iu[0], -cx)\n            np.add.at(dx_o, iu[1], +cx)\n            np.add.at(dy_o, iu[0], -cy)\n            np.add.at(dy_o, iu[1], +cy)\n            # radii contributions\n            np.add.at(dr_o, iu[0], w_pair * hpos)\n            np.add.at(dr_o, iu[1], w_pair * hpos)\n\n        # Total derivatives wrt x and y from explicit dependence in penalties\n        dx_total_pen = P * (w_b * dx_b + dx_o)\n        dy_total_pen = P * (w_b * dy_b + dy_o)\n        # derivative wrt r (for chain rule)\n        dr_total = -1.0 + P * (w_b * dr_b + dr_o)  # df/dr_i\n\n        # include chain contributions via r(x,y)\n        dx_total = dx_total_pen + dr_total * dr_dx\n        dy_total = dy_total_pen + dr_total * dr_dy\n\n        # Now chain through sigmoid: dx/dzx = x*(1-x)\n        dzx = dx_total * (x * (1.0 - x))\n        dzy = dy_total * (y * (1.0 - y))\n\n        grad = np.concatenate([dzx, dzy])\n        return float(f), grad\n\n    # Initialization seeds\n    inits = _multi_start_inits(max_inits=20)\n\n    # Stage schedules\n    alpha_schedule = [0.70, 0.80, 0.86, 0.92, 0.95]\n    P_schedule = [12.0, 80.0, 600.0, 2400.0, 6000.0]  # increasing penalties\n    maxiter_stage = [80, 100, 110, 120, 140]\n\n    best = {'centers': None, 'radii': None, 'sum': -1e9}\n\n    # Limit number of inits to control runtime\n    max_trials = min(12, len(inits))\n    for init_idx in range(max_trials):\n        centers0 = inits[init_idx].copy()\n        # convert to z initial\n        zx0 = logit(np.clip(centers0[:, 0], 1e-9, 1.0 - 1e-9))\n        zy0 = logit(np.clip(centers0[:, 1], 1e-9, 1.0 - 1e-9))\n        z = np.concatenate([zx0, zy0])  # length 2N\n\n        # annealing stages\n        for stage_idx, (alpha, P, maxit) in enumerate(zip(alpha_schedule, P_schedule, maxiter_stage)):\n            try:\n                res = minimize(\n                    fun=lambda zz: _objective_centers_sigmoid(zz, alpha=alpha, P=P, w_b=1.0, w_o=1.0),\n                    x0=z,\n                    method='L-BFGS-B',\n                    jac=True,\n                    options={'maxiter': maxit, 'ftol': 1e-12, 'gtol': 1e-8, 'maxcor': 20}\n                )\n                z = res.x\n            except Exception:\n                # ignore failures and continue\n                pass\n\n            # After stage, map to centers and run LP to get true radii\n            x = sigmoid(z[:N]); y = sigmoid(z[N:2*N])\n            centers_tmp = np.stack([x, y], axis=1)\n            centers_tmp = np.clip(centers_tmp, 1e-9, 1.0 - 1e-9)\n            r_lp = _lp_maximize_sum_r(centers_tmp, eps_pair=1e-6)\n\n            # Optionally perform a short centers-only relaxation keeping LP radii fixed:\n            # We'll do a single quick L-BFGS stage refining centers to reduce penalties for the LP radii.\n            def _obj_centers_for_fixed_r(zz):\n                zx = zz[:N]; zy = zz[N:2*N]\n                x = sigmoid(zx)\n                y = sigmoid(zy)\n                centers = np.stack([x, y], axis=1)\n                # compute simple penalty: boundary hinge and overlaps wrt fixed r_lp\n                r = r_lp\n                s1 = r - x\n                s2 = r - y\n                s3 = r + x - 1.0\n                s4 = r + y - 1.0\n                h1 = np.maximum(0.0, s1)\n                h2 = np.maximum(0.0, s2)\n                h3 = np.maximum(0.0, s3)\n                h4 = np.maximum(0.0, s4)\n                w_vec = _compute_dynamic_weights(centers, tiny=tiny, gamma_density=0.8, gamma_edge=0.65)\n                prox = np.minimum.reduce([x, y, 1.0 - x, 1.0 - y])\n                w_b_vec = 1.0 + 0.4 * (1.0 - np.clip(4.0 * prox, 0.0, 1.0))\n                w_b_vec = w_b_vec * w_vec\n                pen_b = 0.5 * np.sum((h1*h1 + h2*h2 + h3*h3 + h4*h4) * w_b_vec)\n\n                d, dx_mat, dy_mat = _pairwise_distances(centers)\n                iu = np.triu_indices(N, 1)\n                dab = d[iu]\n                hij = r[iu[0]] + r[iu[1]] - dab\n                hpos = np.maximum(0.0, hij)\n                w_i = w_vec[iu[0]]\n                w_j = w_vec[iu[1]]\n                w_pair = (w_i * w_j)\n                pen_o = 0.5 * np.sum(w_pair * (hpos * hpos))\n\n                f = P * (pen_b + pen_o)\n                # gradient (approximate analytic, but ignore gradient of w_vec for speed)\n                # boundary grads\n                dx_b = (-h1 + h3) * w_b_vec\n                dy_b = (-h2 + h4) * w_b_vec\n                dx_o = np.zeros(N); dy_o = np.zeros(N)\n                d_safe = np.maximum(d[iu], tiny)\n                mask_pos = (hpos > 0.0)\n                if np.any(mask_pos):\n                    coeff = np.zeros_like(hpos); coeff[mask_pos] = hpos[mask_pos] / d_safe[mask_pos]\n                    coeff_w = coeff * w_pair\n                    cx = coeff_w * dx_mat[iu]\n                    cy = coeff_w * dy_mat[iu]\n                    np.add.at(dx_o, iu[0], -cx); np.add.at(dx_o, iu[1], +cx)\n                    np.add.at(dy_o, iu[0], -cy); np.add.at(dy_o, iu[1], +cy)\n                dx_total = P * (dx_b + dx_o)\n                dy_total = P * (dy_b + dy_o)\n                dzx = dx_total * (x * (1.0 - x))\n                dzy = dy_total * (y * (1.0 - y))\n                grad = np.concatenate([dzx, dzy])\n                return float(f), grad\n\n            # run short relaxation\n            try:\n                res2 = minimize(\n                    fun=_obj_centers_for_fixed_r,\n                    x0=z,\n                    method='L-BFGS-B',\n                    jac=True,\n                    options={'maxiter': 40, 'ftol': 1e-10, 'gtol': 1e-8}\n                )\n                z = res2.x\n            except Exception:\n                pass\n\n        # After all stages, produce final centers and final LP\n        x = sigmoid(z[:N]); y = sigmoid(z[N:2*N])\n        centers_final = np.stack([x, y], axis=1)\n        centers_final = np.clip(centers_final, 1e-9, 1.0 - 1e-9)\n        r_final = _lp_maximize_sum_r(centers_final, eps_pair=eps_pair_final)\n\n        # verify, if not feasible try uniform shrink\n        if not _verify(centers_final, r_final, tol_pair=eps_pair_final):\n            # Try shrinking radii slightly\n            radii_try = r_final.copy()\n            factor = 0.9995\n            for _ in range(2000):\n                radii_try *= factor\n                if _verify(centers_final, radii_try, tol_pair=eps_pair_final):\n                    r_final = radii_try\n                    break\n\n        # Accept if feasible\n        if _verify(centers_final, r_final, tol_pair=eps_pair_final):\n            s = float(np.sum(r_final))\n            if s > best['sum']:\n                best['centers'] = centers_final.copy()\n                best['radii'] = r_final.copy()\n                best['sum'] = s\n\n    # If none found, fallback deterministic hex and LP\n    if best['centers'] is None:\n        centers_fb = _hex_initial_centers_from_rows([6,5,6,5,4], margin=0.06, jitter_scale=0.02, seed=123)\n        r_fb = _lp_maximize_sum_r(centers_fb, eps_pair=eps_pair_final)\n        if not _verify(centers_fb, r_fb, tol_pair=eps_pair_final):\n            # scale down until feasible\n            fac = 0.995\n            for _ in range(2000):\n                r_fb *= fac\n                if _verify(centers_fb, r_fb, tol_pair=eps_pair_final):\n                    break\n        best['centers'] = centers_fb\n        best['radii'] = r_fb\n        best['sum'] = float(np.sum(r_fb))\n\n    # Final strict LP enforcement once more and final verification\n    centers = best['centers'].astype(float)\n    radii = _lp_maximize_sum_r(centers, eps_pair=eps_pair_final)\n    if not _verify(centers, radii, tol_pair=eps_pair_final):\n        # try scaling down radii slightly\n        fac = 0.9995\n        for _ in range(5000):\n            radii = radii * fac\n            if _verify(centers, radii, tol_pair=eps_pair_final):\n                break\n    radii = np.clip(radii, 0.0, 0.5)\n    sum_radii = float(np.sum(radii))\n\n    # final safety: if still infeasible, zero out and return fallback\n    if not _verify(centers, radii, tol_pair=eps_pair_final):\n        centers = _hex_initial_centers_from_rows([6,5,6,5,4], margin=0.08, jitter_scale=0.01, seed=321)\n        radii = _lp_maximize_sum_r(centers, eps_pair=eps_pair_final)\n        if not _verify(centers, radii, tol_pair=eps_pair_final):\n            # ultimate fallback uniform shrink until feasible\n            radii = radii.copy()\n            fac = 0.995\n            for _ in range(10000):\n                radii *= fac\n                if _verify(centers, radii, tol_pair=eps_pair_final):\n                    break\n        sum_radii = float(np.sum(radii))\n\n    return centers, radii, float(sum_radii)\n```",
  "reasoning": "",
  "timestamp": "2026-02-11T03:05:19.565703",
  "cost_data": null,
  "model_config": {
    "model": "openai/gpt-5-mini",
    "temperature": 0.25
  },
  "llm_call_duration_s": 125.51565081998706,
  "eval_duration_s": 2.569746299064718
}