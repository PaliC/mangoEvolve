{
  "trial_id": "trial_14_1",
  "generation": 14,
  "parent_id": null,
  "code": "import numpy as np\nfrom scipy.optimize import minimize, linprog\n\ndef construct_packing():\n    N = 26\n    eps_dist = 1e-8\n    eps_pair = 1e-6\n    # Precompute pair indices\n    iu, ju = np.triu_indices(N, 1)\n    # Helper: hex initial\n    def _hex(rows, margin, jitter, seed):\n        maxc = max(rows); nr = len(rows)\n        dx = (1-2*margin)/(maxc-1) if maxc>1 else 0\n        dy = (1-2*margin)/(nr-1) if nr>1 else 0\n        rng = np.random.default_rng(seed)\n        C = []\n        for i, cnt in enumerate(rows):\n            y = margin + (dy*i if nr>1 else 0.5)\n            stx = margin + (0.5*dx if cnt<maxc else 0)\n            for k in range(cnt):\n                x = stx + (k*dx if cnt>1 else 0.5-margin)\n                C.append([x, y])\n        C = np.array(C)\n        C += rng.normal(scale=jitter, size=C.shape)\n        return np.clip(C, 1e-3, 1-1e-3)\n    # Helper: edge initial\n    def _edge():\n        pts = []\n        m = 0.01\n        n4 = int(np.ceil(N/4))\n        xs = np.linspace(m,1-m,n4)\n        ys = np.linspace(m,1-m,n4)\n        for x in xs: pts.append([x,m])\n        for y in ys: pts.append([1-m,y])\n        for x in xs: pts.append([x,1-m])\n        for y in ys: pts.append([m,y])\n        return np.array(pts[:N])\n    # Helper: random initial\n    def _rand(seed):\n        rng = np.random.default_rng(seed)\n        return rng.random((N,2))*0.98+0.01\n    # Seeds\n    inits = [\n        _hex([6,5,6,5,4],0.05,0.03,0),\n        _hex([6,5,6,5,4],0.07,0.04,1),\n        _edge(),\n        _rand(2),\n        _rand(3),\n        _hex([5,6,5,5,5],0.06,0.02,4)\n    ]\n    # Pairwise distance and diffs\n    def _pairwise(C):\n        dxy = C[:,None,:]-C[None,:,:]\n        dx = dxy[:,:,0]; dy = dxy[:,:,1]\n        dab = np.hypot(dx,dy)\n        dsv = np.sqrt(dab*dab+eps_dist*eps_dist)\n        return dab, dsv, dx, dy\n    # Build k-NN mask\n    def _knn_mask(C,k):\n        dab,_,_,_ = _pairwise(C)\n        N = dab.shape[0]\n        m = np.full((N,N), False)\n        for i in range(N):\n            idx = np.argpartition(dab[i], k+1)[:k+1]\n            for j in idx:\n                if j!=i: m[i,j]=m[j,i]=True\n        mask = m[iu,ju]\n        return mask\n    # Objective and grad for centers only\n    def _obj(z, r, P, w_b, w_o, mask):\n        x = z[:N]; y = z[N:]\n        # boundary\n        s1 = r-x; s2 = r-y; s3 = r+x-1; s4 = r+y-1\n        h1 = np.maximum(0,s1); h2 = np.maximum(0,s2)\n        h3 = np.maximum(0,s3); h4 = np.maximum(0,s4)\n        pen_b = 0.5*(h1*h1+h2*h2+h3*h3+h4*h4).sum()\n        dx_b = -h1+h3; dy_b = -h2+h4\n        # pairs\n        C = np.stack([x,y],1)\n        dab, dsv, dxm, dym = _pairwise(C)\n        dij = dab[iu,ju]; dsvij = dsv[iu,ju]\n        dxij = dxm[iu,ju]; dyij = dym[iu,ju]\n        hij = r[iu]+r[ju]-dij; hpos = np.maximum(0,hij)\n        active = mask\n        pen_o = 0.5*(hpos[active]**2).sum()\n        # gradients\n        dx_o = np.zeros(N); dy_o = np.zeros(N)\n        # radii fixed, so no dr\n        coeff = hpos[active]/dsvij[active]\n        cx = coeff*dxij[active]; cy = coeff*dyij[active]\n        np.add.at(dx_o, iu[active], -cx); np.add.at(dx_o, ju[active], +cx)\n        np.add.at(dy_o, iu[active], -cy); np.add.at(dy_o, ju[active], +cy)\n        g = np.concatenate([P*(w_b*dx_b + w_o*dx_o),\n                            P*(w_b*dy_b + w_o*dy_o)])\n        f = P*(w_b*pen_b + w_o*pen_o)\n        return f, g\n    # LP for radii\n    def _lp(C):\n        dab,_,_,_ = _pairwise(C)\n        bnd = np.minimum.reduce([C[:,0],C[:,1],1-C[:,0],1-C[:,1]])\n        A = []; b=[]\n        for i,j in zip(iu,ju):\n            row = np.zeros(N); row[i]=row[j]=1\n            A.append(row); b.append(dab[i,j]-eps_pair)\n        A = np.array(A); b=np.array(b)\n        c = -np.ones(N)\n        bounds = [(0,bnd[i]) for i in range(N)]\n        res = linprog(c,A_ub=A,b_ub=b,bounds=bounds,method='highs')\n        if res.success: return np.clip(res.x,0,None)\n        return np.zeros(N)\n    # verify\n    def _verify(C,r):\n        if C.shape!=(N,2) or r.shape!=(N,): return False\n        if np.any(r<-1e-9): return False\n        if np.any(C[:,0]-r<-1e-8) or np.any(C[:,1]-r<-1e-8): return False\n        if np.any(C[:,0]+r>1+1e-8) or np.any(C[:,1]+r>1+1e-8): return False\n        dab,_,_,_ = _pairwise(C)\n        if np.any(r[:,None]+r[None,:]-dab>1e-6): return False\n        return True\n\n    best = {'sum': -1, 'C':None, 'r':None}\n    # penalty stages\n    Psch = [1e2,1e3,1e4]; ks=[6,12,N]; w_b=1.0; w_o=1.0\n    for C0 in inits:\n        C = C0.copy()\n        r = _lp(C)\n        for P,k in zip(Psch,ks):\n            mask = _knn_mask(C,k)\n            z0 = np.r_[C[:,0],C[:,1]]\n            bnds = [(0,1)]*(2*N)\n            fun = lambda z: _obj(z,r,P,w_b,w_o,mask)\n            res = minimize(fun,z0,method='trust-constr',jac=True,bounds=bnds,\n                           options={'maxiter':60,'gtol':1e-8,'verbose':0})\n            z = res.x\n            C = np.stack([z[:N],z[N:]],1)\n            r = _lp(C)\n        # final\n        r = _lp(C)\n        if _verify(C,r):\n            s = r.sum()\n            if s>best['sum']:\n                best.update({'sum':s,'C':C.copy(),'r':r.copy()})\n    if best['C'] is None:\n        C = inits[0]; r=_lp(C); best.update({'sum':r.sum(),'C':C,'r':r})\n    return best['C'], best['r'], float(best['sum'])",
  "metrics": {
    "valid": false,
    "score": 0.0,
    "eval_time": 0.5585105419158936,
    "error": "ValueError: kth(=27) out of bounds (26)"
  },
  "prompt": "\nWrite code that alternates:\n- trust-constr on centers only with explicit Jacobian of active smooth penalty (pairwise overlaps + boundary), using 3 penalty stages with increasing weights and k-NN expansion;\n- LP to recompute optimal radii after each stage;\n- final LP for feasibility.\nUse vectorized gradients/Jacobians: distances stabilized via sqrt(d2 + eps^2); eps only in denominators. Initialize from hex and edge-biased seeds; keep <= 6 seeds, <= 3 stages, short trust-constr iters (e.g., maxiter=60 per stage). Return best LP result.\n\nYou may reuse structure from these references but implement cleanly as described:\n# Ref (trial_5_0):\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.optimize import linprog\n\ndef _pairwise_distances(centers):\n    diff = centers[:, None, :] - centers[None, :, :]\n    d = np.sqrt(np.sum(diff**2, axis=2))\n    return d, diff[..., 0], diff[..., 1]\n\ndef _objective_and_grad(z, N, P, w_b=1.0, w_o=1.0):\n    # z = [x(0..N-1), y(0..N-1), r(0..N-1)]\n    x = z[:N]\n    y = z[N:2*N]\n    r = z[2*N:]\n    tiny = 1e-12\n\n    # Boundary penalties: hinges\n    s1 = r - x\n    s2 = r - y\n    s3 = r + x - 1.0\n    s4 = r + y - 1.0\n\n    h1 = np.maximum(0.0, s1)\n    h2 = np.maximum(0.0, s2)\n    h3 = np.maximum(0.0, s3)\n    h4 = np.maximum(0.0, s4)\n\n    pen_b = 0.5 * np.sum(h1*h1 + h2*h2 + h3*h3 + h4*h4)\n\n    # Overlap penalties: hinges for i<j\n    centers = np.stack([x, y], axis=1)\n    d, dx_mat, dy_mat = _pairwise_distances(centers)\n    iu = np.triu_indices(N, 1)\n    dab = d[iu]\n    dxab = dx_mat[iu]\n    dyab = dy_mat[iu]\n    d_safe = np.maximum(dab, tiny)\n    hij = r[iu[0]] + r[iu[1]] - dab\n    hpos = np.maximum(0.0, hij)\n    pen_o = 0.5 * np.sum(hpos*hpos)\n\n    # Objective\n    f = -np.sum(r) + P * (w_b * pen_b + w_o * pen_o)\n\n    # Gradients for boundary penalties\n    dr_b = h1 + h2 + h3 + h4\n    dx_b = -h1 + h3\n    dy_b = -h2 + h4\n\n    # Overlap gradients (accumulate with np.add.at)\n    dx_o = np.zeros(N, dtype=float)\n    dy_o = np.zeros(N, dtype=float)\n    dr_o = np.zeros(N, dtype=float)\n\n    # For centers:\n    # grad_x_a += -h * (dxab/d), grad_x_b += +h * (dxab/d)\n    # grad_y_a += -h * (dyab/d), grad_y_b += +h * (dyab/d)\n    coeff = hpos / d_safe\n    cx = coeff * dxab\n    cy = coeff * dyab\n    np.add.at(dx_o, iu[0], -cx)\n    np.add.at(dx_o, iu[1], +cx)\n    np.add.at(dy_o, iu[0], -cy)\n    np.add.at(dy_o, iu[1], +cy)\n    # For radii:\n    np.add.at(dr_o, iu[0], hpos)\n    np.add.at(dr_o, iu[1], hpos)\n\n    dx_total = P * (w_b * dx_b + w_o * dx_o)\n    dy_total = P * (w_b * dy_b + w_o * dy_o)\n    dr_total = -1.0 + P * (w_b * dr_b + w_o * dr_o)\n\n    grad = np.concatenate([dx_total, dy_total, dr_total])\n    return f, grad\n\ndef _objective_centers_only(z_xy, r_fixed, P, w_b=1.0, w_o=1.0):\n    # z_xy = [x(0..N-1), y(0..N-1)], radii fixed\n    N = r_fixed.shape[0]\n    x = z_xy[:N]\n    y = z_xy[N:2*N]\n    r = r_fixed\n    tiny = 1e-12\n\n    # Boundary hinges\n    s1 = r - x\n    s2 = r - y\n    s3 = r + x - 1.0\n    s4 = r + y - 1.0\n    h1 = np.maximum(0.0, s1)\n    h2 = np.maximum(0.0, s2)\n    h3 = np.maximum(0.0, s3)\n    h4 = np.maximum(0.0, s4)\n    pen_b = 0.5 * np.sum(h1*h1 + h2*h2 + h3*h3 + h4*h4)\n\n    # Overlap hinges\n    centers = np.stack([x, y], axis=1)\n    d, dx_mat, dy_mat = _pairwise_distances(centers)\n    iu = np.triu_indices(N, 1)\n    dab = d[iu]\n    dxab = dx_mat[iu]\n    dyab = dy_mat[iu]\n    d_safe = np.maximum(dab, tiny)\n    hij = r[iu[0]] + r[iu[1]] - dab\n    hpos = np.maximum(0.0, hij)\n    pen_o = 0.5 * np.sum(hpos*hpos)\n\n    f = P * (w_b * pen_b + w_o * pen_o)\n\n    # Gradients wrt x,y only\n    dx_b = -h1 + h3\n    dy_b = -h2 + h4\n\n    dx_o = np.zeros(N, dtype=float)\n    dy_o = np.zeros(N, dtype=float)\n    coeff = hpos / d_safe\n    cx = coeff * dxab\n    cy = coeff * dyab\n    np.add.at(dx_o, iu[0], -cx)\n    np.add.at(dx_o, iu[1], +cx)\n    np.add.at(dy_o, iu[0], -cy)\n    np.add.at(dy_o, iu[1], +cy)\n\n    dx_total = P * (w_b * dx_b + w_o * dx_o)\n    dy_total = P * (w_b * dy_b + w_o * dy_o)\n    g = np.concatenate([dx_total, dy_total])\n    return f, g\n\ndef _lp_feasibilize(centers, r_max=None, eps_pair=1e-6):\n    \"\"\"\n    Maximize sum(r) subject to:\n      r_i >= 0\n      r_i <= r_max[i] (if provided)\n      r_i <= x_i, r_i <= y_i, r_i <= 1-x_i, r_i <= 1-y_i\n      r_i + r_j <= dist(c_i,c_j) - eps_pair\n    \"\"\"\n    N = centers.shape[0]\n    x = centers[:, 0]\n    y = centers[:, 1]\n\n    # Build A_ub, b_ub\n    A_rows = []\n    b_vals = []\n\n    # Boundary constraints\n    for i in range(N):\n        e = np.zeros(N); e[i] = 1.0\n        A_rows.append(e.copy()); b_vals.append(x[i])\n        A_rows.append(e.copy()); b_vals.append(y[i])\n        A_rows.append(e.copy()); b_vals.append(1.0 - x[i])\n        A_rows.append(e.copy()); b_vals.append(1.0 - y[i])\n\n    # Pairwise constraints\n    d, _, _ = _pairwise_distances(centers)\n    for i in range(N):\n        for j in range(i+1, N):\n            e = np.zeros(N); e[i] = 1.0; e[j] = 1.0\n            A_rows.append(e); b_vals.append(d[i, j] - eps_pair)\n\n    # Optional upper bounds r_i <= r_max[i]\n    if r_max is not None:\n        for i in range(N):\n            e = np.zeros(N); e[i] = 1.0\n            A_rows.append(e); b_vals.append(r_max[i])\n\n    A_ub = np.array(A_rows)\n    b_ub = np.array(b_vals)\n\n    # Bounds on r: [0, 0.5]\n    bounds = [(0.0, 0.5) for _ in range(N)]\n    c = -np.ones(N)\n\n    res = linprog(c, A_ub=A_ub, b_ub=b_ub, bounds=bounds, method='highs')\n    if res.success:\n        return np.clip(res.x, 0.0, 0.5)\n\n    # Retry with eps_pair = 0 to get a feasible base, then scale to satisfy eps_pair\n    b_vals2 = []\n    A_rows2 = []\n    for i in range(N):\n        e = np.zeros(N); e[i] = 1.0\n        A_rows2.append(e.copy()); b_vals2.append(x[i])\n        A_rows2.append(e.copy()); b_vals2.append(y[i])\n        A_rows2.append(e.copy()); b_vals2.append(1.0 - x[i])\n        A_rows2.append(e.copy()); b_vals2.append(1.0 - y[i])\n\n    for i in range(N):\n        for j in range(i+1, N):\n            e = np.zeros(N); e[i] = 1.0; e[j] = 1.0\n            A_rows2.append(e); b_vals2.append(d[i, j])\n\n    if r_max is not None:\n        for i in range(N):\n            e = np.zeros(N); e[i] = 1.0\n            A_rows2.append(e); b_vals2.append(r_max[i])\n\n    A_ub2 = np.array(A_rows2)\n    b_ub2 = np.array(b_vals2)\n\n    res2 = linprog(c, A_ub=A_ub2, b_ub=b_ub2, bounds=bounds, method='highs')\n    if not res2.success:\n        return np.zeros(N)\n\n    r_base = np.clip(res2.x, 0.0, 0.5)\n    d, _, _ = _pairwise_distances(centers)\n    denom = r_base[:, None] + r_base[None, :]\n    with np.errstate(divide='ignore', invalid='ignore'):\n        ratio = np.where(denom > 0, (d - eps_pair) / denom, np.inf)\n    iu = np.triu_indices(N, 1)\n    if ratio[iu].size > 0:\n        s_pairs = np.min(ratio[iu])\n        if not np.isfinite(s_pairs):\n            s_pairs = 1.0\n        s = min(1.0, s_pairs)\n    else:\n        s = 1.0\n    s = max(0.0, s)\n    r = s * r_base\n    return np.clip(r, 0.0, 0.5)\n\ndef _verify(centers, radii, tol_pair=1e-6):\n    N = len(radii)\n    x = centers[:, 0]\n    y = centers[:, 1]\n    if centers.shape != (N, 2):\n        return False\n    if np.any(radii < -1e-12):\n        return False\n    if np.any(x - radii < -1e-9):\n        return False\n    if np.any(y - radii < -1e-9):\n        return False\n    if np.any(x + radii > 1.0 + 1e-9):\n        return False\n    if np.any(y + radii > 1.0 + 1e-9):\n        return False\n    d, _, _ = _pairwise_distances(centers)\n    iu = np.triu_indices(N, 1)\n    lhs = radii[:, None] + radii[None, :]\n    viol = lhs[iu] - d[iu] + tol_pair\n    if np.any(viol > 1e-12):\n        return False\n    return True\n\ndef _hex_initial_centers_from_rows(rows, margin=0.05, jitter_scale=0.03, seed=0):\n    # rows: list of counts per row, centered with hex offset when fewer than max\n    assert sum(rows) == 26\n    max_cols = max(rows)\n    nrows = len(rows)\n    dx = (1.0 - 2*margin) / (max_cols - 1) if max_cols > 1 else 0.0\n    dy = (1.0 - 2*margin) / (nrows - 1) if nrows > 1 else 0.0\n    centers = []\n    rng = np.random.default_rng(seed)\n    for ri, cols in enumerate(rows):\n        y = margin + ri * dy if nrows > 1 else 0.5\n        if cols == max_cols:\n            startx = margin\n        else:\n            startx = margin + 0.5 * dx\n        if cols == 1:\n            startx = 0.5\n        for k in range(cols):\n            x = startx + (k * dx if cols > 1 else 0.0)\n            centers.append([x, y])\n    centers = np.array(centers)\n    # Jitter and clip\n    jitter = rng.normal(scale=jitter_scale, size=centers.shape)\n    centers = centers + jitter\n    centers = np.clip(centers, 1e-3, 1 - 1e-3)\n    return centers\n\ndef _multi_start_inits(N):\n    # Various row patterns summing to 26\n    patterns = [\n        [6, 5, 6, 5, 4],\n        [6, 5, 5, 5, 5],\n        [5, 6, 5, 5, 5],\n        [5, 5, 6, 5, 5],\n        [6, 6, 5, 5, 4],\n        [5, 6, 6, 5, 4],\n        [6, 5, 6, 4, 5],\n    ]\n    params = [\n        (0.03, 0.02), (0.04, 0.03), (0.05, 0.03),\n        (0.06, 0.02), (0.07, 0.03), (0.05, 0.04),\n    ]\n    inits = []\n    seed = 0\n    for pat in patterns:\n        for (m, j) in params:\n            inits.append(_hex_initial_centers_from_rows(pat, margin=m, jitter_scale=j, seed=seed))\n            seed += 1\n            if len(inits) >= 16:  # Limit number of starts to keep runtime in check\n                return inits\n    return inits\n\ndef _center_only_relax(centers, r_fixed, P=2000.0, w_b=1.0, w_o=1.0, iters=80):\n    N = centers.shape[0]\n    z0 = np.concatenate([centers[:, 0], centers[:, 1]])\n    bounds_xy = [(0.0, 1.0)] * (2 * N)\n    res = minimize(\n        fun=lambda zz: _objective_centers_only(zz, r_fixed, P, w_b, w_o),\n        x0=z0,\n        method='L-BFGS-B',\n        jac=True,\n        bounds=bounds_xy,\n        options={'maxiter': iters, 'ftol': 1e-12, 'gtol': 1e-8, 'maxcor': 20}\n    )\n    z = res.x\n    centers_new = np.stack([z[:N], z[N:2*N]], axis=1)\n    return centers_new\n\ndef construct_packing():\n    N = 26\n    best = {'centers': None, 'radii': None, 'sum': -np.inf}\n\n    # Penalty schedule and bounds\n    P_schedule = [8.0, 80.0, 800.0, 3000.0, 10000.0]\n    maxiter_stage = [90, 100, 110, 120, 130]\n    bounds = [(0.0, 1.0)] * (2*N) + [(0.0, 0.5)] * N\n\n    # Weighting of penalties\n    w_b = 1.0\n    w_o = 1.0\n\n    # Initializations\n    inits = _multi_start_inits(N)\n    if not inits:\n        inits = [_hex_initial_centers_from_rows([6,5,6,5,4], margin=0.05, jitter_scale=0.03, seed=42)]\n\n    for init_idx, centers0 in enumerate(inits):\n        x0 = centers0[:, 0]\n        y0 = centers0[:, 1]\n        # Start with small radii within local boundary\n        base_bound = np.minimum.reduce([x0, y0, 1 - x0, 1 - y0])\n        r0 = 0.05 * base_bound\n        z = np.concatenate([x0, y0, r0])\n\n        # Annealing optimization with periodic LP tightening and center-only relaxation\n        for si, P in enumerate(P_schedule):\n            res = minimize(\n                fun=lambda zz: _objective_and_grad(zz, N, P, w_b, w_o),\n                x0=z,\n                method='L-BFGS-B',\n                jac=True,\n                bounds=bounds,\n                options={'maxiter': maxiter_stage[si], 'ftol': 1e-12, 'gtol': 1e-8, 'maxcor': 30}\n            )\n            z = res.x\n            centers_tmp = np.stack([z[:N], z[N:2*N]], axis=1)\n            r_cur = np.clip(z[2*N:], 0.0, 0.5)\n            # Conservative LP cap slightly above current radii\n            cap_factor = 1.08 if si < len(P_schedule) - 1 else 1.15\n            r_cap = np.minimum(0.5, cap_factor * np.maximum(1e-9, r_cur))\n            r_lp = _lp_feasibilize(centers_tmp, r_max=r_cap, eps_pair=1e-6)\n            # Small center-only relaxation to open space, then re-LP\n            centers_relaxed = _center_only_relax(centers_tmp, r_lp, P=max(1000.0, P), w_b=w_b, w_o=w_o, iters=70)\n            r_lp2 = _lp_feasibilize(centers_relaxed, r_max=None, eps_pair=1e-6)\n            z = np.concatenate([centers_relaxed[:, 0], centers_relaxed[:, 1], np.clip(r_lp2, 0.0, 0.5)])\n\n        # Final LP without cap to fully utilize geometry\n        centers_cur = np.stack([z[:N], z[N:2*N]], axis=1)\n        r_final = _lp_feasibilize(centers_cur, r_max=None, eps_pair=1e-6)\n\n        # Verify and update best\n        if _verify(centers_cur, r_final, tol_pair=1e-6):\n            s = float(np.sum(r_final))\n            if s > best['sum']:\n                best['centers'] = centers_cur.copy()\n                best['radii'] = r_final.copy()\n                best['sum'] = s\n        else:\n            # Try slight shrink and re-LP to fix tiny violations\n            d, _, _ = _pairwise_distances(centers_cur)\n            denom = r_final[:, None] + r_final[None, :]\n            with np.errstate(divide='ignore', invalid='ignore'):\n                ratio = np.where(denom > 0, (d - 1.0e-6) / denom, np.inf)\n            iu = np.triu_indices(N, 1)\n            s_pairs = np.min(ratio[iu]) if ratio[iu].size > 0 else 1.0\n            if not np.isfinite(s_pairs):\n                s_pairs = 1.0\n            s_pairs = max(0.0, min(1.0, s_pairs))\n            r_try = s_pairs * r_final\n            # Boundary scaling\n            bx = np.minimum.reduce([centers_cur[:,0], centers_cur[:,1], 1-centers_cur[:,0], 1-centers_cur[:,1]])\n            with np.errstate(divide='ignore', invalid='ignore'):\n                scale_b = np.min(np.where(r_try > 0, bx / r_try, 1.0))\n            scale_b = min(1.0, scale_b if np.isfinite(scale_b) else 1.0)\n            r_try = scale_b * r_try\n            if _verify(centers_cur, r_try, tol_pair=1e-6):\n                s = float(np.sum(r_try))\n                if s > best['sum']:\n                    best['centers'] = centers_cur.copy()\n                    best['radii'] = r_try.copy()\n                    best['sum'] = s\n\n    # Fallback if nothing verified\n    if best['centers'] is None:\n        centers_fb = _hex_initial_centers_from_rows([6,5,6,5,4], margin=0.06, jitter_scale=0.02, seed=123)\n        r_fb = _lp_feasibilize(centers_fb, r_max=None, eps_pair=1e-6)\n        if not _verify(centers_fb, r_fb, tol_pair=1e-6):\n            r_fb = 0.95 * r_fb\n        best['centers'] = centers_fb\n        best['radii'] = np.clip(r_fb, 0.0, 0.5)\n        best['sum'] = float(np.sum(best['radii']))\n\n    # Ensure strict feasibility and final verify\n    radii_strict = _lp_feasibilize(best['centers'], r_max=None, eps_pair=1.1e-6)\n    if _verify(best['centers'], radii_strict, tol_pair=1e-6):\n        radii = radii_strict\n    else:\n        radii = best['radii']\n\n    centers = best['centers']\n    # Final hardening: if any violation persists, scale radii conservatively\n    if not _verify(centers, radii, tol_pair=1e-6):\n        d, _, _ = _pairwise_distances(centers)\n        denom = radii[:, None] + radii[None, :]\n        with np.errstate(divide='ignore', invalid='ignore'):\n            ratio = np.where(denom > 0, (d - 1e-6) / denom, np.inf)\n        iu = np.triu_indices(N, 1)\n        s_pairs = np.min(ratio[iu]) if ratio[iu].size > 0 else 1.0\n        s_pairs = min(1.0, s_pairs if np.isfinite(s_pairs) else 1.0)\n        s_pairs = max(0.0, s_pairs)\n        radii = s_pairs * radii\n        bx = np.minimum.reduce([centers[:,0], centers[:,1], 1-centers[:,0], 1-centers[:,1]])\n        with np.errstate(divide='ignore', invalid='ignore'):\n            scale_b = np.min(np.where(radii > 0, bx / radii, 1.0))\n        scale_b = min(1.0, scale_b if np.isfinite(scale_b) else 1.0)\n        radii = scale_b * radii\n\n    radii = np.clip(radii, 0.0, 0.5)\n    sum_radii = float(np.sum(radii))\n    return centers.astype(float), radii.astype(float), sum_radii\n\ndef run_packing():\n    centers, radii, sum_radii = construct_packing()\n    return centers, radii, sum_radii\n# Ref (trial_13_6):\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.optimize import linprog\n\ndef construct_packing():\n    \"\"\"Return (centers, radii, sum_radii) with:\n        - centers: np.array shape (26, 2)\n        - radii:   np.array shape (26,)\n        - sum_radii: float, strictly feasible sum of radii\n    \"\"\"\n    # ---------------- Helper functions ----------------\n    def _pairwise_distances(centers):\n        diff = centers[:, None, :] - centers[None, :, :]\n        d = np.sqrt(np.sum(diff**2, axis=2))\n        return d, diff[..., 0], diff[..., 1]\n\n    def _density_weights(centers, k=5, eps=1e-12):\n        # Local density proxy: inverse of mean k-NN distance\n        d, _, _ = _pairwise_distances(centers)\n        N = centers.shape[0]\n        # Exclude self-distances by setting to inf then sort\n        d2 = d.copy()\n        np.fill_diagonal(d2, np.inf)\n        # k-NN mean distances\n        # For N <= k, fallback to finite minima\n        k_eff = min(k, max(1, N - 1))\n        # Partition to get k smallest\n        knn = np.partition(d2, kth=k_eff-1, axis=1)[:, :k_eff]\n        m = np.mean(knn, axis=1)\n        dens = 1.0 / (m + eps)\n        return dens\n\n    def _objective_and_grad(z, N, P, w_b=1.0, w_o=1.0, use_density=True):\n        # z = [x(0..N-1), y(0..N-1), r(0..N-1)]\n        x = z[:N]\n        y = z[N:2*N]\n        r = z[2*N:]\n        tiny = 1e-12\n\n        # Boundary penalties: hinges\n        s1 = r - x\n        s2 = r - y\n        s3 = r + x - 1.0\n        s4 = r + y - 1.0\n\n        h1 = np.maximum(0.0, s1)\n        h2 = np.maximum(0.0, s2)\n        h3 = np.maximum(0.0, s3)\n        h4 = np.maximum(0.0, s4)\n\n        pen_b = 0.5 * np.sum(h1*h1 + h2*h2 + h3*h3 + h4*h4)\n\n        # Overlap penalties: hinges for i<j\n        centers = np.stack([x, y], axis=1)\n        d, dx_mat, dy_mat = _pairwise_distances(centers)\n        iu = np.triu_indices(N, 1)\n        dab = d[iu]\n        dxab = dx_mat[iu]\n        dyab = dy_mat[iu]\n        d_safe = np.maximum(dab, tiny)\n        hij = r[iu[0]] + r[iu[1]] - dab\n        hpos = np.maximum(0.0, hij)\n\n        if use_density:\n            dens = _density_weights(centers, k=5, eps=tiny)\n            wij = 0.5 * (dens[iu[0]] + dens[iu[1]])\n        else:\n            wij = 1.0\n\n        pen_o = 0.5 * np.sum(wij * (hpos*hpos))\n\n        # Objective\n        f = -np.sum(r) + P * (w_b * pen_b + w_o * pen_o)\n\n        # Gradients for boundary penalties\n        dr_b = h1 + h2 + h3 + h4\n        dx_b = -h1 + h3\n        dy_b = -h2 + h4\n\n        # Overlap gradients (accumulate with np.add.at)\n        dx_o = np.zeros(N, dtype=float)\n        dy_o = np.zeros(N, dtype=float)\n        dr_o = np.zeros(N, dtype=float)\n\n        coeff = (wij * hpos) / d_safe\n        cx = coeff * dxab\n        cy = coeff * dyab\n        np.add.at(dx_o, iu[0], -cx)\n        np.add.at(dx_o, iu[1], +cx)\n        np.add.at(dy_o, iu[0], -cy)\n        np.add.at(dy_o, iu[1], +cy)\n        # For radii:\n        contrib_r = wij * hpos\n        np.add.at(dr_o, iu[0], contrib_r)\n        np.add.at(dr_o, iu[1], contrib_r)\n\n        dx_total = P * (w_b * dx_b + w_o * dx_o)\n        dy_total = P * (w_b * dy_b + w_o * dy_o)\n        dr_total = -1.0 + P * (w_b * dr_b + w_o * dr_o)\n\n        grad = np.concatenate([dx_total, dy_total, dr_total])\n        return f, grad\n\n    def _objective_centers_only(z_xy, r_fixed, P, w_b=1.0, w_o=1.0, use_density=True):\n        # z_xy = [x(0..N-1), y(0..N-1)], radii fixed\n        N = r_fixed.shape[0]\n        x = z_xy[:N]\n        y = z_xy[N:2*N]\n        r = r_fixed\n        tiny = 1e-12\n\n        # Boundary hinges\n        s1 = r - x\n        s2 = r - y\n        s3 = r + x - 1.0\n        s4 = r + y - 1.0\n        h1 = np.maximum(0.0, s1)\n        h2 = np.maximum(0.0, s2)\n        h3 = np.maximum(0.0, s3)\n        h4 = np.maximum(0.0, s4)\n        pen_b = 0.5 * np.sum(h1*h1 + h2*h2 + h3*h3 + h4*h4)\n\n        # Overlap hinges\n        centers = np.stack([x, y], axis=1)\n        d, dx_mat, dy_mat = _pairwise_distances(centers)\n        iu = np.triu_indices(N, 1)\n        dab = d[iu]\n        dxab = dx_mat[iu]\n        dyab = dy_mat[iu]\n        d_safe = np.maximum(dab, tiny)\n        hij = r[iu[0]] + r[iu[1]] - dab\n        hpos = np.maximum(0.0, hij)\n\n        if use_density:\n            dens = _density_weights(centers, k=5, eps=tiny)\n            wij = 0.5 * (dens[iu[0]] + dens[iu[1]])\n        else:\n            wij = 1.0\n\n        pen_o = 0.5 * np.sum(wij * (hpos*hpos))\n\n        f = P * (w_b * pen_b + w_o * pen_o)\n\n        # Gradients wrt x,y only\n        dx_b = -h1 + h3\n        dy_b = -h2 + h4\n\n        dx_o = np.zeros(N, dtype=float)\n        dy_o = np.zeros(N, dtype=float)\n        coeff = (wij * hpos) / d_safe\n        cx = coeff * dxab\n        cy = coeff * dyab\n        np.add.at(dx_o, iu[0], -cx)\n        np.add.at(dx_o, iu[1], +cx)\n        np.add.at(dy_o, iu[0], -cy)\n        np.add.at(dy_o, iu[1], +cy)\n\n        dx_total = P * (w_b * dx_b + w_o * dx_o)\n        dy_total = P * (w_b * dy_b + w_o * dy_o)\n        g = np.concatenate([dx_total, dy_total])\n        return f, g\n\n    def _lp_feasibilize(centers, r_max=None, eps_pair=1e-6):\n        \"\"\"\n        Maximize sum(r) subject to:\n          r_i >= 0\n          r_i <= r_max[i] (if provided)\n          r_i <= x_i, r_i <= y_i, r_i <= 1-x_i, r_i <= 1-y_i\n          r_i + r_j <= dist(c_i,c_j) - eps_pair\n        \"\"\"\n        N = centers.shape[0]\n        x = centers[:, 0]\n        y = centers[:, 1]\n\n        # Build A_ub, b_ub\n        A_rows = []\n        b_vals = []\n\n        # Boundary constraints\n        for i in range(N):\n            e = np.zeros(N); e[i] = 1.0\n            A_rows.append(e.copy()); b_vals.append(x[i])\n            A_rows.append(e.copy()); b_vals.append(y[i])\n            A_rows.append(e.copy()); b_vals.append(1.0 - x[i])\n            A_rows.append(e.copy()); b_vals.append(1.0 - y[i])\n\n        # Pairwise constraints\n        d, _, _ = _pairwise_distances(centers)\n        for i in range(N):\n            for j in range(i+1, N):\n                e = np.zeros(N); e[i] = 1.0; e[j] = 1.0\n                A_rows.append(e); b_vals.append(d[i, j] - eps_pair)\n\n        # Optional upper bounds r_i <= r_max[i]\n        if r_max is not None:\n            for i in range(N):\n                e = np.zeros(N); e[i] = 1.0\n                A_rows.append(e); b_vals.append(r_max[i])\n\n        A_ub = np.array(A_rows)\n        b_ub = np.array(b_vals)\n\n        # Bounds on r: [0, 0.5]\n        bounds = [(0.0, 0.5) for _ in range(N)]\n        c = -np.ones(N)\n\n        res = linprog(c, A_ub=A_ub, b_ub=b_ub, bounds=bounds, method='highs')\n        if res.success:\n            return np.clip(res.x, 0.0, 0.5)\n\n        # Retry with eps_pair = 0 to get a feasible base, then scale to satisfy eps_pair\n        b_vals2 = []\n        A_rows2 = []\n        for i in range(N):\n            e = np.zeros(N); e[i] = 1.0\n            A_rows2.append(e.copy()); b_vals2.append(x[i])\n            A_rows2.append(e.copy()); b_vals2.append(y[i])\n            A_rows2.append(e.copy()); b_vals2.append(1.0 - x[i])\n            A_rows2.append(e.copy()); b_vals2.append(1.0 - y[i])\n\n        for i in range(N):\n            for j in range(i+1, N):\n                e = np.zeros(N); e[i] = 1.0; e[j] = 1.0\n                A_rows2.append(e); b_vals2.append(d[i, j])\n\n        if r_max is not None:\n            for i in range(N):\n                e = np.zeros(N); e[i] = 1.0\n                A_rows2.append(e); b_vals2.append(r_max[i])\n\n        A_ub2 = np.array(A_rows2)\n        b_ub2 = np.array(b_vals2)\n\n        res2 = linprog(c, A_ub=A_ub2, b_ub=b_ub2, bounds=bounds, method='highs')\n        if not res2.success:\n            return np.zeros(N)\n\n        r_base = np.clip(res2.x, 0.0, 0.5)\n        d, _, _ = _pairwise_distances(centers)\n        denom = r_base[:, None] + r_base[None, :]\n        with np.errstate(divide='ignore', invalid='ignore'):\n            ratio = np.where(denom > 0, (d - eps_pair) / denom, np.inf)\n        iu = np.triu_indices(N, 1)\n        if ratio[iu].size > 0:\n            s_pairs = np.min(ratio[iu])\n            if not np.isfinite(s_pairs):\n                s_pairs = 1.0\n            s = min(1.0, s_pairs)\n        else:\n            s = 1.0\n        s = max(0.0, s)\n        r = s * r_base\n        return np.clip(r, 0.0, 0.5)\n\n    def _verify(centers, radii, tol_pair=1e-6):\n        N = len(radii)\n        x = centers[:, 0]\n        y = centers[:, 1]\n        if centers.shape != (N, 2):\n            return False\n        if np.any(radii < -1e-12):\n            return False\n        if np.any(x - radii < -1e-9):\n            return False\n        if np.any(y - radii < -1e-9):\n            return False\n        if np.any(x + radii > 1.0 + 1e-9):\n            return False\n        if np.any(y + radii > 1.0 + 1e-9):\n            return False\n        d, _, _ = _pairwise_distances(centers)\n        iu = np.triu_indices(N, 1)\n        lhs = radii[:, None] + radii[None, :]\n        viol = lhs[iu] - d[iu] + tol_pair\n        if np.any(viol > 1e-12):\n            return False\n        return True\n\n    def _hex_initial_centers_from_rows(rows, margin=0.05, jitter_scale=0.03, seed=0):\n        # rows: list of counts per row, centered with hex offset when fewer than max\n        assert sum(rows) == 26\n        max_cols = max(rows)\n        nrows = len(rows)\n        dx = (1.0 - 2*margin) / (max_cols - 1) if max_cols > 1 else 0.0\n        dy = (1.0 - 2*margin) / (nrows - 1) if nrows > 1 else 0.0\n        centers = []\n        rng = np.random.default_rng(seed)\n        for ri, cols in enumerate(rows):\n            y = margin + ri * dy if nrows > 1 else 0.5\n            if cols == max_cols:\n                startx = margin\n            else:\n                startx = margin + 0.5 * dx\n            if cols == 1:\n                startx = 0.5\n            for k in range(cols):\n                x = startx + (k * dx if cols > 1 else 0.0)\n                centers.append([x, y])\n        centers = np.array(centers, dtype=float)\n        # Jitter and clip\n        jitter = rng.normal(scale=jitter_scale, size=centers.shape)\n        centers = centers + jitter\n        centers = np.clip(centers, 1e-3, 1 - 1e-3)\n        return centers\n\n    def _multi_start_inits(N):\n        # Various row patterns summing to 26\n        patterns = [\n            [6, 5, 6, 5, 4],\n            [6, 5, 5, 5, 5],\n            [5, 6, 5, 5, 5],\n            [5, 5, 6, 5, 5],\n            [6, 6, 5, 5, 4],\n            [5, 6, 6, 5, 4],\n            [6, 5, 6, 4, 5],\n        ]\n        params = [\n            (0.03, 0.02), (0.04, 0.03), (0.05, 0.03),\n            (0.06, 0.02), (0.07, 0.03), (0.05, 0.04),\n        ]\n        inits = []\n        seed = 0\n        target_starts = 18\n        for pat in patterns:\n            for (m, j) in params:\n                inits.append(_hex_initial_centers_from_rows(pat, margin=m, jitter_scale=j, seed=seed))\n                seed += 1\n                if len(inits) >= target_starts:\n                    return inits\n        return inits\n\n    def _center_only_relax(centers, r_fixed, P=2000.0, w_b=1.0, w_o=1.0, iters=80):\n        N = centers.shape[0]\n        z0 = np.concatenate([centers[:, 0], centers[:, 1]])\n        bounds_xy = [(0.0, 1.0)] * (2 * N)\n        res = minimize(\n            fun=lambda zz: _objective_centers_only(zz, r_fixed, P, w_b, w_o, True),\n            x0=z0,\n            method='L-BFGS-B',\n            jac=True,\n            bounds=bounds_xy,\n            options={'maxiter': iters, 'ftol': 1e-12, 'gtol': 1e-8, 'maxcor': 20}\n        )\n        z = res.x\n        centers_new = np.stack([z[:N], z[N:2*N]], axis=1)\n        return centers_new\n\n    # ---------------- Main optimization ----------------\n    N = 26\n    best = {'centers': None, 'radii': None, 'sum': -np.inf}\n\n    # Penalty schedule and bounds\n    P_schedule = [8.0, 80.0, 800.0, 3000.0, 10000.0]\n    maxiter_stage = [90, 100, 110, 120, 130]\n    bounds = [(0.0, 1.0)] * (2*N) + [(0.0, 0.5)] * N\n\n    # Weighting of penalties\n    w_b = 1.0\n    w_o = 1.0\n\n    # Initializations\n    inits = _multi_start_inits(N)\n    if not inits:\n        inits = [_hex_initial_centers_from_rows([6,5,6,5,4], margin=0.05, jitter_scale=0.03, seed=42)]\n\n    for init_idx, centers0 in enumerate(inits):\n        x0 = centers0[:, 0]\n        y0 = centers0[:, 1]\n        # Start with small radii within local boundary\n        base_bound = np.minimum.reduce([x0, y0, 1 - x0, 1 - y0])\n        r0 = 0.05 * base_bound\n        z = np.concatenate([x0, y0, r0])\n\n        # Annealing optimization with periodic LP tightening and center-only relaxation\n        for si, P in enumerate(P_schedule):\n            res = minimize(\n                fun=lambda zz: _objective_and_grad(zz, N, P, w_b, w_o, True),\n                x0=z,\n                method='L-BFGS-B',\n                jac=True,\n                bounds=bounds,\n                options={'maxiter': maxiter_stage[si], 'ftol': 1e-12, 'gtol': 1e-8, 'maxcor': 30}\n            )\n            z = res.x\n            centers_tmp = np.stack([z[:N], z[N:2*N]], axis=1)\n            r_cur = np.clip(z[2*N:], 0.0, 0.5)\n            # Conservative LP cap slightly above current radii\n            cap_factor = 1.08 if si < len(P_schedule) - 1 else 1.15\n            r_cap = np.minimum(0.5, cap_factor * np.maximum(1e-9, r_cur))\n            r_lp = _lp_feasibilize(centers_tmp, r_max=r_cap, eps_pair=1e-6)\n            # Small center-only relaxation to open space, then re-LP\n            centers_relaxed = _center_only_relax(centers_tmp, r_lp, P=max(1000.0, P), w_b=w_b, w_o=w_o, iters=70)\n            r_lp2 = _lp_feasibilize(centers_relaxed, r_max=None, eps_pair=1e-6)\n            z = np.concatenate([centers_relaxed[:, 0], centers_relaxed[:, 1], np.clip(r_lp2, 0.0, 0.5)])\n\n        # Final LP without cap to fully utilize geometry\n        centers_cur = np.stack([z[:N], z[N:2*N]], axis=1)\n        r_final = _lp_feasibilize(centers_cur, r_max=None, eps_pair=1e-6)\n\n        # Verify and update best\n        if _verify(centers_cur, r_final, tol_pair=1e-6):\n            s = float(np.sum(r_final))\n            if s > best['sum']:\n                best['centers'] = centers_cur.copy()\n                best['radii'] = r_final.copy()\n                best['sum'] = s\n        else:\n            # Try slight shrink and re-LP to fix tiny violations\n            d, _, _ = _pairwise_distances(centers_cur)\n            denom = r_final[:, None] + r_final[None, :]\n            with np.errstate(divide='ignore', invalid='ignore'):\n                ratio = np.where(denom > 0, (d - 1.0e-6) / denom, np.inf)\n            iu = np.triu_indices(N, 1)\n            s_pairs = np.min(ratio[iu]) if ratio[iu].size > 0 else 1.0\n            if not np.isfinite(s_pairs):\n                s_pairs = 1.0\n            s_pairs = max(0.0, min(1.0, s_pairs))\n            r_try = s_pairs * r_final\n            # Boundary scaling\n            bx = np.minimum.reduce([centers_cur[:,0], centers_cur[:,1], 1-centers_cur[:,0], 1-centers_cur[:,1]])\n            with np.errstate(divide='ignore', invalid='ignore'):\n                scale_b = np.min(np.where(r_try > 0, bx / r_try, 1.0))\n            scale_b = min(1.0, scale_b if np.isfinite(scale_b) else 1.0)\n            r_try = scale_b * r_try\n            if _verify(centers_cur, r_try, tol_pair=1e-6):\n                s = float(np.sum(r_try))\n                if s > best['sum']:\n                    best['centers'] = centers_cur.copy()\n                    best['radii'] = r_try.copy()\n                    best['sum'] = s\n\n    # Fallback if nothing verified\n    if best['centers'] is None:\n        centers_fb = _hex_initial_centers_from_rows([6,5,6,5,4], margin=0.06, jitter_scale=0.02, seed=123)\n        r_fb = _lp_feasibilize(centers_fb, r_max=None, eps_pair=1e-6)\n        if not _verify(centers_fb, r_fb, tol_pair=1e-6):\n            r_fb = 0.95 * r_fb\n        best['centers'] = centers_fb\n        best['radii'] = np.clip(r_fb, 0.0, 0.5)\n        best['sum'] = float(np.sum(best['radii']))\n\n    # Ensure strict feasibility and final verify\n    radii_strict = _lp_feasibilize(best['centers'], r_max=None, eps_pair=1.1e-6)\n    if _verify(best['centers'], radii_strict, tol_pair=1e-6):\n        radii = radii_strict\n    else:\n        radii = best['radii']\n\n    centers = best['centers']\n    # Final hardening: if any violation persists, scale radii conservatively\n    if not _verify(centers, radii, tol_pair=1e-6):\n        d, _, _ = _pairwise_distances(centers)\n        denom = radii[:, None] + radii[None, :]\n        with np.errstate(divide='ignore', invalid='ignore'):\n            ratio = np.where(denom > 0, (d - 1e-6) / denom, np.inf)\n        iu = np.triu_indices(N, 1)\n        s_pairs = np.min(ratio[iu]) if ratio[iu].size > 0 else 1.0\n        s_pairs = min(1.0, s_pairs if np.isfinite(s_pairs) else 1.0)\n        s_pairs = max(0.0, s_pairs)\n        radii = s_pairs * radii\n        bx = np.minimum.reduce([centers[:,0], centers[:,1], 1-centers[:,0], 1-centers[:,1]])\n        with np.errstate(divide='ignore', invalid='ignore'):\n            scale_b = np.min(np.where(radii > 0, bx / radii, 1.0))\n        scale_b = min(1.0, scale_b if np.isfinite(scale_b) else 1.0)\n        radii = scale_b * radii\n\n    radii = np.clip(radii, 0.0, 0.5)\n    sum_radii = float(np.sum(radii))\n    return centers.astype(float), radii.astype(float), sum_radii\n\n\nRequirements:\n- Single Python code block only. No prose. Implement construct_packing() as entry point.\n- Return (centers, radii, sum_radii) strictly feasible for 26 circles in [0,1]^2.\n- Use numpy, scipy only. No prints. Runtime <= 300s.\n- Use analytic, vectorized gradients if optimizing; apply eps only in divisions (not in penalties).\n- Finalize with an LP: maximize sum(r) s.t.\n  0 <= r_i <= bound_i where bound_i = min(x_i, y_i, 1-x_i, 1-y_i), and\n  r_i + r_j <= dist_ij - 1e-6 for all i<j.\n- Shapes: centers (26,2), radii (26,), sum_radii float.\n\nImplementation tips:\n- Multi-start on centers; seeds: hex grid, edge/corner-biased jitter, and refined best-known-like layouts.\n- Penalized objective for centers (optional radii vars) with annealed penalties; adaptive pairwise weights by local density.\n- Stable distances: use d = sqrt(||\u0394||^2 + eps^2); hinge = relu(overlap).\n- Cache pairwise indices; optionally k-NN masks for speed; widen mask over annealing.\n- LP: build A_ub once; warm-start bounds from current radii; use HiGHS method.\n\nFeasibility:\n- Always LP at end; clip tiny negatives to 0; verify non-overlap with 1e-6 tolerance.\n\n",
  "response": "```python\nimport numpy as np\nfrom scipy.optimize import minimize, linprog\n\ndef construct_packing():\n    N = 26\n    eps_dist = 1e-8\n    eps_pair = 1e-6\n    # Precompute pair indices\n    iu, ju = np.triu_indices(N, 1)\n    # Helper: hex initial\n    def _hex(rows, margin, jitter, seed):\n        maxc = max(rows); nr = len(rows)\n        dx = (1-2*margin)/(maxc-1) if maxc>1 else 0\n        dy = (1-2*margin)/(nr-1) if nr>1 else 0\n        rng = np.random.default_rng(seed)\n        C = []\n        for i, cnt in enumerate(rows):\n            y = margin + (dy*i if nr>1 else 0.5)\n            stx = margin + (0.5*dx if cnt<maxc else 0)\n            for k in range(cnt):\n                x = stx + (k*dx if cnt>1 else 0.5-margin)\n                C.append([x, y])\n        C = np.array(C)\n        C += rng.normal(scale=jitter, size=C.shape)\n        return np.clip(C, 1e-3, 1-1e-3)\n    # Helper: edge initial\n    def _edge():\n        pts = []\n        m = 0.01\n        n4 = int(np.ceil(N/4))\n        xs = np.linspace(m,1-m,n4)\n        ys = np.linspace(m,1-m,n4)\n        for x in xs: pts.append([x,m])\n        for y in ys: pts.append([1-m,y])\n        for x in xs: pts.append([x,1-m])\n        for y in ys: pts.append([m,y])\n        return np.array(pts[:N])\n    # Helper: random initial\n    def _rand(seed):\n        rng = np.random.default_rng(seed)\n        return rng.random((N,2))*0.98+0.01\n    # Seeds\n    inits = [\n        _hex([6,5,6,5,4],0.05,0.03,0),\n        _hex([6,5,6,5,4],0.07,0.04,1),\n        _edge(),\n        _rand(2),\n        _rand(3),\n        _hex([5,6,5,5,5],0.06,0.02,4)\n    ]\n    # Pairwise distance and diffs\n    def _pairwise(C):\n        dxy = C[:,None,:]-C[None,:,:]\n        dx = dxy[:,:,0]; dy = dxy[:,:,1]\n        dab = np.hypot(dx,dy)\n        dsv = np.sqrt(dab*dab+eps_dist*eps_dist)\n        return dab, dsv, dx, dy\n    # Build k-NN mask\n    def _knn_mask(C,k):\n        dab,_,_,_ = _pairwise(C)\n        N = dab.shape[0]\n        m = np.full((N,N), False)\n        for i in range(N):\n            idx = np.argpartition(dab[i], k+1)[:k+1]\n            for j in idx:\n                if j!=i: m[i,j]=m[j,i]=True\n        mask = m[iu,ju]\n        return mask\n    # Objective and grad for centers only\n    def _obj(z, r, P, w_b, w_o, mask):\n        x = z[:N]; y = z[N:]\n        # boundary\n        s1 = r-x; s2 = r-y; s3 = r+x-1; s4 = r+y-1\n        h1 = np.maximum(0,s1); h2 = np.maximum(0,s2)\n        h3 = np.maximum(0,s3); h4 = np.maximum(0,s4)\n        pen_b = 0.5*(h1*h1+h2*h2+h3*h3+h4*h4).sum()\n        dx_b = -h1+h3; dy_b = -h2+h4\n        # pairs\n        C = np.stack([x,y],1)\n        dab, dsv, dxm, dym = _pairwise(C)\n        dij = dab[iu,ju]; dsvij = dsv[iu,ju]\n        dxij = dxm[iu,ju]; dyij = dym[iu,ju]\n        hij = r[iu]+r[ju]-dij; hpos = np.maximum(0,hij)\n        active = mask\n        pen_o = 0.5*(hpos[active]**2).sum()\n        # gradients\n        dx_o = np.zeros(N); dy_o = np.zeros(N)\n        # radii fixed, so no dr\n        coeff = hpos[active]/dsvij[active]\n        cx = coeff*dxij[active]; cy = coeff*dyij[active]\n        np.add.at(dx_o, iu[active], -cx); np.add.at(dx_o, ju[active], +cx)\n        np.add.at(dy_o, iu[active], -cy); np.add.at(dy_o, ju[active], +cy)\n        g = np.concatenate([P*(w_b*dx_b + w_o*dx_o),\n                            P*(w_b*dy_b + w_o*dy_o)])\n        f = P*(w_b*pen_b + w_o*pen_o)\n        return f, g\n    # LP for radii\n    def _lp(C):\n        dab,_,_,_ = _pairwise(C)\n        bnd = np.minimum.reduce([C[:,0],C[:,1],1-C[:,0],1-C[:,1]])\n        A = []; b=[]\n        for i,j in zip(iu,ju):\n            row = np.zeros(N); row[i]=row[j]=1\n            A.append(row); b.append(dab[i,j]-eps_pair)\n        A = np.array(A); b=np.array(b)\n        c = -np.ones(N)\n        bounds = [(0,bnd[i]) for i in range(N)]\n        res = linprog(c,A_ub=A,b_ub=b,bounds=bounds,method='highs')\n        if res.success: return np.clip(res.x,0,None)\n        return np.zeros(N)\n    # verify\n    def _verify(C,r):\n        if C.shape!=(N,2) or r.shape!=(N,): return False\n        if np.any(r<-1e-9): return False\n        if np.any(C[:,0]-r<-1e-8) or np.any(C[:,1]-r<-1e-8): return False\n        if np.any(C[:,0]+r>1+1e-8) or np.any(C[:,1]+r>1+1e-8): return False\n        dab,_,_,_ = _pairwise(C)\n        if np.any(r[:,None]+r[None,:]-dab>1e-6): return False\n        return True\n\n    best = {'sum': -1, 'C':None, 'r':None}\n    # penalty stages\n    Psch = [1e2,1e3,1e4]; ks=[6,12,N]; w_b=1.0; w_o=1.0\n    for C0 in inits:\n        C = C0.copy()\n        r = _lp(C)\n        for P,k in zip(Psch,ks):\n            mask = _knn_mask(C,k)\n            z0 = np.r_[C[:,0],C[:,1]]\n            bnds = [(0,1)]*(2*N)\n            fun = lambda z: _obj(z,r,P,w_b,w_o,mask)\n            res = minimize(fun,z0,method='trust-constr',jac=True,bounds=bnds,\n                           options={'maxiter':60,'gtol':1e-8,'verbose':0})\n            z = res.x\n            C = np.stack([z[:N],z[N:]],1)\n            r = _lp(C)\n        # final\n        r = _lp(C)\n        if _verify(C,r):\n            s = r.sum()\n            if s>best['sum']:\n                best.update({'sum':s,'C':C.copy(),'r':r.copy()})\n    if best['C'] is None:\n        C = inits[0]; r=_lp(C); best.update({'sum':r.sum(),'C':C,'r':r})\n    return best['C'], best['r'], float(best['sum'])\n```\n",
  "reasoning": "",
  "timestamp": "2026-02-11T03:03:58.125302",
  "cost_data": null,
  "model_config": {
    "model": "openai/o4-mini",
    "temperature": 0.3
  },
  "llm_call_duration_s": 46.08788302401081,
  "eval_duration_s": 0.5585167049430311
}