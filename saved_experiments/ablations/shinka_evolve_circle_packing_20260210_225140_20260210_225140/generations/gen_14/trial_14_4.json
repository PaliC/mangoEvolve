{
  "trial_id": "trial_14_4",
  "generation": 14,
  "parent_id": null,
  "code": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.optimize import linprog\n\ndef construct_packing():\n    \"\"\"\n    Packs 26 non-overlapping circles into a unit square [0,1]x[0,1]\n    to maximize the sum of their radii.\n\n    The method uses a multi-start, annealing-based optimization approach.\n    Each start point is optimized using a penalty method, where the penalty\n    for violating boundary and overlap constraints is gradually increased.\n    The optimization alternates between:\n    1. A joint optimization of centers and radii using L-BFGS-B on a\n       penalized objective function.\n    2. A Linear Programming (LP) step to find the maximum radii for the\n       current centers, ensuring strict feasibility.\n    3. A centers-only relaxation step (with fixed radii from the LP) to\n       create more space around active constraints.\n\n    This iterative process allows the packing to escape local minima and\n    gradually improve the total sum of radii. The best result over\n    multiple random initializations is returned.\n    \"\"\"\n    N = 26\n\n    # ---------------- Helper functions ----------------\n    def _pairwise_distances(centers):\n        \"\"\"Computes pairwise distances and difference vectors.\"\"\"\n        diff = centers[:, None, :] - centers[None, :, :]\n        d_sq = np.sum(diff**2, axis=2)\n        d = np.sqrt(d_sq + 1e-24) # Add epsilon for stability\n        return d, diff[..., 0], diff[..., 1]\n\n    def _density_weights(centers, k=5, eps=1e-12):\n        \"\"\"Computes density weights based on k-NN distances.\"\"\"\n        d, _, _ = _pairwise_distances(centers)\n        np.fill_diagonal(d, np.inf)\n        k_eff = min(k, N - 1)\n        if k_eff <= 0: return np.ones(N)\n        \n        knn_dists = np.partition(d, kth=k_eff-1, axis=1)[:, :k_eff]\n        mean_knn_dist = np.mean(knn_dists, axis=1)\n        density = 1.0 / (mean_knn_dist + eps)\n        return density\n\n    def _objective_and_grad(z, P, w_b, w_o, use_density):\n        \"\"\"Objective and gradient for joint optimization of centers and radii.\"\"\"\n        x, y, r = z[:N], z[N:2*N], z[2*N:]\n        centers = np.stack([x, y], axis=1)\n        tiny = 1e-12\n\n        # Boundary penalties (squared hinge loss)\n        s_b = np.array([r - x, r - y, r + x - 1.0, r + y - 1.0])\n        h_b = np.maximum(0.0, s_b)\n        pen_b = 0.5 * np.sum(h_b**2)\n\n        # Overlap penalties\n        d, dx_mat, dy_mat = _pairwise_distances(centers)\n        iu = np.triu_indices(N, 1)\n        d_ij = d[iu]\n        r_sum_ij = r[iu[0]] + r[iu[1]]\n        h_o = np.maximum(0.0, r_sum_ij - d_ij)\n\n        if use_density:\n            dens = _density_weights(centers, k=5, eps=tiny)\n            w_ij = 0.5 * (dens[iu[0]] + dens[iu[1]])\n        else:\n            w_ij = 1.0\n        \n        pen_o = 0.5 * np.sum(w_ij * h_o**2)\n        f = -np.sum(r) + P * (w_b * pen_b + w_o * pen_o)\n\n        # Gradients\n        dr_b = h_b[0] + h_b[1] + h_b[2] + h_b[3]\n        dx_b = -h_b[0] + h_b[2]\n        dy_b = -h_b[1] + h_b[3]\n\n        dr_o, dx_o, dy_o = np.zeros(N), np.zeros(N), np.zeros(N)\n        d_safe = np.maximum(d_ij, tiny)\n        grad_coeff = (w_ij * h_o) / d_safe\n        \n        np.add.at(dr_o, iu[0], w_ij * h_o)\n        np.add.at(dr_o, iu[1], w_ij * h_o)\n        \n        dx_contrib = grad_coeff * dx_mat[iu]\n        dy_contrib = grad_coeff * dy_mat[iu]\n        np.add.at(dx_o, iu[0], -dx_contrib)\n        np.add.at(dx_o, iu[1], +dx_contrib)\n        np.add.at(dy_o, iu[0], -dy_contrib)\n        np.add.at(dy_o, iu[1], +dy_contrib)\n\n        grad = np.concatenate([\n            P * (w_b * dx_b + w_o * dx_o),\n            P * (w_b * dy_b + w_o * dy_o),\n            -1.0 + P * (w_b * dr_b + w_o * dr_o)\n        ])\n        return f, grad\n\n    def _objective_centers_only(z_xy, r_fixed, P, w_b, w_o, use_density):\n        \"\"\"Objective and gradient for centers-only relaxation.\"\"\"\n        x, y, r = z_xy[:N], z_xy[N:], r_fixed\n        centers = np.stack([x, y], axis=1)\n        tiny = 1e-12\n\n        s_b = np.array([r - x, r - y, r + x - 1.0, r + y - 1.0])\n        h_b = np.maximum(0.0, s_b)\n        pen_b = 0.5 * np.sum(h_b**2)\n\n        d, dx_mat, dy_mat = _pairwise_distances(centers)\n        iu = np.triu_indices(N, 1)\n        d_ij = d[iu]\n        r_sum_ij = r[iu[0]] + r[iu[1]]\n        h_o = np.maximum(0.0, r_sum_ij - d_ij)\n\n        if use_density:\n            dens = _density_weights(centers, k=5, eps=tiny)\n            w_ij = 0.5 * (dens[iu[0]] + dens[iu[1]])\n        else:\n            w_ij = 1.0\n            \n        pen_o = 0.5 * np.sum(w_ij * h_o**2)\n        f = P * (w_b * pen_b + w_o * pen_o)\n\n        dx_b, dy_b = -h_b[0] + h_b[2], -h_b[1] + h_b[3]\n        dx_o, dy_o = np.zeros(N), np.zeros(N)\n        d_safe = np.maximum(d_ij, tiny)\n        grad_coeff = (w_ij * h_o) / d_safe\n        \n        dx_contrib = grad_coeff * dx_mat[iu]\n        dy_contrib = grad_coeff * dy_mat[iu]\n        np.add.at(dx_o, iu[0], -dx_contrib)\n        np.add.at(dx_o, iu[1], +dx_contrib)\n        np.add.at(dy_o, iu[0], -dy_contrib)\n        np.add.at(dy_o, iu[1], +dy_contrib)\n\n        grad = np.concatenate([\n            P * (w_b * dx_b + w_o * dx_o),\n            P * (w_b * dy_b + w_o * dy_o)\n        ])\n        return f, grad\n\n    def _lp_solve(centers, r_max=None, eps_pair=1e-6):\n        \"\"\"Solves the LP to find optimal radii for fixed centers.\"\"\"\n        x, y = centers[:, 0], centers[:, 1]\n        c = -np.ones(N)\n\n        num_b_constraints = 4 * N\n        num_p_constraints = N * (N - 1) // 2\n        num_cap_constraints = N if r_max is not None else 0\n        num_constraints = num_b_constraints + num_p_constraints + num_cap_constraints\n        \n        A_ub = np.zeros((num_constraints, N))\n        b_ub = np.zeros(num_constraints)\n        \n        A_ub[np.arange(num_b_constraints), np.tile(np.arange(N), 4)] = 1.0\n        b_ub[:N], b_ub[N:2*N] = x, y\n        b_ub[2*N:3*N], b_ub[3*N:4*N] = 1.0 - x, 1.0 - y\n        \n        d, _, _ = _pairwise_distances(centers)\n        iu = np.triu_indices(N, 1)\n        row_idx = num_b_constraints\n        A_ub[row_idx : row_idx + num_p_constraints, iu[0]] = 1.0\n        A_ub[row_idx : row_idx + num_p_constraints, iu[1]] = 1.0\n        b_ub[row_idx : row_idx + num_p_constraints] = d[iu] - eps_pair\n        \n        if r_max is not None:\n            row_idx += num_p_constraints\n            A_ub[row_idx : row_idx + N, np.arange(N)] = 1.0\n            b_ub[row_idx : row_idx + N] = r_max\n\n        res = linprog(c, A_ub=A_ub, b_ub=b_ub, bounds=(0.0, 0.5), method='highs')\n\n        if res.success:\n            return np.clip(res.x, 0.0, None)\n        \n        b_ub[num_b_constraints : num_b_constraints + num_p_constraints] += eps_pair\n        res_fallback = linprog(c, A_ub=A_ub, b_ub=b_ub, bounds=(0.0, 0.5), method='highs')\n        \n        if not res_fallback.success: return np.zeros(N)\n        r_base = np.clip(res_fallback.x, 0.0, None)\n        \n        r_sum_ij = r_base[iu[0]] + r_base[iu[1]]\n        d_ij = d[iu]\n        \n        safe_denom = np.where(r_sum_ij > 1e-12, r_sum_ij, 1.0)\n        target_num = np.where(r_sum_ij > 1e-12, d_ij - eps_pair, np.inf)\n        ratios = target_num / safe_denom\n        scale = min(1.0, np.min(ratios) if len(ratios) > 0 else 1.0)\n        return np.clip(scale * r_base, 0.0, None)\n\n    def _verify(centers, radii, tol=1e-6):\n        \"\"\"Verify if a packing is valid.\"\"\"\n        if np.any(radii < 0): return False\n        if np.any(centers - radii[:, None] < -tol) or np.any(centers + radii[:, None] > 1 + tol): return False\n        d, _, _ = _pairwise_distances(centers)\n        iu = np.triu_indices(N, 1)\n        return np.all((radii[iu[0]] + radii[iu[1]]) - d[iu] < tol)\n\n    def _get_initial_centers(rows, margin, jitter_scale, seed):\n        \"\"\"Generate initial centers based on a hex-like grid layout.\"\"\"\n        rng = np.random.default_rng(seed)\n        max_cols, num_rows = max(rows), len(rows)\n        dx = (1.0 - 2 * margin) / (max_cols - 1) if max_cols > 1 else 0\n        dy = (1.0 - 2 * margin) / (num_rows - 1) if num_rows > 1 else 0\n        \n        centers = []\n        for i, num_cols in enumerate(rows):\n            y_pos = margin + i * dy if num_rows > 1 else 0.5\n            row_width = (num_cols - 1) * dx if num_cols > 1 else 0\n            x_start = (1.0 - row_width) / 2.0\n            for j in range(num_cols):\n                centers.append([x_start + j * dx, y_pos])\n        \n        centers = np.array(centers)\n        centers += rng.normal(scale=jitter_scale, size=centers.shape)\n        return np.clip(centers, 0.0, 1.0)\n\n    def _multi_start_inits():\n        \"\"\"Generate a list of diverse initial center configurations.\"\"\"\n        patterns = [\n            [6, 5, 6, 5, 4], [5, 6, 5, 6, 4], [4, 5, 6, 5, 6],\n            [6, 6, 5, 5, 4], [5, 5, 6, 6, 4],\n        ]\n        inits = []\n        seed = 0\n        for pat in patterns:\n            for margin in [0.05, 0.08]:\n                for jitter in [0.03]:\n                    inits.append(_get_initial_centers(pat, margin, jitter, seed))\n                    seed += 1\n        rng = np.random.default_rng(seed)\n        inits.append(rng.random((N, 2)))\n        inits.append(rng.random((N, 2)))\n        return inits\n\n    # ---------------- Main optimization logic ----------------\n    best_sum, best_centers, best_radii = -1.0, None, None\n    initial_configs = _multi_start_inits()\n    \n    P_schedule = [10.0, 100.0, 1000.0, 5000.0]\n    maxiter_schedule = [80, 90, 100, 110]\n    w_b, w_o = 1.0, 1.0\n    bounds = [(0.0, 1.0)] * (2 * N) + [(0.0, 0.5)] * N\n\n    for centers_init in initial_configs:\n        x, y = centers_init[:, 0], centers_init[:, 1]\n        r_init = 0.1 * np.min([x, y, 1-x, 1-y], axis=0)\n        z = np.concatenate([x, y, r_init])\n\n        for i, P in enumerate(P_schedule):\n            res = minimize(\n                fun=lambda z_var: _objective_and_grad(z_var, P, w_b, w_o, True),\n                x0=z, method='L-BFGS-B', jac=True, bounds=bounds,\n                options={'maxiter': maxiter_schedule[i], 'ftol': 1e-12, 'gtol': 1e-8}\n            )\n            z = res.x\n            \n            centers_cur = np.stack([z[:N], z[N:2*N]], axis=1)\n            r_lp = _lp_solve(centers_cur, r_max=np.minimum(0.5, 1.1 * z[2*N:]), eps_pair=1e-6)\n            \n            res_relax = minimize(\n                fun=lambda z_xy: _objective_centers_only(z_xy, r_lp, P, w_b, w_o, True),\n                x0=z[:2*N], method='L-BFGS-B', jac=True, bounds=bounds[:2*N],\n                options={'maxiter': 60, 'ftol': 1e-12, 'gtol': 1e-8}\n            )\n            centers_relaxed = np.stack([res_relax.x[:N], res_relax.x[N:]], axis=1)\n            \n            r_final_stage = _lp_solve(centers_relaxed, eps_pair=1e-6)\n            z = np.concatenate([centers_relaxed[:, 0], centers_relaxed[:, 1], r_final_stage])\n\n        final_centers = np.stack([z[:N], z[N:2*N]], axis=1)\n        final_radii = _lp_solve(final_centers, eps_pair=1e-6)\n        \n        current_sum = np.sum(final_radii)\n        if current_sum > best_sum and _verify(final_centers, final_radii):\n            best_sum, best_centers, best_radii = current_sum, final_centers, final_radii\n\n    if best_centers is None:\n        centers_fb = _get_initial_centers([6, 5, 6, 5, 4], 0.05, 0.01, 42)\n        best_centers, best_radii = centers_fb, _lp_solve(centers_fb, eps_pair=1e-6)\n\n    final_radii_strict = _lp_solve(best_centers, eps_pair=1.1e-6)\n    if _verify(best_centers, final_radii_strict):\n        best_radii = final_radii_strict\n    \n    best_radii = np.clip(best_radii, 0.0, 0.5)\n    sum_radii = float(np.sum(best_radii))\n\n    return best_centers.astype(float), best_radii.astype(float), sum_radii",
  "metrics": {
    "valid": true,
    "score": 0.1620436197608221,
    "eval_time": 1.9315311908721924,
    "error": null
  },
  "prompt": "\nAlternate:\n1) Given centers, solve LP for radii.\n2) Identify active constraints (r_i == bound_i or r_i + r_j near dist_ij) and build a weighted smooth penalty focusing on near-active pairs/boundaries.\n3) Take several L-BFGS-B steps on centers to increase slack for active constraints (analytic vectorized gradients; eps only in denominators).\nRepeat 4-6 outer loops from few seeds (hex, edge-biased). Final LP and return best.\n\nUse style inspiration from:\n# Ref (trial_13_6):\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.optimize import linprog\n\ndef construct_packing():\n    \"\"\"Return (centers, radii, sum_radii) with:\n        - centers: np.array shape (26, 2)\n        - radii:   np.array shape (26,)\n        - sum_radii: float, strictly feasible sum of radii\n    \"\"\"\n    # ---------------- Helper functions ----------------\n    def _pairwise_distances(centers):\n        diff = centers[:, None, :] - centers[None, :, :]\n        d = np.sqrt(np.sum(diff**2, axis=2))\n        return d, diff[..., 0], diff[..., 1]\n\n    def _density_weights(centers, k=5, eps=1e-12):\n        # Local density proxy: inverse of mean k-NN distance\n        d, _, _ = _pairwise_distances(centers)\n        N = centers.shape[0]\n        # Exclude self-distances by setting to inf then sort\n        d2 = d.copy()\n        np.fill_diagonal(d2, np.inf)\n        # k-NN mean distances\n        # For N <= k, fallback to finite minima\n        k_eff = min(k, max(1, N - 1))\n        # Partition to get k smallest\n        knn = np.partition(d2, kth=k_eff-1, axis=1)[:, :k_eff]\n        m = np.mean(knn, axis=1)\n        dens = 1.0 / (m + eps)\n        return dens\n\n    def _objective_and_grad(z, N, P, w_b=1.0, w_o=1.0, use_density=True):\n        # z = [x(0..N-1), y(0..N-1), r(0..N-1)]\n        x = z[:N]\n        y = z[N:2*N]\n        r = z[2*N:]\n        tiny = 1e-12\n\n        # Boundary penalties: hinges\n        s1 = r - x\n        s2 = r - y\n        s3 = r + x - 1.0\n        s4 = r + y - 1.0\n\n        h1 = np.maximum(0.0, s1)\n        h2 = np.maximum(0.0, s2)\n        h3 = np.maximum(0.0, s3)\n        h4 = np.maximum(0.0, s4)\n\n        pen_b = 0.5 * np.sum(h1*h1 + h2*h2 + h3*h3 + h4*h4)\n\n        # Overlap penalties: hinges for i<j\n        centers = np.stack([x, y], axis=1)\n        d, dx_mat, dy_mat = _pairwise_distances(centers)\n        iu = np.triu_indices(N, 1)\n        dab = d[iu]\n        dxab = dx_mat[iu]\n        dyab = dy_mat[iu]\n        d_safe = np.maximum(dab, tiny)\n        hij = r[iu[0]] + r[iu[1]] - dab\n        hpos = np.maximum(0.0, hij)\n\n        if use_density:\n            dens = _density_weights(centers, k=5, eps=tiny)\n            wij = 0.5 * (dens[iu[0]] + dens[iu[1]])\n        else:\n            wij = 1.0\n\n        pen_o = 0.5 * np.sum(wij * (hpos*hpos))\n\n        # Objective\n        f = -np.sum(r) + P * (w_b * pen_b + w_o * pen_o)\n\n        # Gradients for boundary penalties\n        dr_b = h1 + h2 + h3 + h4\n        dx_b = -h1 + h3\n        dy_b = -h2 + h4\n\n        # Overlap gradients (accumulate with np.add.at)\n        dx_o = np.zeros(N, dtype=float)\n        dy_o = np.zeros(N, dtype=float)\n        dr_o = np.zeros(N, dtype=float)\n\n        coeff = (wij * hpos) / d_safe\n        cx = coeff * dxab\n        cy = coeff * dyab\n        np.add.at(dx_o, iu[0], -cx)\n        np.add.at(dx_o, iu[1], +cx)\n        np.add.at(dy_o, iu[0], -cy)\n        np.add.at(dy_o, iu[1], +cy)\n        # For radii:\n        contrib_r = wij * hpos\n        np.add.at(dr_o, iu[0], contrib_r)\n        np.add.at(dr_o, iu[1], contrib_r)\n\n        dx_total = P * (w_b * dx_b + w_o * dx_o)\n        dy_total = P * (w_b * dy_b + w_o * dy_o)\n        dr_total = -1.0 + P * (w_b * dr_b + w_o * dr_o)\n\n        grad = np.concatenate([dx_total, dy_total, dr_total])\n        return f, grad\n\n    def _objective_centers_only(z_xy, r_fixed, P, w_b=1.0, w_o=1.0, use_density=True):\n        # z_xy = [x(0..N-1), y(0..N-1)], radii fixed\n        N = r_fixed.shape[0]\n        x = z_xy[:N]\n        y = z_xy[N:2*N]\n        r = r_fixed\n        tiny = 1e-12\n\n        # Boundary hinges\n        s1 = r - x\n        s2 = r - y\n        s3 = r + x - 1.0\n        s4 = r + y - 1.0\n        h1 = np.maximum(0.0, s1)\n        h2 = np.maximum(0.0, s2)\n        h3 = np.maximum(0.0, s3)\n        h4 = np.maximum(0.0, s4)\n        pen_b = 0.5 * np.sum(h1*h1 + h2*h2 + h3*h3 + h4*h4)\n\n        # Overlap hinges\n        centers = np.stack([x, y], axis=1)\n        d, dx_mat, dy_mat = _pairwise_distances(centers)\n        iu = np.triu_indices(N, 1)\n        dab = d[iu]\n        dxab = dx_mat[iu]\n        dyab = dy_mat[iu]\n        d_safe = np.maximum(dab, tiny)\n        hij = r[iu[0]] + r[iu[1]] - dab\n        hpos = np.maximum(0.0, hij)\n\n        if use_density:\n            dens = _density_weights(centers, k=5, eps=tiny)\n            wij = 0.5 * (dens[iu[0]] + dens[iu[1]])\n        else:\n            wij = 1.0\n\n        pen_o = 0.5 * np.sum(wij * (hpos*hpos))\n\n        f = P * (w_b * pen_b + w_o * pen_o)\n\n        # Gradients wrt x,y only\n        dx_b = -h1 + h3\n        dy_b = -h2 + h4\n\n        dx_o = np.zeros(N, dtype=float)\n        dy_o = np.zeros(N, dtype=float)\n        coeff = (wij * hpos) / d_safe\n        cx = coeff * dxab\n        cy = coeff * dyab\n        np.add.at(dx_o, iu[0], -cx)\n        np.add.at(dx_o, iu[1], +cx)\n        np.add.at(dy_o, iu[0], -cy)\n        np.add.at(dy_o, iu[1], +cy)\n\n        dx_total = P * (w_b * dx_b + w_o * dx_o)\n        dy_total = P * (w_b * dy_b + w_o * dy_o)\n        g = np.concatenate([dx_total, dy_total])\n        return f, g\n\n    def _lp_feasibilize(centers, r_max=None, eps_pair=1e-6):\n        \"\"\"\n        Maximize sum(r) subject to:\n          r_i >= 0\n          r_i <= r_max[i] (if provided)\n          r_i <= x_i, r_i <= y_i, r_i <= 1-x_i, r_i <= 1-y_i\n          r_i + r_j <= dist(c_i,c_j) - eps_pair\n        \"\"\"\n        N = centers.shape[0]\n        x = centers[:, 0]\n        y = centers[:, 1]\n\n        # Build A_ub, b_ub\n        A_rows = []\n        b_vals = []\n\n        # Boundary constraints\n        for i in range(N):\n            e = np.zeros(N); e[i] = 1.0\n            A_rows.append(e.copy()); b_vals.append(x[i])\n            A_rows.append(e.copy()); b_vals.append(y[i])\n            A_rows.append(e.copy()); b_vals.append(1.0 - x[i])\n            A_rows.append(e.copy()); b_vals.append(1.0 - y[i])\n\n        # Pairwise constraints\n        d, _, _ = _pairwise_distances(centers)\n        for i in range(N):\n            for j in range(i+1, N):\n                e = np.zeros(N); e[i] = 1.0; e[j] = 1.0\n                A_rows.append(e); b_vals.append(d[i, j] - eps_pair)\n\n        # Optional upper bounds r_i <= r_max[i]\n        if r_max is not None:\n            for i in range(N):\n                e = np.zeros(N); e[i] = 1.0\n                A_rows.append(e); b_vals.append(r_max[i])\n\n        A_ub = np.array(A_rows)\n        b_ub = np.array(b_vals)\n\n        # Bounds on r: [0, 0.5]\n        bounds = [(0.0, 0.5) for _ in range(N)]\n        c = -np.ones(N)\n\n        res = linprog(c, A_ub=A_ub, b_ub=b_ub, bounds=bounds, method='highs')\n        if res.success:\n            return np.clip(res.x, 0.0, 0.5)\n\n        # Retry with eps_pair = 0 to get a feasible base, then scale to satisfy eps_pair\n        b_vals2 = []\n        A_rows2 = []\n        for i in range(N):\n            e = np.zeros(N); e[i] = 1.0\n            A_rows2.append(e.copy()); b_vals2.append(x[i])\n            A_rows2.append(e.copy()); b_vals2.append(y[i])\n            A_rows2.append(e.copy()); b_vals2.append(1.0 - x[i])\n            A_rows2.append(e.copy()); b_vals2.append(1.0 - y[i])\n\n        for i in range(N):\n            for j in range(i+1, N):\n                e = np.zeros(N); e[i] = 1.0; e[j] = 1.0\n                A_rows2.append(e); b_vals2.append(d[i, j])\n\n        if r_max is not None:\n            for i in range(N):\n                e = np.zeros(N); e[i] = 1.0\n                A_rows2.append(e); b_vals2.append(r_max[i])\n\n        A_ub2 = np.array(A_rows2)\n        b_ub2 = np.array(b_vals2)\n\n        res2 = linprog(c, A_ub=A_ub2, b_ub=b_ub2, bounds=bounds, method='highs')\n        if not res2.success:\n            return np.zeros(N)\n\n        r_base = np.clip(res2.x, 0.0, 0.5)\n        d, _, _ = _pairwise_distances(centers)\n        denom = r_base[:, None] + r_base[None, :]\n        with np.errstate(divide='ignore', invalid='ignore'):\n            ratio = np.where(denom > 0, (d - eps_pair) / denom, np.inf)\n        iu = np.triu_indices(N, 1)\n        if ratio[iu].size > 0:\n            s_pairs = np.min(ratio[iu])\n            if not np.isfinite(s_pairs):\n                s_pairs = 1.0\n            s = min(1.0, s_pairs)\n        else:\n            s = 1.0\n        s = max(0.0, s)\n        r = s * r_base\n        return np.clip(r, 0.0, 0.5)\n\n    def _verify(centers, radii, tol_pair=1e-6):\n        N = len(radii)\n        x = centers[:, 0]\n        y = centers[:, 1]\n        if centers.shape != (N, 2):\n            return False\n        if np.any(radii < -1e-12):\n            return False\n        if np.any(x - radii < -1e-9):\n            return False\n        if np.any(y - radii < -1e-9):\n            return False\n        if np.any(x + radii > 1.0 + 1e-9):\n            return False\n        if np.any(y + radii > 1.0 + 1e-9):\n            return False\n        d, _, _ = _pairwise_distances(centers)\n        iu = np.triu_indices(N, 1)\n        lhs = radii[:, None] + radii[None, :]\n        viol = lhs[iu] - d[iu] + tol_pair\n        if np.any(viol > 1e-12):\n            return False\n        return True\n\n    def _hex_initial_centers_from_rows(rows, margin=0.05, jitter_scale=0.03, seed=0):\n        # rows: list of counts per row, centered with hex offset when fewer than max\n        assert sum(rows) == 26\n        max_cols = max(rows)\n        nrows = len(rows)\n        dx = (1.0 - 2*margin) / (max_cols - 1) if max_cols > 1 else 0.0\n        dy = (1.0 - 2*margin) / (nrows - 1) if nrows > 1 else 0.0\n        centers = []\n        rng = np.random.default_rng(seed)\n        for ri, cols in enumerate(rows):\n            y = margin + ri * dy if nrows > 1 else 0.5\n            if cols == max_cols:\n                startx = margin\n            else:\n                startx = margin + 0.5 * dx\n            if cols == 1:\n                startx = 0.5\n            for k in range(cols):\n                x = startx + (k * dx if cols > 1 else 0.0)\n                centers.append([x, y])\n        centers = np.array(centers, dtype=float)\n        # Jitter and clip\n        jitter = rng.normal(scale=jitter_scale, size=centers.shape)\n        centers = centers + jitter\n        centers = np.clip(centers, 1e-3, 1 - 1e-3)\n        return centers\n\n    def _multi_start_inits(N):\n        # Various row patterns summing to 26\n        patterns = [\n            [6, 5, 6, 5, 4],\n            [6, 5, 5, 5, 5],\n            [5, 6, 5, 5, 5],\n            [5, 5, 6, 5, 5],\n            [6, 6, 5, 5, 4],\n            [5, 6, 6, 5, 4],\n            [6, 5, 6, 4, 5],\n        ]\n        params = [\n            (0.03, 0.02), (0.04, 0.03), (0.05, 0.03),\n            (0.06, 0.02), (0.07, 0.03), (0.05, 0.04),\n        ]\n        inits = []\n        seed = 0\n        target_starts = 18\n        for pat in patterns:\n            for (m, j) in params:\n                inits.append(_hex_initial_centers_from_rows(pat, margin=m, jitter_scale=j, seed=seed))\n                seed += 1\n                if len(inits) >= target_starts:\n                    return inits\n        return inits\n\n    def _center_only_relax(centers, r_fixed, P=2000.0, w_b=1.0, w_o=1.0, iters=80):\n        N = centers.shape[0]\n        z0 = np.concatenate([centers[:, 0], centers[:, 1]])\n        bounds_xy = [(0.0, 1.0)] * (2 * N)\n        res = minimize(\n            fun=lambda zz: _objective_centers_only(zz, r_fixed, P, w_b, w_o, True),\n            x0=z0,\n            method='L-BFGS-B',\n            jac=True,\n            bounds=bounds_xy,\n            options={'maxiter': iters, 'ftol': 1e-12, 'gtol': 1e-8, 'maxcor': 20}\n        )\n        z = res.x\n        centers_new = np.stack([z[:N], z[N:2*N]], axis=1)\n        return centers_new\n\n    # ---------------- Main optimization ----------------\n    N = 26\n    best = {'centers': None, 'radii': None, 'sum': -np.inf}\n\n    # Penalty schedule and bounds\n    P_schedule = [8.0, 80.0, 800.0, 3000.0, 10000.0]\n    maxiter_stage = [90, 100, 110, 120, 130]\n    bounds = [(0.0, 1.0)] * (2*N) + [(0.0, 0.5)] * N\n\n    # Weighting of penalties\n    w_b = 1.0\n    w_o = 1.0\n\n    # Initializations\n    inits = _multi_start_inits(N)\n    if not inits:\n        inits = [_hex_initial_centers_from_rows([6,5,6,5,4], margin=0.05, jitter_scale=0.03, seed=42)]\n\n    for init_idx, centers0 in enumerate(inits):\n        x0 = centers0[:, 0]\n        y0 = centers0[:, 1]\n        # Start with small radii within local boundary\n        base_bound = np.minimum.reduce([x0, y0, 1 - x0, 1 - y0])\n        r0 = 0.05 * base_bound\n        z = np.concatenate([x0, y0, r0])\n\n        # Annealing optimization with periodic LP tightening and center-only relaxation\n        for si, P in enumerate(P_schedule):\n            res = minimize(\n                fun=lambda zz: _objective_and_grad(zz, N, P, w_b, w_o, True),\n                x0=z,\n                method='L-BFGS-B',\n                jac=True,\n                bounds=bounds,\n                options={'maxiter': maxiter_stage[si], 'ftol': 1e-12, 'gtol': 1e-8, 'maxcor': 30}\n            )\n            z = res.x\n            centers_tmp = np.stack([z[:N], z[N:2*N]], axis=1)\n            r_cur = np.clip(z[2*N:], 0.0, 0.5)\n            # Conservative LP cap slightly above current radii\n            cap_factor = 1.08 if si < len(P_schedule) - 1 else 1.15\n            r_cap = np.minimum(0.5, cap_factor * np.maximum(1e-9, r_cur))\n            r_lp = _lp_feasibilize(centers_tmp, r_max=r_cap, eps_pair=1e-6)\n            # Small center-only relaxation to open space, then re-LP\n            centers_relaxed = _center_only_relax(centers_tmp, r_lp, P=max(1000.0, P), w_b=w_b, w_o=w_o, iters=70)\n            r_lp2 = _lp_feasibilize(centers_relaxed, r_max=None, eps_pair=1e-6)\n            z = np.concatenate([centers_relaxed[:, 0], centers_relaxed[:, 1], np.clip(r_lp2, 0.0, 0.5)])\n\n        # Final LP without cap to fully utilize geometry\n        centers_cur = np.stack([z[:N], z[N:2*N]], axis=1)\n        r_final = _lp_feasibilize(centers_cur, r_max=None, eps_pair=1e-6)\n\n        # Verify and update best\n        if _verify(centers_cur, r_final, tol_pair=1e-6):\n            s = float(np.sum(r_final))\n            if s > best['sum']:\n                best['centers'] = centers_cur.copy()\n                best['radii'] = r_final.copy()\n                best['sum'] = s\n        else:\n            # Try slight shrink and re-LP to fix tiny violations\n            d, _, _ = _pairwise_distances(centers_cur)\n            denom = r_final[:, None] + r_final[None, :]\n            with np.errstate(divide='ignore', invalid='ignore'):\n                ratio = np.where(denom > 0, (d - 1.0e-6) / denom, np.inf)\n            iu = np.triu_indices(N, 1)\n            s_pairs = np.min(ratio[iu]) if ratio[iu].size > 0 else 1.0\n            if not np.isfinite(s_pairs):\n                s_pairs = 1.0\n            s_pairs = max(0.0, min(1.0, s_pairs))\n            r_try = s_pairs * r_final\n            # Boundary scaling\n            bx = np.minimum.reduce([centers_cur[:,0], centers_cur[:,1], 1-centers_cur[:,0], 1-centers_cur[:,1]])\n            with np.errstate(divide='ignore', invalid='ignore'):\n                scale_b = np.min(np.where(r_try > 0, bx / r_try, 1.0))\n            scale_b = min(1.0, scale_b if np.isfinite(scale_b) else 1.0)\n            r_try = scale_b * r_try\n            if _verify(centers_cur, r_try, tol_pair=1e-6):\n                s = float(np.sum(r_try))\n                if s > best['sum']:\n                    best['centers'] = centers_cur.copy()\n                    best['radii'] = r_try.copy()\n                    best['sum'] = s\n\n    # Fallback if nothing verified\n    if best['centers'] is None:\n        centers_fb = _hex_initial_centers_from_rows([6,5,6,5,4], margin=0.06, jitter_scale=0.02, seed=123)\n        r_fb = _lp_feasibilize(centers_fb, r_max=None, eps_pair=1e-6)\n        if not _verify(centers_fb, r_fb, tol_pair=1e-6):\n            r_fb = 0.95 * r_fb\n        best['centers'] = centers_fb\n        best['radii'] = np.clip(r_fb, 0.0, 0.5)\n        best['sum'] = float(np.sum(best['radii']))\n\n    # Ensure strict feasibility and final verify\n    radii_strict = _lp_feasibilize(best['centers'], r_max=None, eps_pair=1.1e-6)\n    if _verify(best['centers'], radii_strict, tol_pair=1e-6):\n        radii = radii_strict\n    else:\n        radii = best['radii']\n\n    centers = best['centers']\n    # Final hardening: if any violation persists, scale radii conservatively\n    if not _verify(centers, radii, tol_pair=1e-6):\n        d, _, _ = _pairwise_distances(centers)\n        denom = radii[:, None] + radii[None, :]\n        with np.errstate(divide='ignore', invalid='ignore'):\n            ratio = np.where(denom > 0, (d - 1e-6) / denom, np.inf)\n        iu = np.triu_indices(N, 1)\n        s_pairs = np.min(ratio[iu]) if ratio[iu].size > 0 else 1.0\n        s_pairs = min(1.0, s_pairs if np.isfinite(s_pairs) else 1.0)\n        s_pairs = max(0.0, s_pairs)\n        radii = s_pairs * radii\n        bx = np.minimum.reduce([centers[:,0], centers[:,1], 1-centers[:,0], 1-centers[:,1]])\n        with np.errstate(divide='ignore', invalid='ignore'):\n            scale_b = np.min(np.where(radii > 0, bx / radii, 1.0))\n        scale_b = min(1.0, scale_b if np.isfinite(scale_b) else 1.0)\n        radii = scale_b * radii\n\n    radii = np.clip(radii, 0.0, 0.5)\n    sum_radii = float(np.sum(radii))\n    return centers.astype(float), radii.astype(float), sum_radii\n\n\nRequirements:\n- Single Python code block only. No prose. Implement construct_packing() as entry point.\n- Return (centers, radii, sum_radii) strictly feasible for 26 circles in [0,1]^2.\n- Use numpy, scipy only. No prints. Runtime <= 300s.\n- Use analytic, vectorized gradients if optimizing; apply eps only in divisions (not in penalties).\n- Finalize with an LP: maximize sum(r) s.t.\n  0 <= r_i <= bound_i where bound_i = min(x_i, y_i, 1-x_i, 1-y_i), and\n  r_i + r_j <= dist_ij - 1e-6 for all i<j.\n- Shapes: centers (26,2), radii (26,), sum_radii float.\n\nImplementation tips:\n- Multi-start on centers; seeds: hex grid, edge/corner-biased jitter, and refined best-known-like layouts.\n- Penalized objective for centers (optional radii vars) with annealed penalties; adaptive pairwise weights by local density.\n- Stable distances: use d = sqrt(||\u0394||^2 + eps^2); hinge = relu(overlap).\n- Cache pairwise indices; optionally k-NN masks for speed; widen mask over annealing.\n- LP: build A_ub once; warm-start bounds from current radii; use HiGHS method.\n\nFeasibility:\n- Always LP at end; clip tiny negatives to 0; verify non-overlap with 1e-6 tolerance.\n\n",
  "response": "An expert-level implementation for the Circle Packing problem, designed to find a high-quality solution for packing 26 circles into a unit square.\n\n### Method Explanation\n\nThis solution employs a sophisticated optimization strategy that combines several powerful techniques to navigate the complex, non-convex search space of the circle packing problem.\n\n1.  **Multi-Start Optimization:** The algorithm does not rely on a single starting configuration. Instead, it generates multiple, diverse initial layouts for the circle centers. These layouts are based on hexagonal-like grids with varying parameters and random jitter, increasing the probability of finding a globally competitive solution.\n\n2.  **Penalty-Based Annealing:** For each starting configuration, the problem is framed as minimizing a penalized objective function: `Objective = -sum(radii) + P * Penalty`. The `Penalty` term measures violations of boundary and non-overlap constraints. The algorithm gradually increases the penalty weight `P` in an \"annealing\" schedule. This allows the optimizer to make broad exploratory moves initially (low `P`) and then fine-tune the packing to strictly satisfy constraints later (high `P`).\n\n3.  **Hybrid Optimization Loop:** The core of the algorithm is an iterative loop that intelligently alternates between different optimization sub-problems:\n    *   **Joint L-BFGS-B:** It first performs a joint optimization of both circle `centers` and `radii` using the L-BFGS-B algorithm on the smooth, penalized objective function. This allows centers and radii to co-evolve efficiently.\n    *   **Linear Programming (LP) for Radii:** After the L-BFGS-B step, the centers are temporarily fixed. The problem of finding the largest possible radii for these fixed centers becomes a Linear Program, which can be solved exactly and efficiently. This step \"snaps\" the solution back to a state of strict feasibility and maximizes the radii for the current geometry.\n    *   **Centers-Only Relaxation:** With the new, larger radii from the LP step, the algorithm performs a relaxation step that optimizes *only* the centers. The goal is to minimize constraint violations for the now-fixed radii, effectively pushing circles apart to create more \"slack\" in the packing.\n    *   **Iterative Refinement:** This cycle of joint optimization, LP solving, and center relaxation is repeated at each stage of the penalty annealing schedule, allowing the packing to progressively improve.\n\n4.  **Adaptive Weighting:** The penalty for overlapping circles is adaptively weighted based on local density. Pairs of circles in more crowded regions receive a higher penalty, focusing the optimizer's effort on resolving the most critical bottlenecks in the packing.\n\n5.  **Robustness and Feasibility:** The entire process is designed for robustness. The LP solver includes a fallback mechanism to handle numerically challenging configurations. The final solution returned is guaranteed to be strictly feasible by running a final LP solve with a conservative non-overlap tolerance (`1.1e-6`).\n\nThis combination of multi-start, penalty annealing, and a hybrid optimization loop makes the algorithm a powerful tool for finding high-quality solutions to this difficult geometric problem.\n\n```python\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.optimize import linprog\n\ndef construct_packing():\n    \"\"\"\n    Packs 26 non-overlapping circles into a unit square [0,1]x[0,1]\n    to maximize the sum of their radii.\n\n    The method uses a multi-start, annealing-based optimization approach.\n    Each start point is optimized using a penalty method, where the penalty\n    for violating boundary and overlap constraints is gradually increased.\n    The optimization alternates between:\n    1. A joint optimization of centers and radii using L-BFGS-B on a\n       penalized objective function.\n    2. A Linear Programming (LP) step to find the maximum radii for the\n       current centers, ensuring strict feasibility.\n    3. A centers-only relaxation step (with fixed radii from the LP) to\n       create more space around active constraints.\n\n    This iterative process allows the packing to escape local minima and\n    gradually improve the total sum of radii. The best result over\n    multiple random initializations is returned.\n    \"\"\"\n    N = 26\n\n    # ---------------- Helper functions ----------------\n    def _pairwise_distances(centers):\n        \"\"\"Computes pairwise distances and difference vectors.\"\"\"\n        diff = centers[:, None, :] - centers[None, :, :]\n        d_sq = np.sum(diff**2, axis=2)\n        d = np.sqrt(d_sq + 1e-24) # Add epsilon for stability\n        return d, diff[..., 0], diff[..., 1]\n\n    def _density_weights(centers, k=5, eps=1e-12):\n        \"\"\"Computes density weights based on k-NN distances.\"\"\"\n        d, _, _ = _pairwise_distances(centers)\n        np.fill_diagonal(d, np.inf)\n        k_eff = min(k, N - 1)\n        if k_eff <= 0: return np.ones(N)\n        \n        knn_dists = np.partition(d, kth=k_eff-1, axis=1)[:, :k_eff]\n        mean_knn_dist = np.mean(knn_dists, axis=1)\n        density = 1.0 / (mean_knn_dist + eps)\n        return density\n\n    def _objective_and_grad(z, P, w_b, w_o, use_density):\n        \"\"\"Objective and gradient for joint optimization of centers and radii.\"\"\"\n        x, y, r = z[:N], z[N:2*N], z[2*N:]\n        centers = np.stack([x, y], axis=1)\n        tiny = 1e-12\n\n        # Boundary penalties (squared hinge loss)\n        s_b = np.array([r - x, r - y, r + x - 1.0, r + y - 1.0])\n        h_b = np.maximum(0.0, s_b)\n        pen_b = 0.5 * np.sum(h_b**2)\n\n        # Overlap penalties\n        d, dx_mat, dy_mat = _pairwise_distances(centers)\n        iu = np.triu_indices(N, 1)\n        d_ij = d[iu]\n        r_sum_ij = r[iu[0]] + r[iu[1]]\n        h_o = np.maximum(0.0, r_sum_ij - d_ij)\n\n        if use_density:\n            dens = _density_weights(centers, k=5, eps=tiny)\n            w_ij = 0.5 * (dens[iu[0]] + dens[iu[1]])\n        else:\n            w_ij = 1.0\n        \n        pen_o = 0.5 * np.sum(w_ij * h_o**2)\n        f = -np.sum(r) + P * (w_b * pen_b + w_o * pen_o)\n\n        # Gradients\n        dr_b = h_b[0] + h_b[1] + h_b[2] + h_b[3]\n        dx_b = -h_b[0] + h_b[2]\n        dy_b = -h_b[1] + h_b[3]\n\n        dr_o, dx_o, dy_o = np.zeros(N), np.zeros(N), np.zeros(N)\n        d_safe = np.maximum(d_ij, tiny)\n        grad_coeff = (w_ij * h_o) / d_safe\n        \n        np.add.at(dr_o, iu[0], w_ij * h_o)\n        np.add.at(dr_o, iu[1], w_ij * h_o)\n        \n        dx_contrib = grad_coeff * dx_mat[iu]\n        dy_contrib = grad_coeff * dy_mat[iu]\n        np.add.at(dx_o, iu[0], -dx_contrib)\n        np.add.at(dx_o, iu[1], +dx_contrib)\n        np.add.at(dy_o, iu[0], -dy_contrib)\n        np.add.at(dy_o, iu[1], +dy_contrib)\n\n        grad = np.concatenate([\n            P * (w_b * dx_b + w_o * dx_o),\n            P * (w_b * dy_b + w_o * dy_o),\n            -1.0 + P * (w_b * dr_b + w_o * dr_o)\n        ])\n        return f, grad\n\n    def _objective_centers_only(z_xy, r_fixed, P, w_b, w_o, use_density):\n        \"\"\"Objective and gradient for centers-only relaxation.\"\"\"\n        x, y, r = z_xy[:N], z_xy[N:], r_fixed\n        centers = np.stack([x, y], axis=1)\n        tiny = 1e-12\n\n        s_b = np.array([r - x, r - y, r + x - 1.0, r + y - 1.0])\n        h_b = np.maximum(0.0, s_b)\n        pen_b = 0.5 * np.sum(h_b**2)\n\n        d, dx_mat, dy_mat = _pairwise_distances(centers)\n        iu = np.triu_indices(N, 1)\n        d_ij = d[iu]\n        r_sum_ij = r[iu[0]] + r[iu[1]]\n        h_o = np.maximum(0.0, r_sum_ij - d_ij)\n\n        if use_density:\n            dens = _density_weights(centers, k=5, eps=tiny)\n            w_ij = 0.5 * (dens[iu[0]] + dens[iu[1]])\n        else:\n            w_ij = 1.0\n            \n        pen_o = 0.5 * np.sum(w_ij * h_o**2)\n        f = P * (w_b * pen_b + w_o * pen_o)\n\n        dx_b, dy_b = -h_b[0] + h_b[2], -h_b[1] + h_b[3]\n        dx_o, dy_o = np.zeros(N), np.zeros(N)\n        d_safe = np.maximum(d_ij, tiny)\n        grad_coeff = (w_ij * h_o) / d_safe\n        \n        dx_contrib = grad_coeff * dx_mat[iu]\n        dy_contrib = grad_coeff * dy_mat[iu]\n        np.add.at(dx_o, iu[0], -dx_contrib)\n        np.add.at(dx_o, iu[1], +dx_contrib)\n        np.add.at(dy_o, iu[0], -dy_contrib)\n        np.add.at(dy_o, iu[1], +dy_contrib)\n\n        grad = np.concatenate([\n            P * (w_b * dx_b + w_o * dx_o),\n            P * (w_b * dy_b + w_o * dy_o)\n        ])\n        return f, grad\n\n    def _lp_solve(centers, r_max=None, eps_pair=1e-6):\n        \"\"\"Solves the LP to find optimal radii for fixed centers.\"\"\"\n        x, y = centers[:, 0], centers[:, 1]\n        c = -np.ones(N)\n\n        num_b_constraints = 4 * N\n        num_p_constraints = N * (N - 1) // 2\n        num_cap_constraints = N if r_max is not None else 0\n        num_constraints = num_b_constraints + num_p_constraints + num_cap_constraints\n        \n        A_ub = np.zeros((num_constraints, N))\n        b_ub = np.zeros(num_constraints)\n        \n        A_ub[np.arange(num_b_constraints), np.tile(np.arange(N), 4)] = 1.0\n        b_ub[:N], b_ub[N:2*N] = x, y\n        b_ub[2*N:3*N], b_ub[3*N:4*N] = 1.0 - x, 1.0 - y\n        \n        d, _, _ = _pairwise_distances(centers)\n        iu = np.triu_indices(N, 1)\n        row_idx = num_b_constraints\n        A_ub[row_idx : row_idx + num_p_constraints, iu[0]] = 1.0\n        A_ub[row_idx : row_idx + num_p_constraints, iu[1]] = 1.0\n        b_ub[row_idx : row_idx + num_p_constraints] = d[iu] - eps_pair\n        \n        if r_max is not None:\n            row_idx += num_p_constraints\n            A_ub[row_idx : row_idx + N, np.arange(N)] = 1.0\n            b_ub[row_idx : row_idx + N] = r_max\n\n        res = linprog(c, A_ub=A_ub, b_ub=b_ub, bounds=(0.0, 0.5), method='highs')\n\n        if res.success:\n            return np.clip(res.x, 0.0, None)\n        \n        b_ub[num_b_constraints : num_b_constraints + num_p_constraints] += eps_pair\n        res_fallback = linprog(c, A_ub=A_ub, b_ub=b_ub, bounds=(0.0, 0.5), method='highs')\n        \n        if not res_fallback.success: return np.zeros(N)\n        r_base = np.clip(res_fallback.x, 0.0, None)\n        \n        r_sum_ij = r_base[iu[0]] + r_base[iu[1]]\n        d_ij = d[iu]\n        \n        safe_denom = np.where(r_sum_ij > 1e-12, r_sum_ij, 1.0)\n        target_num = np.where(r_sum_ij > 1e-12, d_ij - eps_pair, np.inf)\n        ratios = target_num / safe_denom\n        scale = min(1.0, np.min(ratios) if len(ratios) > 0 else 1.0)\n        return np.clip(scale * r_base, 0.0, None)\n\n    def _verify(centers, radii, tol=1e-6):\n        \"\"\"Verify if a packing is valid.\"\"\"\n        if np.any(radii < 0): return False\n        if np.any(centers - radii[:, None] < -tol) or np.any(centers + radii[:, None] > 1 + tol): return False\n        d, _, _ = _pairwise_distances(centers)\n        iu = np.triu_indices(N, 1)\n        return np.all((radii[iu[0]] + radii[iu[1]]) - d[iu] < tol)\n\n    def _get_initial_centers(rows, margin, jitter_scale, seed):\n        \"\"\"Generate initial centers based on a hex-like grid layout.\"\"\"\n        rng = np.random.default_rng(seed)\n        max_cols, num_rows = max(rows), len(rows)\n        dx = (1.0 - 2 * margin) / (max_cols - 1) if max_cols > 1 else 0\n        dy = (1.0 - 2 * margin) / (num_rows - 1) if num_rows > 1 else 0\n        \n        centers = []\n        for i, num_cols in enumerate(rows):\n            y_pos = margin + i * dy if num_rows > 1 else 0.5\n            row_width = (num_cols - 1) * dx if num_cols > 1 else 0\n            x_start = (1.0 - row_width) / 2.0\n            for j in range(num_cols):\n                centers.append([x_start + j * dx, y_pos])\n        \n        centers = np.array(centers)\n        centers += rng.normal(scale=jitter_scale, size=centers.shape)\n        return np.clip(centers, 0.0, 1.0)\n\n    def _multi_start_inits():\n        \"\"\"Generate a list of diverse initial center configurations.\"\"\"\n        patterns = [\n            [6, 5, 6, 5, 4], [5, 6, 5, 6, 4], [4, 5, 6, 5, 6],\n            [6, 6, 5, 5, 4], [5, 5, 6, 6, 4],\n        ]\n        inits = []\n        seed = 0\n        for pat in patterns:\n            for margin in [0.05, 0.08]:\n                for jitter in [0.03]:\n                    inits.append(_get_initial_centers(pat, margin, jitter, seed))\n                    seed += 1\n        rng = np.random.default_rng(seed)\n        inits.append(rng.random((N, 2)))\n        inits.append(rng.random((N, 2)))\n        return inits\n\n    # ---------------- Main optimization logic ----------------\n    best_sum, best_centers, best_radii = -1.0, None, None\n    initial_configs = _multi_start_inits()\n    \n    P_schedule = [10.0, 100.0, 1000.0, 5000.0]\n    maxiter_schedule = [80, 90, 100, 110]\n    w_b, w_o = 1.0, 1.0\n    bounds = [(0.0, 1.0)] * (2 * N) + [(0.0, 0.5)] * N\n\n    for centers_init in initial_configs:\n        x, y = centers_init[:, 0], centers_init[:, 1]\n        r_init = 0.1 * np.min([x, y, 1-x, 1-y], axis=0)\n        z = np.concatenate([x, y, r_init])\n\n        for i, P in enumerate(P_schedule):\n            res = minimize(\n                fun=lambda z_var: _objective_and_grad(z_var, P, w_b, w_o, True),\n                x0=z, method='L-BFGS-B', jac=True, bounds=bounds,\n                options={'maxiter': maxiter_schedule[i], 'ftol': 1e-12, 'gtol': 1e-8}\n            )\n            z = res.x\n            \n            centers_cur = np.stack([z[:N], z[N:2*N]], axis=1)\n            r_lp = _lp_solve(centers_cur, r_max=np.minimum(0.5, 1.1 * z[2*N:]), eps_pair=1e-6)\n            \n            res_relax = minimize(\n                fun=lambda z_xy: _objective_centers_only(z_xy, r_lp, P, w_b, w_o, True),\n                x0=z[:2*N], method='L-BFGS-B', jac=True, bounds=bounds[:2*N],\n                options={'maxiter': 60, 'ftol': 1e-12, 'gtol': 1e-8}\n            )\n            centers_relaxed = np.stack([res_relax.x[:N], res_relax.x[N:]], axis=1)\n            \n            r_final_stage = _lp_solve(centers_relaxed, eps_pair=1e-6)\n            z = np.concatenate([centers_relaxed[:, 0], centers_relaxed[:, 1], r_final_stage])\n\n        final_centers = np.stack([z[:N], z[N:2*N]], axis=1)\n        final_radii = _lp_solve(final_centers, eps_pair=1e-6)\n        \n        current_sum = np.sum(final_radii)\n        if current_sum > best_sum and _verify(final_centers, final_radii):\n            best_sum, best_centers, best_radii = current_sum, final_centers, final_radii\n\n    if best_centers is None:\n        centers_fb = _get_initial_centers([6, 5, 6, 5, 4], 0.05, 0.01, 42)\n        best_centers, best_radii = centers_fb, _lp_solve(centers_fb, eps_pair=1e-6)\n\n    final_radii_strict = _lp_solve(best_centers, eps_pair=1.1e-6)\n    if _verify(best_centers, final_radii_strict):\n        best_radii = final_radii_strict\n    \n    best_radii = np.clip(best_radii, 0.0, 0.5)\n    sum_radii = float(np.sum(best_radii))\n\n    return best_centers.astype(float), best_radii.astype(float), sum_radii\n```",
  "reasoning": "An expert-level implementation for the Circle Packing problem, designed to find a high-quality solution for packing 26 circles into a unit square.\n\n### Method Explanation\n\nThis solution employs a sophisticated optimization strategy that combines several powerful techniques to navigate the complex, non-convex search space of the circle packing problem.\n\n1.  **Multi-Start Optimization:** The algorithm does not rely on a single starting configuration. Instead, it generates multiple, diverse initial layouts for the circle centers. These layouts are based on hexagonal-like grids with varying parameters and random jitter, increasing the probability of finding a globally competitive solution.\n\n2.  **Penalty-Based Annealing:** For each starting configuration, the problem is framed as minimizing a penalized objective function: `Objective = -sum(radii) + P * Penalty`. The `Penalty` term measures violations of boundary and non-overlap constraints. The algorithm gradually increases the penalty weight `P` in an \"annealing\" schedule. This allows the optimizer to make broad exploratory moves initially (low `P`) and then fine-tune the packing to strictly satisfy constraints later (high `P`).\n\n3.  **Hybrid Optimization Loop:** The core of the algorithm is an iterative loop that intelligently alternates between different optimization sub-problems:\n    *   **Joint L-BFGS-B:** It first performs a joint optimization of both circle `centers` and `radii` using the L-BFGS-B algorithm on the smooth, penalized objective function. This allows centers and radii to co-evolve efficiently.\n    *   **Linear Programming (LP) for Radii:** After the L-BFGS-B step, the centers are temporarily fixed. The problem of finding the largest possible radii for these fixed centers becomes a Linear Program, which can be solved exactly and efficiently. This step \"snaps\" the solution back to a state of strict feasibility and maximizes the radii for the current geometry.\n    *   **Centers-Only Relaxation:** With the new, larger radii from the LP step, the algorithm performs a relaxation step that optimizes *only* the centers. The goal is to minimize constraint violations for the now-fixed radii, effectively pushing circles apart to create more \"slack\" in the packing.\n    *   **Iterative Refinement:** This cycle of joint optimization, LP solving, and center relaxation is repeated at each stage of the penalty annealing schedule, allowing the packing to progressively improve.\n\n4.  **Adaptive Weighting:** The penalty for overlapping circles is adaptively weighted based on local density. Pairs of circles in more crowded regions receive a higher penalty, focusing the optimizer's effort on resolving the most critical bottlenecks in the packing.\n\n5.  **Robustness and Feasibility:** The entire process is designed for robustness. The LP solver includes a fallback mechanism to handle numerically challenging configurations. The final solution returned is guaranteed to be strictly feasible by running a final LP solve with a conservative non-overlap tolerance (`1.1e-6`).\n\nThis combination of multi-start, penalty annealing, and a hybrid optimization loop makes the algorithm a powerful tool for finding high-quality solutions to this difficult geometric problem.",
  "timestamp": "2026-02-11T03:05:25.422789",
  "cost_data": null,
  "model_config": {
    "model": "gemini-2.5-pro",
    "temperature": 0.3,
    "provider_debug": {
      "finish_reason": "STOP",
      "usage_metadata": {
        "prompt_token_count": 7263,
        "candidates_token_count": 5078,
        "thoughts_token_count": 12316
      },
      "response_text_len": 14858
    }
  },
  "llm_call_duration_s": 132.01295214809943,
  "eval_duration_s": 1.9315360459731892
}