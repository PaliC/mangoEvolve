{
  "trial_id": "trial_18_0",
  "generation": 18,
  "parent_id": "trial_10_2",
  "code": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.optimize import linprog\n\n# ---------------------------- Utilities ---------------------------- #\n\ndef _pairwise_distances(centers):\n    # centers: (N,2)\n    diff = centers[:, None, :] - centers[None, :, :]\n    d = np.sqrt(np.sum(diff * diff, axis=2))\n    return d, diff[..., 0], diff[..., 1]\n\ndef _adaptive_pair_weights(centers, k=8, alpha=1.0, eps=1e-12, clip=(0.3, 3.0)):\n    \"\"\"\n    Compute adaptive weights w_ij for pairs (i<j) based on local crowding.\n    For each point, compute mean distance to its k nearest neighbors (excluding itself).\n    Weight for pair (i,j):\n        w_ij = ( 1 / (eps + 0.5*(mean_kNN_i + mean_kNN_j)) )**alpha\n    Then normalize to mean 1 and clip to [clip[0], clip[1]] to stabilize.\n    Returns:\n        w_ij for upper-triangular index order (np.triu_indices(N,1))\n    \"\"\"\n    N = centers.shape[0]\n    d, _, _ = _pairwise_distances(centers)\n    # Exclude self-distances by setting to inf\n    with np.errstate(invalid='ignore'):\n        d2 = d.copy()\n        np.fill_diagonal(d2, np.inf)\n    k_eff = max(1, min(k, max(1, N - 1)))\n    # partial sort to k smallest\n    # Use np.partition for efficiency\n    kth = np.partition(d2, kth=k_eff-1, axis=1)[:, :k_eff]  # (N,k_eff)\n    mean_knn = np.mean(kth, axis=1)  # (N,)\n    iu = np.triu_indices(N, 1)\n    avg_local = 0.5 * (mean_knn[iu[0]] + mean_knn[iu[1]])\n    w = (1.0 / (eps + avg_local)) ** alpha\n    # normalize to mean 1\n    m = np.mean(w)\n    if m > 0:\n        w = w / m\n    # clip\n    if clip is not None:\n        w = np.clip(w, clip[0], clip[1])\n    return w\n\n# -------------------- Penalized objective with grads -------------------- #\n\ndef _objective_and_grad(z, N, mu, w_b=1.0, w_o=1.0, k_neigh=8, alpha_w=1.0):\n    \"\"\"\n    z = [x_0..x_{N-1}, y_0..y_{N-1}, r_0..r_{N-1}]\n    Objective:\n      f = -sum(r) + mu * ( w_b * boundary_penalty + w_o * overlap_penalty )\n    Boundary penalty uses squared ReLU on each of the four hinges per circle:\n      h1 = max(0, r - x), h2 = max(0, r - y), h3 = max(0, r + x - 1), h4 = max(0, r + y - 1)\n      pen_b = 0.5 * sum_i (h1^2 + h2^2 + h3^2 + h4^2)\n    Overlap penalty uses squared ReLU on violations with adaptive pair weights:\n      v_ij = max(0, r_i + r_j - d_ij)\n      pen_o = 0.5 * sum_{i<j} w_ij * v_ij^2\n    Notes:\n      - We recompute w_ij from current centers but treat it as constant for gradient (common in reweighting).\n      - Stabilize only in denominators with a tiny epsilon; no bias in hinges themselves.\n    Returns (f, grad)\n    \"\"\"\n    x = z[:N]\n    y = z[N:2*N]\n    r = z[2*N:]\n\n    # Boundary penalty\n    s1 = r - x\n    s2 = r - y\n    s3 = r + x - 1.0\n    s4 = r + y - 1.0\n\n    h1 = np.maximum(0.0, s1)\n    h2 = np.maximum(0.0, s2)\n    h3 = np.maximum(0.0, s3)\n    h4 = np.maximum(0.0, s4)\n\n    pen_b = 0.5 * np.sum(h1*h1 + h2*h2 + h3*h3 + h4*h4)\n\n    # Centers and pairwise distances\n    centers = np.stack([x, y], axis=1)\n    d, dx_mat, dy_mat = _pairwise_distances(centers)\n    iu = np.triu_indices(N, 1)\n    dab = d[iu]\n    dxab = dx_mat[iu]\n    dyab = dy_mat[iu]\n\n    # Adaptive weights for pairs\n    w_pairs = _adaptive_pair_weights(centers, k=k_neigh, alpha=alpha_w, eps=1e-12, clip=(0.4, 2.5))\n\n    # Overlap violations\n    hij = r[iu[0]] + r[iu[1]] - dab\n    hpos = np.maximum(0.0, hij)\n\n    pen_o = 0.5 * np.sum(w_pairs * hpos * hpos)\n\n    # Total objective\n    f = -np.sum(r) + mu * (w_b * pen_b + w_o * pen_o)\n\n    # Gradients\n    # Boundary gradients\n    dr_b = h1 + h2 + h3 + h4\n    dx_b = -h1 + h3\n    dy_b = -h2 + h4\n\n    # Overlap gradients\n    tiny = 1e-12\n    d_safe = np.maximum(dab, tiny)\n    coeff = (w_pairs * hpos) / d_safe  # shape (M,), M = N*(N-1)/2\n\n    dx_o = np.zeros(N, dtype=float)\n    dy_o = np.zeros(N, dtype=float)\n    dr_o = np.zeros(N, dtype=float)\n\n    cx = coeff * dxab\n    cy = coeff * dyab\n\n    # center grads: for pair (a,b): grad_x_a += -cx, grad_x_b += +cx\n    np.add.at(dx_o, iu[0], -cx)\n    np.add.at(dx_o, iu[1], +cx)\n    np.add.at(dy_o, iu[0], -cy)\n    np.add.at(dy_o, iu[1], +cy)\n\n    # radii grads: grad_r_a += w*hpos, grad_r_b += w*hpos\n    np.add.at(dr_o, iu[0], w_pairs * hpos)\n    np.add.at(dr_o, iu[1], w_pairs * hpos)\n\n    # Combine\n    gx = mu * (w_b * dx_b + w_o * dx_o)\n    gy = mu * (w_b * dy_b + w_o * dy_o)\n    gr = -1.0 + mu * (w_b * dr_b + w_o * dr_o)\n\n    g = np.concatenate([gx, gy, gr])\n    return f, g\n\n# ------------------------ Linear Programming ------------------------ #\n\ndef _lp_maximize_radii(centers, eps_pair=1e-6):\n    \"\"\"\n    Solve LP: maximize sum r_i\n    subject to:\n      0 <= r_i <= 0.5\n      r_i <= x_i - eps, r_i <= y_i - eps, r_i <= 1-x_i - eps, r_i <= 1-y_i - eps\n      r_i + r_j <= ||c_i - c_j|| - eps for all i<j\n    Uses SciPy linprog with 'highs'.\n    Returns radii (N,) or zeros if LP fails (shouldn't).\n    \"\"\"\n    N = centers.shape[0]\n    x = centers[:, 0]\n    y = centers[:, 1]\n\n    A_rows = []\n    b_vals = []\n\n    # Boundary constraints\n    for i in range(N):\n        e = np.zeros(N); e[i] = 1.0\n        A_rows.append(e.copy()); b_vals.append(x[i] - eps_pair)\n        A_rows.append(e.copy()); b_vals.append(y[i] - eps_pair)\n        A_rows.append(e.copy()); b_vals.append(1.0 - x[i] - eps_pair)\n        A_rows.append(e.copy()); b_vals.append(1.0 - y[i] - eps_pair)\n\n    # Pairwise constraints\n    d, _, _ = _pairwise_distances(centers)\n    for i in range(N):\n        for j in range(i+1, N):\n            e = np.zeros(N); e[i] = 1.0; e[j] = 1.0\n            A_rows.append(e); b_vals.append(d[i, j] - eps_pair)\n\n    A_ub = np.array(A_rows, dtype=float)\n    b_ub = np.array(b_vals, dtype=float)\n\n    bounds = [(0.0, 0.5) for _ in range(N)]\n    c = -np.ones(N, dtype=float)  # maximize sum r -> minimize -sum r\n\n    res = linprog(c, A_ub=A_ub, b_ub=b_ub, bounds=bounds, method='highs')\n    if res.success:\n        r = np.clip(res.x, 0.0, 0.5)\n    else:\n        # Fallback (should rarely happen): return zeros\n        r = np.zeros(N, dtype=float)\n    return r\n\ndef _verify(centers, radii, tol_pair=1e-6):\n    \"\"\"\n    Strict feasibility check with tolerance tol_pair in pairwise constraints.\n    \"\"\"\n    N = centers.shape[0]\n    if centers.shape != (N, 2): return False\n    if radii.shape != (N,): return False\n    if np.any(radii < -1e-12): return False\n    x = centers[:, 0]; y = centers[:, 1]\n    # boundary\n    if np.any(x - radii < tol_pair): return False\n    if np.any(y - radii < tol_pair): return False\n    if np.any(x + radii > 1.0 - tol_pair): return False\n    if np.any(y + radii > 1.0 - tol_pair): return False\n    # pairs\n    d, _, _ = _pairwise_distances(centers)\n    iu = np.triu_indices(N, 1)\n    lhs = radii[:, None] + radii[None, :]\n    if np.any(lhs[iu] > d[iu] - tol_pair + 1e-12): return False\n    return True\n\n# ------------------------ Initializations ------------------------ #\n\ndef _hex_initial_centers_from_rows(rows, margin=0.05, jitter_scale=0.02, seed=0):\n    \"\"\"\n    Build centers using a hex-like row pattern with optional jitter and margin.\n    rows: list of ints summing to 26.\n    \"\"\"\n    assert sum(rows) == 26\n    max_cols = max(rows)\n    nrows = len(rows)\n    if max_cols > 1:\n        dx = (1.0 - 2*margin) / (max_cols - 1)\n    else:\n        dx = 0.0\n    if nrows > 1:\n        dy = (1.0 - 2*margin) / (nrows - 1)\n    else:\n        dy = 0.0\n\n    c = []\n    for r_i, cols in enumerate(rows):\n        y = margin + r_i * dy if nrows > 1 else 0.5\n        if cols > 1:\n            width = (cols - 1) * dx\n            startx = 0.5 - 0.5 * width\n            if r_i % 2 == 1 and max_cols > 1:\n                startx += 0.5 * dx\n        else:\n            startx = 0.5\n        for k in range(cols):\n            x = startx + (k * dx if cols > 1 else 0.0)\n            c.append([x, y])\n    c = np.array(c, dtype=float)\n    rng = np.random.default_rng(seed)\n    c = c + rng.normal(scale=jitter_scale, size=c.shape)\n    c = np.clip(c, 1e-3, 1.0 - 1e-3)\n    return c\n\ndef _triangular_lattice_init(n, margin=0.04, jitter=0.02, seed=0):\n    \"\"\"\n    Approximate triangular-lattice fill of the square.\n    Returns exactly n points by cropping/striding.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    # grid dims\n    rows = int(np.sqrt(n)) + 3\n    cols = rows + 2\n    if cols < 3: cols = 3\n    dx = (1.0 - 2*margin) / (cols - 1)\n    dy = (1.0 - 2*margin) / (rows - 1)\n    pts = []\n    for i in range(rows):\n        y = margin + i * dy\n        off = 0.5 * dx if (i % 2 == 1) else 0.0\n        for j in range(cols):\n            x = margin + j * dx + off\n            if 0.0 <= x <= 1.0:\n                pts.append([x, y])\n    pts = np.array(pts, dtype=float)\n    # pick n points with a spread: take every stride\n    if pts.shape[0] > n:\n        idx = np.linspace(0, pts.shape[0]-1, n, dtype=int)\n        pts = pts[idx]\n    elif pts.shape[0] < n:\n        # oversample with random duplicates slightly jittered\n        extra = n - pts.shape[0]\n        add_idx = rng.integers(0, pts.shape[0], size=extra)\n        add = pts[add_idx] + rng.normal(scale=0.5*jitter, size=(extra,2))\n        pts = np.vstack([pts, add])\n    pts = pts + rng.normal(scale=jitter, size=pts.shape)\n    pts = np.clip(pts, 1e-3, 1.0 - 1e-3)\n    return pts\n\ndef _edge_biased_init(n, margin=0.02, jitter=0.02, seed=0):\n    \"\"\"\n    Place some points near edges/corners to encourage boundary hugging.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    m = margin\n    pts = []\n    # corners and edge midpoints bias\n    anchors = [\n        (m, m), (1-m, m), (m, 1-m), (1-m, 1-m),\n        (0.5, m), (0.5, 1-m), (m, 0.5), (1-m, 0.5)\n    ]\n    k = min(len(anchors), n)\n    pts.extend(anchors[:k])\n    # Fill rest with a hex-like background\n    need = n - len(pts)\n    if need > 0:\n        rows = [7, 6, 7, 6]  # sums to 26 if used alone\n        base = _hex_initial_centers_from_rows(rows, margin=0.06, jitter_scale=0.0, seed=seed+1)\n        idx = rng.choice(base.shape[0], size=need, replace=False)\n        pts.extend(base[idx].tolist())\n    pts = np.array(pts, dtype=float)\n    pts = pts + rng.normal(scale=jitter, size=pts.shape)\n    pts = np.clip(pts, 1e-3, 1.0 - 1e-3)\n    return pts\n\ndef _multi_start_inits(N=26):\n    inits = []\n    seeds = list(range(32))\n    patterns = [\n        [6, 5, 6, 5, 4],\n        [7, 6, 7, 6],\n        [6, 7, 6, 7],\n        [6, 6, 7, 7],\n        [5, 6, 5, 5, 5],\n        [6, 6, 6, 4, 4],\n    ]\n    params = [\n        (0.03, 0.02), (0.05, 0.02), (0.06, 0.03), (0.04, 0.025)\n    ]\n    cnt = 0\n    for p in patterns:\n        for (m, j) in params:\n            inits.append(_hex_initial_centers_from_rows(p, margin=m, jitter_scale=j, seed=seeds[cnt % len(seeds)]))\n            cnt += 1\n    # Add lattice and edge-biased variants\n    for s in range(6):\n        inits.append(_triangular_lattice_init(N, margin=0.045 + 0.005*s, jitter=0.02, seed=100+s))\n    for s in range(6):\n        inits.append(_edge_biased_init(N, margin=0.02 + 0.004*s, jitter=0.02, seed=200+s))\n    # Ensure exactly N points for each init and clip\n    ret = []\n    for c in inits:\n        if c.shape[0] != N:\n            # if mismatch (shouldn't), fix via stride or padding\n            if c.shape[0] > N:\n                idx = np.linspace(0, c.shape[0]-1, N, dtype=int)\n                c = c[idx]\n            else:\n                # pad by random picks with tiny jitter\n                rng = np.random.default_rng(999)\n                add_idx = rng.integers(0, c.shape[0], size=N - c.shape[0])\n                add = c[add_idx] + rng.normal(scale=1e-3, size=(N - c.shape[0], 2))\n                c = np.vstack([c, add])\n        ret.append(np.clip(c, 1e-3, 1.0 - 1e-3))\n    return ret\n\n# ------------------------ Main construction ------------------------ #\n\ndef construct_packing():\n    \"\"\"\n    Returns (centers, radii, sum_radii) for N=26 circles.\n    Strategy:\n      - Multi-start centers.\n      - Penalized L-BFGS-B on (x,y,r) with staged annealing on mu and adaptive neighbor weights.\n      - Final LP to maximize radii strictly feasibly.\n      - Elite refinement of top candidates.\n    \"\"\"\n    N = 26\n    # Bounds: x,y in [0,1]; r in [0,0.5]\n    bounds = [(0.0, 1.0)] * (2*N) + [(0.0, 0.5)] * N\n\n    # Annealing schedules\n    mu_schedule_primary = [300.0, 3000.0, 30000.0, 120000.0]\n    k_schedule_primary = [10, 8, 6, 6]\n    iters_primary = [80, 90, 100, 120]\n\n    mu_schedule_refine = [5000.0, 50000.0, 200000.0]\n    k_schedule_refine = [8, 6, 5]\n    iters_refine = [90, 110, 130]\n\n    w_b = 1.0\n    w_o = 1.0\n\n    inits = _multi_start_inits(N)\n    best_candidates = []  # list of dicts with 'centers','r','sum'\n\n    # Primary multi-start optimization\n    for idx, centers0 in enumerate(inits):\n        x0 = centers0[:, 0]\n        y0 = centers0[:, 1]\n        base_bound = np.minimum.reduce([x0, y0, 1 - x0, 1 - y0])\n        r0 = 0.04 * base_bound  # small initial radii\n\n        z = np.concatenate([x0, y0, r0])\n\n        for si, mu in enumerate(mu_schedule_primary):\n            k_neigh = k_schedule_primary[si]\n            max_it = iters_primary[si]\n            res = minimize(\n                fun=lambda zz, mu=mu, k_neigh=k_neigh: _objective_and_grad(\n                    zz, N, mu, w_b=w_b, w_o=w_o, k_neigh=k_neigh, alpha_w=1.0\n                ),\n                x0=z,\n                method='L-BFGS-B',\n                jac=True,\n                bounds=bounds,\n                options={'maxiter': max_it, 'ftol': 1e-12, 'gtol': 1e-8, 'maxcor': 30}\n            )\n            z = res.x  # continue from here\n\n        centers_opt = np.stack([z[:N], z[N:2*N]], axis=1)\n        # Finalize by LP\n        r_lp = _lp_maximize_radii(centers_opt, eps_pair=1e-6)\n        s_lp = float(np.sum(r_lp))\n        best_candidates.append({'centers': centers_opt.copy(), 'r': r_lp.copy(), 'sum': s_lp})\n\n    # Select elite candidates\n    best_candidates.sort(key=lambda d: d['sum'], reverse=True)\n    elites = best_candidates[:6] if len(best_candidates) >= 6 else best_candidates\n\n    # Elite refinement\n    refined = []\n    for cand in elites:\n        centers = cand['centers']\n        r = cand['r']\n        # Start from slightly shrunken radii to allow movement\n        r_start = 0.98 * r\n        z = np.concatenate([centers[:, 0], centers[:, 1], r_start])\n        for si, mu in enumerate(mu_schedule_refine):\n            k_neigh = k_schedule_refine[si]\n            max_it = iters_refine[si]\n            res = minimize(\n                fun=lambda zz, mu=mu, k_neigh=k_neigh: _objective_and_grad(\n                    zz, N, mu, w_b=w_b, w_o=w_o, k_neigh=k_neigh, alpha_w=1.0\n                ),\n                x0=z,\n                method='L-BFGS-B',\n                jac=True,\n                bounds=bounds,\n                options={'maxiter': max_it, 'ftol': 1e-12, 'gtol': 1e-8, 'maxcor': 30}\n            )\n            z = res.x\n\n        centers_ref = np.stack([z[:N], z[N:2*N]], axis=1)\n        r_ref = _lp_maximize_radii(centers_ref, eps_pair=1e-6)\n        s_ref = float(np.sum(r_ref))\n        refined.append({'centers': centers_ref.copy(), 'r': r_ref.copy(), 'sum': s_ref})\n\n    # Choose the best feasible solution\n    all_solutions = refined if refined else best_candidates\n    all_solutions.sort(key=lambda d: d['sum'], reverse=True)\n\n    best = None\n    for sol in all_solutions:\n        if _verify(sol['centers'], sol['r'], tol_pair=1e-6):\n            best = sol\n            break\n    if best is None:\n        # As a safe fallback, take the best LP radii (first one) and ensure feasibility by tiny scaling\n        sol = all_solutions[0]\n        centers = sol['centers']\n        r = sol['r']\n        # Hardening: rerun LP with slightly larger epsilon\n        r2 = _lp_maximize_radii(centers, eps_pair=1.1e-6)\n        if _verify(centers, r2, tol_pair=1e-6):\n            best = {'centers': centers, 'r': r2, 'sum': float(np.sum(r2))}\n        else:\n            # tiny global scale\n            d, _, _ = _pairwise_distances(centers)\n            lhs = r[:, None] + r[None, :]\n            iu = np.triu_indices(N, 1)\n            with np.errstate(divide='ignore', invalid='ignore'):\n                s_pairs = np.min((d[iu] - 1e-6) / np.maximum(lhs[iu], 1e-12))\n            s_pairs = float(np.clip(s_pairs, 0.0, 1.0))\n            bnd = np.minimum.reduce([centers[:, 0], centers[:, 1], 1-centers[:, 0], 1-centers[:, 1]])\n            with np.errstate(divide='ignore', invalid='ignore'):\n                s_b = np.min((bnd - 1e-6) / np.maximum(r, 1e-12))\n            s_b = float(np.clip(s_b if np.isfinite(s_b) else 1.0, 0.0, 1.0))\n            s = min(s_pairs, s_b)\n            r3 = np.clip(s * r, 0.0, 0.5)\n            best = {'centers': centers, 'r': r3, 'sum': float(np.sum(r3))}\n\n    centers = best['centers'].astype(float)\n    radii = np.clip(best['r'], 0.0, 0.5).astype(float)\n    # Final strict LP pass to maximize under strict constraints\n    radii_final = _lp_maximize_radii(centers, eps_pair=1.0e-6)\n    if _verify(centers, radii_final, tol_pair=1.0e-6):\n        radii = radii_final\n\n    sum_radii = float(np.sum(radii))\n    return centers, radii, sum_radii\n\ndef run_packing():\n    return construct_packing()",
  "metrics": {
    "valid": true,
    "score": 2.6067329829977846,
    "eval_time": 5.161789894104004,
    "error": null
  },
  "prompt": "\nImprove the elite penalized L-BFGS-B + LP approach with adaptive neighbor weights and staged annealing.\nReference (for style/ideas, not to copy verbatim):\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.optimize import linprog\n\ndef _pairwise_distances(centers):\n    \"\"\"\n    Calculates pairwise Euclidean distances between centers.\n    Returns distance matrix, and matrices of dx and dy.\n    \"\"\"\n    diff = centers[:, None, :] - centers[None, :, :]\n    d = np.sqrt(np.sum(diff**2, axis=2))\n    return d, diff[..., 0], diff[..., 1]\n\ndef _objective_and_grad(z, N, P, w_b=1.0, w_o=1.0):\n    \"\"\"\n    Objective function and its gradient for joint optimization of centers (x,y) and radii (r).\n    z = [x_0..x_N-1, y_0..y_N-1, r_0..r_N-1]\n    Objective: -sum(r) + P * (w_b * boundary_penalty + w_o * overlap_penalty)\n    Penalties are squared hinge functions.\n    \"\"\"\n    x = z[:N]\n    y = z[N:2*N]\n    r = z[2*N:]\n    tiny = 1e-12 # Small value to prevent division by zero\n\n    # Boundary penalties: hinges for r - x, r - y, r + x - 1, r + y - 1\n    s1 = r - x\n    s2 = r - y\n    s3 = r + x - 1.0\n    s4 = r + y - 1.0\n\n    h1 = np.maximum(0.0, s1)\n    h2 = np.maximum(0.0, s2)\n    h3 = np.maximum(0.0, s3)\n    h4 = np.maximum(0.0, s4)\n\n    pen_b = 0.5 * np.sum(h1*h1 + h2*h2 + h3*h3 + h4*h4)\n\n    # Overlap penalties: hinges for r_i + r_j - d_ij\n    centers = np.stack([x, y], axis=1)\n    d, dx_mat, dy_mat = _pairwise_distances(centers)\n    iu = np.triu_indices(N, 1) # Upper triangle indices for unique pairs (i, j where i < j)\n    dab = d[iu] # Distances for unique pairs\n    dxab = dx_mat[iu]\n    dyab = dy_mat[iu]\n    d_safe = np.maximum(dab, tiny) # Use tiny to prevent division by zero in gradients\n    hij = r[iu[0]] + r[iu[1]] - dab # Overlap amount for each pair\n    hpos = np.maximum(0.0, hij) # Positive part of overlap (hinge)\n    pen_o = 0.5 * np.sum(hpos*hpos)\n\n    # Total objective function value\n    f = -np.sum(r) + P * (w_b * pen_b + w_o * pen_o)\n\n    # Gradients for boundary penalties\n    dr_b = h1 + h2 + h3 + h4\n    dx_b = -h1 + h3\n    dy_b = -h2 + h4\n\n    # Overlap gradients (accumulate with np.add.at for efficiency)\n    dx_o = np.zeros(N, dtype=float)\n    dy_o = np.zeros(N, dtype=float)\n    dr_o = np.zeros(N, dtype=float)\n\n    # Gradient contribution for centers (x, y)\n    # For a pair (a, b), the force on 'a' is in direction (c_b - c_a), force on 'b' is (c_a - c_b)\n    # grad_x_a += -h * (dxab/d), grad_x_b += +h * (dxab/d)\n    coeff = hpos / d_safe # h / d\n    cx = coeff * dxab\n    cy = coeff * dyab\n    np.add.at(dx_o, iu[0], -cx) # For circle i\n    np.add.at(dx_o, iu[1], +cx) # For circle j\n    np.add.at(dy_o, iu[0], -cy)\n    np.add.at(dy_o, iu[1], +cy)\n    \n    # Gradient contribution for radii (r)\n    # grad_r_a += h, grad_r_b += h\n    np.add.at(dr_o, iu[0], hpos)\n    np.add.at(dr_o, iu[1], hpos)\n\n    # Combine all gradients\n    dx_total = P * (w_b * dx_b + w_o * dx_o)\n    dy_total = P * (w_b * dy_b + w_o * dy_o)\n    dr_total = -1.0 + P * (w_b * dr_b + w_o * dr_o) # -1.0 from -sum(r)\n\n    grad = np.concatenate([dx_total, dy_total, dr_total])\n    return f, grad\n\ndef _objective_centers_only(z_xy, r_fixed, P, w_b=1.0, w_o=1.0):\n    \"\"\"\n    Objective function and its gradient for optimizing centers (x,y) only, with fixed radii.\n    z_xy = [x_0..x_N-1, y_0..y_N-1]\n    Objective: P * (w_b * boundary_penalty + w_o * overlap_penalty)\n    \"\"\"\n    N = r_fixed.shape[0]\n    x = z_xy[:N]\n    y = z_xy[N:2*N]\n    r = r_fixed\n    tiny = 1e-12\n\n    # Boundary hinges\n    s1 = r - x\n    s2 = r - y\n    s3 = r + x - 1.0\n    s4 = r + y - 1.0\n    h1 = np.maximum(0.0, s1)\n    h2 = np.maximum(0.0, s2)\n    h3 = np.maximum(0.0, s3)\n    h4 = np.maximum(0.0, s4)\n    pen_b = 0.5 * np.sum(h1*h1 + h2*h2 + h3*h3 + h4*h4)\n\n    # Overlap hinges\n    centers = np.stack([x, y], axis=1)\n    d, dx_mat, dy_mat = _pairwise_distances(centers)\n    iu = np.triu_indices(N, 1)\n    dab = d[iu]\n    dxab = dx_mat[iu]\n    dyab = dy_mat[iu]\n    d_safe = np.maximum(dab, tiny)\n    hij = r[iu[0]] + r[iu[1]] - dab\n    hpos = np.maximum(0.0, hij)\n    pen_o = 0.5 * np.sum(hpos*hpos)\n\n    f = P * (w_b * pen_b + w_o * pen_o)\n\n    # Gradients wrt x,y only\n    dx_b = -h1 + h3\n    dy_b = -h2 + h4\n\n    dx_o = np.zeros(N, dtype=float)\n    dy_o = np.zeros(N, dtype=float)\n    coeff = hpos / d_safe\n    cx = coeff * dxab\n    cy = coeff * dyab\n    np.add.at(dx_o, iu[0], -cx)\n    np.add.at(dx_o, iu[1], +cx)\n    np.add.at(dy_o, iu[0], -cy)\n    np.add.at(dy_o, iu[1], +cy)\n\n    dx_total = P * (w_b * dx_b + w_o * dx_o)\n    dy_total = P * (w_b * dy_b + w_o * dy_o)\n    g = np.concatenate([dx_total, dy_total])\n    return f, g\n\ndef _lp_feasibilize(centers, r_max=None, eps_pair=1e-6):\n    \"\"\"\n    Maximizes sum(r) for given centers using linear programming, ensuring feasibility.\n    Constraints:\n      r_i >= 0\n      r_i <= r_max[i] (if provided)\n      r_i <= x_i - eps_pair, r_i <= y_i - eps_pair, r_i <= 1-x_i - eps_pair, r_i <= 1-y_i - eps_pair\n      r_i + r_j <= dist(c_i,c_j) - eps_pair\n    Includes a retry mechanism if the initial LP with eps_pair is infeasible.\n    \"\"\"\n    N = centers.shape[0]\n    x = centers[:, 0]\n    y = centers[:, 1]\n\n    A_rows = []\n    b_vals = []\n\n    # Boundary constraints: r_i <= c_k - eps_pair\n    for i in range(N):\n        e = np.zeros(N); e[i] = 1.0\n        A_rows.append(e.copy()); b_vals.append(x[i] - eps_pair)\n        A_rows.append(e.copy()); b_vals.append(y[i] - eps_pair)\n        A_rows.append(e.copy()); b_vals.append(1.0 - x[i] - eps_pair)\n        A_rows.append(e.copy()); b_vals.append(1.0 - y[i] - eps_pair)\n\n    # Pairwise overlap constraints: r_i + r_j <= dist(c_i,c_j) - eps_pair\n    d, _, _ = _pairwise_distances(centers)\n    for i in range(N):\n        for j in range(i+1, N):\n            e = np.zeros(N); e[i] = 1.0; e[j] = 1.0\n            A_rows.append(e); b_vals.append(d[i, j] - eps_pair)\n\n    # Optional upper bounds r_i <= r_max[i]\n    if r_max is not None:\n        for i in range(N):\n            e = np.zeros(N); e[i] = 1.0\n            A_rows.append(e); b_vals.append(r_max[i])\n\n    A_ub = np.array(A_rows)\n    b_ub = np.array(b_vals)\n\n    # Radii bounds: [0, 0.5] (max possible radius in unit square)\n    bounds = [(0.0, 0.5) for _ in range(N)]\n    c = -np.ones(N) # Objective for linprog is to minimize c.T @ x, so -sum(r) means maximize sum(r)\n\n    res = linprog(c, A_ub=A_ub, b_ub=b_ub, bounds=bounds, method='highs')\n    if res.success:\n        return np.clip(res.x, 0.0, 0.5)\n\n    # Fallback: If LP with eps_pair is infeasible, try with eps_pair = 0 to get a base solution,\n    # then scale radii down to satisfy the eps_pair.\n    b_vals2 = []\n    A_rows2 = []\n    for i in range(N):\n        e = np.zeros(N); e[i] = 1.0\n        A_rows2.append(e.copy()); b_vals2.append(x[i])\n        A_rows2.append(e.copy()); b_vals2.append(y[i])\n        A_rows2.append(e.copy()); b_vals2.append(1.0 - x[i])\n        A_rows2.append(e.copy()); b_vals2.append(1.0 - y[i])\n\n    for i in range(N):\n        for j in range(i+1, N):\n            e = np.zeros(N); e[i] = 1.0; e[j] = 1.0\n            A_rows2.append(e); b_vals2.append(d[i, j])\n\n    if r_max is not None:\n        for i in range(N):\n            e = np.zeros(N); e[i] = 1.0\n            A_rows2.append(e); b_vals2.append(r_max[i])\n\n    A_ub2 = np.array(A_rows2)\n    b_ub2 = np.array(b_vals2)\n\n    res2 = linprog(c, A_ub=A_ub2, b_ub=b_ub2, bounds=bounds, method='highs')\n    if not res2.success:\n        return np.zeros(N) # If even without epsilon, no solution, return zero radii\n\n    r_base = np.clip(res2.x, 0.0, 0.5)\n    \n    # Scale r_base to satisfy eps_pair for both pairwise and boundary constraints\n    d, _, _ = _pairwise_distances(centers)\n    \n    # Scaling factor for pairwise overlaps: s * (r_i + r_j) <= d_ij - eps_pair\n    denom_pairs = r_base[:, None] + r_base[None, :]\n    with np.errstate(divide='ignore', invalid='ignore'):\n        ratio_pairs = np.where(denom_pairs > 1e-9, (d - eps_pair) / denom_pairs, np.inf)\n    iu = np.triu_indices(N, 1)\n    s_pairs = np.min(ratio_pairs[iu]) if ratio_pairs[iu].size > 0 else 1.0\n    if not np.isfinite(s_pairs): s_pairs = 1.0\n    s_pairs = max(0.0, min(1.0, s_pairs))\n\n    # Scaling factor for boundary constraints: s * r_i <= c_k - eps_pair\n    with np.errstate(divide='ignore', invalid='ignore'):\n        s_boundary_x_min = np.where(r_base > 1e-9, (x - eps_pair) / r_base, np.inf)\n        s_boundary_y_min = np.where(r_base > 1e-9, (y - eps_pair) / r_base, np.inf)\n        s_boundary_x_max = np.where(r_base > 1e-9, (1.0 - x - eps_pair) / r_base, np.inf)\n        s_boundary_y_max = np.where(r_base > 1e-9, (1.0 - y - eps_pair) / r_base, np.inf)\n\n    s_boundary = np.min([s_boundary_x_min, s_boundary_y_min, s_boundary_x_max, s_boundary_y_max])\n    if not np.isfinite(s_boundary): s_boundary = 1.0\n    s_boundary = max(0.0, min(1.0, s_boundary))\n\n    s = min(s_pairs, s_boundary)\n    r = s * r_base\n    return np.clip(r, 0.0, 0.5)\n\ndef _verify(centers, radii, tol_pair=1e-6):\n    \"\"\"\n    Verifies if a given packing is strictly feasible within the unit square\n    and without overlaps, considering a tolerance.\n    \"\"\"\n    N = len(radii)\n    x = centers[:, 0]\n    y = centers[:, 1]\n    \n    if centers.shape != (N, 2): return False\n    if np.any(radii < -1e-12): return False # Radii must be non-negative\n\n    # Check boundary constraints: r_i <= c_k - tol_pair\n    if np.any(x - radii < tol_pair): return False # x_i - r_i >= tol_pair\n    if np.any(y - radii < tol_pair): return False # y_i - r_i >= tol_pair\n    if np.any(x + radii > 1.0 - tol_pair): return False # x_i + r_i <= 1 - tol_pair\n    if np.any(y + radii > 1.0 - tol_pair): return False # y_i + r_i <= 1 - tol_pair\n    \n    # Check pairwise overlap constraints: r_i + r_j <= dist(c_i,c_j) - tol_pair\n    d, _, _ = _pairwise_distances(centers)\n    iu = np.triu_indices(N, 1)\n    lhs = radii[:, None] + radii[None, :]\n    viol = lhs[iu] - d[iu] + tol_pair # Should be <= 0 for no overlap\n    if np.any(viol > 1e-12): return False # Allow a tiny buffer for float precision\n    \n    return True\n\ndef _hex_initial_centers_from_rows(rows, margin=0.05, jitter_scale=0.03, seed=0):\n    \"\"\"\n    Generates initial centers in a hexagonal-like grid pattern.\n    Rows is a list of circle counts per row, e.g., [6,5,6,5,4].\n    \"\"\"\n    assert sum(rows) == 26\n    max_cols = max(rows)\n    nrows = len(rows)\n    \n    # Calculate base spacing based on max columns and rows\n    dx_base = (1.0 - 2*margin) / (max_cols - 1) if max_cols > 1 else 0.0\n    dy_base = (1.0 - 2*margin) / (nrows - 1) if nrows > 1 else 0.0\n\n    centers = []\n    rng = np.random.default_rng(seed)\n    for ri, cols in enumerate(rows):\n        y = margin + ri * dy_base if nrows > 1 else 0.5\n        \n        # Calculate starting x to center the row, applying hex offset for alternating rows\n        if cols > 1:\n            row_width = (cols - 1) * dx_base\n            startx = 0.5 - row_width / 2.0\n            if ri % 2 == 1: # Apply hex offset\n                startx += dx_base / 2.0\n        else: # Single circle in row, center it\n            startx = 0.5\n\n        for k in range(cols):\n            x = startx + (k * dx_base if cols > 1 else 0.0)\n            centers.append([x, y])\n    centers = np.array(centers)\n    \n    # Add jitter and clip to ensure centers are within bounds\n    jitter = rng.normal(scale=jitter_scale, size=centers.shape)\n    centers = centers + jitter\n    centers = np.clip(centers, 1e-3, 1 - 1e-3) # Keep centers slightly away from boundary\n    return centers\n\ndef _multi_start_inits(N):\n    \"\"\"\n    Generates a list of diverse initial center configurations using various\n    hexagonal patterns, margins, and jitter scales.\n    \"\"\"\n    # A selected subset of diverse patterns and parameters for N=26\n    patterns = [\n        [6, 5, 6, 5, 4], # 5 rows\n        [6, 5, 5, 5, 5],\n        [5, 6, 5, 5, 5],\n        [7, 6, 6, 7],   # 4 rows\n        [6, 7, 6, 7],\n        [5, 5, 5, 5, 6], # 5 rows, different distribution\n        [6, 6, 6, 4, 4],\n    ]\n    params = [\n        (0.03, 0.02), (0.05, 0.03), (0.07, 0.03), (0.045, 0.02)\n    ]\n    inits = []\n    seed = 0\n    for pat in patterns:\n        for (m, j) in params:\n            inits.append(_hex_initial_centers_from_rows(pat, margin=m, jitter_scale=j, seed=seed))\n            seed += 1\n    return inits\n\ndef _center_only_relax(centers, r_fixed, P=2000.0, w_b=1.0, w_o=1.0, iters=80):\n    \"\"\"\n    Performs a short optimization run on centers only, with fixed radii,\n    to relax the configuration and potentially open up space.\n    \"\"\"\n    N = centers.shape[0]\n    z0 = np.concatenate([centers[:, 0], centers[:, 1]])\n    bounds_xy = [(0.0, 1.0)] * (2 * N) # Centers must stay within [0,1]\n    res = minimize(\n        fun=lambda zz: _objective_centers_only(zz, r_fixed, P, w_b, w_o),\n        x0=z0,\n        method='L-BFGS-B',\n        jac=True,\n        bounds=bounds_xy,\n        options={'maxiter': iters, 'ftol': 1e-12, 'gtol': 1e-8, 'maxcor': 20}\n    )\n    z = res.x\n    centers_new = np.stack([z[:N], z[N:2*N]], axis=1)\n    return centers_new\n\ndef construct_packing():\n    N = 26\n    best = {'centers': None, 'radii': None, 'sum': -np.inf}\n\n    # Penalty annealing schedule and max iterations per stage\n    P_schedule = [10.0, 100.0, 1000.0, 5000.0, 20000.0, 50000.0] # Increasing penalty\n    maxiter_stage = [70, 80, 90, 100, 110, 120] # Iterations per stage\n    \n    # Bounds for x, y, r. Radii must be non-negative and max 0.5.\n    bounds = [(0.0, 1.0)] * (2*N) + [(0.0, 0.5)] * N \n\n    w_b = 1.0 # Weight for boundary penalty\n    w_o = 1.0 # Weight for overlap penalty\n\n    selected_inits = _multi_start_inits(N)\n    \n    # Fallback if selected_inits is empty (shouldn't happen with the above logic)\n    if not selected_inits:\n        selected_inits = [_hex_initial_centers_from_rows([6,5,6,5,4], margin=0.05, jitter_scale=0.03, seed=42)]\n\n\n    for init_idx, centers0 in enumerate(selected_inits):\n        x0 = centers0[:, 0]\n        y0 = centers0[:, 1]\n        # Initialize radii to a small value, respecting boundary constraints\n        base_bound = np.minimum.reduce([x0, y0, 1 - x0, 1 - y0])\n        r0 = 0.05 * base_bound # Start with small radii\n        z = np.concatenate([x0, y0, r0])\n\n        # Annealing optimization loop\n        for si, P in enumerate(P_schedule):\n            # Stage 1: Joint optimization of (x, y, r) using penalty method\n            res = minimize(\n                fun=lambda zz: _objective_and_grad(zz, N, P, w_b, w_o),\n                x0=z,\n                method='L-BFGS-B',\n                jac=True,\n                bounds=bounds,\n                options={'maxiter': maxiter_stage[si], 'ftol': 1e-12, 'gtol': 1e-8, 'maxcor': 30}\n            )\n            z = res.x\n            centers_tmp = np.stack([z[:N], z[N:2*N]], axis=1)\n            r_cur = np.clip(z[2*N:], 0.0, 0.5)\n            \n            # Stage 2: LP Feasibilization with a radius cap\n            # The cap prevents radii from growing too aggressively too early, allowing centers to adjust.\n            cap_factor = 1.05 if si < len(P_schedule) - 1 else 1.10 # Slightly smaller cap factor\n            r_cap = np.minimum(0.5, cap_factor * np.maximum(1e-9, r_cur))\n            r_lp = _lp_feasibilize(centers_tmp, r_max=r_cap, eps_pair=1e-6)\n            \n            # Stage 3: Center-only relaxation\n            # Move centers slightly with fixed (LP-derived) radii to open up space.\n            centers_relaxed = _center_only_relax(centers_tmp, r_lp, P=max(1000.0, P), w_b=w_b, w_o=w_o, iters=70)\n            \n            # Stage 4: Final LP for this stage (no cap)\n            # Maximize radii again for the relaxed centers.\n            r_lp2 = _lp_feasibilize(centers_relaxed, r_max=None, eps_pair=1e-6)\n            \n            # Update the state (z) for the next annealing stage\n            z = np.concatenate([centers_relaxed[:, 0], centers_relaxed[:, 1], np.clip(r_lp2, 0.0, 0.5)])\n\n        # After all annealing stages, perform a final LP to get the best possible radii\n        centers_final_opt = np.stack([z[:N], z[N:2*N]], axis=1)\n        r_final = _lp_feasibilize(centers_final_opt, r_max=None, eps_pair=1e-6)\n\n        # Verify the solution and update the best found packing\n        if _verify(centers_final_opt, r_final, tol_pair=1e-6):\n            s = float(np.sum(r_final))\n            if s > best['sum']:\n                best['centers'] = centers_final_opt.copy()\n                best['radii'] = r_final.copy()\n                best['sum'] = s\n        else:\n            # If the solution is not strictly feasible, try to scale down radii\n            # to achieve feasibility. This is a \"hardening\" step.\n            d, _, _ = _pairwise_distances(centers_final_opt)\n            \n            # Calculate scaling factor for pairwise overlaps\n            denom_pairs = r_final[:, None] + r_final[None, :]\n            with np.errstate(divide='ignore', invalid='ignore'):\n                ratio_pairs = np.where(denom_pairs > 1e-9, (d - 1.0e-6) / denom_pairs, np.inf)\n            iu = np.triu_indices(N, 1)\n            s_pairs = np.min(ratio_pairs[iu]) if ratio_pairs[iu].size > 0 else 1.0\n            if not np.isfinite(s_pairs): s_pairs = 1.0\n            s_pairs = max(0.0, min(1.0, s_pairs))\n\n            # Calculate scaling factor for boundary constraints\n            bx = np.minimum.reduce([centers_final_opt[:,0], centers_final_opt[:,1], 1-centers_final_opt[:,0], 1-centers_final_opt[:,1]])\n            with np.errstate(divide='ignore', invalid='ignore'):\n                scale_b = np.min(np.where(r_final > 1e-9, (bx - 1e-6) / r_final, np.inf))\n            scale_b = min(1.0, scale_b if np.isfinite(scale_b) else 1.0)\n            scale_b = max(0.0, scale_b)\n\n            r_try = min(s_pairs, scale_b) * r_final\n            \n            if _verify(centers_final_opt, r_try, tol_pair=1e-6):\n                s = float(np.sum(r_try))\n                if s > best['sum']:\n                    best['centers'] = centers_final_opt.copy()\n                    best['radii'] = r_try.copy()\n                    best['sum'] = s\n\n    # Fallback mechanism: if no valid solution was found after all multi-starts,\n    # generate a simple hexagonal pattern and run LP on it.\n    if best['centers'] is None:\n        centers_fb = _hex_initial_centers_from_rows([6,5,6,5,4], margin=0.06, jitter_scale=0.02, seed=123)\n        r_fb = _lp_feasibilize(centers_fb, r_max=None, eps_pair=1e-6)\n        # If still not feasible, shrink a bit more\n        if not _verify(centers_fb, r_fb, tol_pair=1e-6):\n            r_fb = 0.95 * r_fb \n        best['centers'] = centers_fb\n        best['radii'] = np.clip(r_fb, 0.0, 0.5)\n        best['sum'] = float(np.sum(best['radii']))\n\n    # Final hardening step to guarantee strict feasibility for the returned solution.\n    # Re-run LP with a slightly stricter epsilon.\n    radii_strict = _lp_feasibilize(best['centers'], r_max=None, eps_pair=1.1e-6)\n    if _verify(best['centers'], radii_strict, tol_pair=1e-6):\n        radii = radii_strict\n    else:\n        # If stricter LP fails, use the best found radii and apply a final scaling if needed.\n        radii = best['radii']\n        if not _verify(best['centers'], radii, tol_pair=1e-6):\n            d, _, _ = _pairwise_distances(best['centers'])\n            \n            denom = radii[:, None] + radii[None, :]\n            with np.errstate(divide='ignore', invalid='ignore'):\n                s_pairs = np.min(np.where(denom > 1e-9, (d - 1e-6) / denom, np.inf)[np.triu_indices(N, 1)])\n            s_pairs = min(1.0, s_pairs if np.isfinite(s_pairs) else 1.0)\n            s_pairs = max(0.0, s_pairs)\n            \n            bx = np.minimum.reduce([best['centers'][:,0], best['centers'][:,1], 1-best['centers'][:,0], 1-best['centers'][:,1]])\n            with np.errstate(divide='ignore', invalid='ignore'):\n                scale_b = np.min(np.where(radii > 1e-9, (bx - 1e-6) / radii, np.inf))\n            scale_b = min(1.0, scale_b if np.isfinite(scale_b) else 1.0)\n            scale_b = max(0.0, scale_b)\n            \n            radii = min(s_pairs, scale_b) * radii\n\n    centers = best['centers']\n    radii = np.clip(radii, 0.0, 0.5) # Final clip to ensure radii are non-negative and within max possible\n    sum_radii = float(np.sum(radii))\n    return centers.astype(float), radii.astype(float), sum_radii\n\nRequirements:\n\nYou must write Python code that defines:\n- construct_packing() -> (centers, radii, sum_radii)\nOptionally: run_packing() to call construct_packing()\n\nConstraints:\n- Exactly 26 circles\n- centers: np.array shape (26,2) in [0,1]^2\n- radii: np.array shape (26,) with r >= 0\n- Non-overlap: for all i<j, ||c_i - c_j|| >= r_i + r_j with tolerance 1e-6\n- Strict containment: r_i <= min(x_i, y_i, 1 - x_i, 1 - y_i)\n- Maximize sum(r)\n\nRequired approach:\n- Optimize a smooth penalized surrogate with analytic, fully vectorized gradients. Apply epsilon only in denominators (no biasing the penalty).\n- Finalize by solving a linear program (SciPy linprog, method='highs') to maximize sum(r) subject to:\n  r_i >= 0, r_i <= x_i, r_i <= y_i, r_i <= 1-x_i, r_i <= 1-y_i,\n  and r_i + r_j <= dist_ij - 1e-6 for all i<j.\n- Do not cap LP radii by the current candidate radii (no upper bounds r_i <= r_i^cand).\n- Return the strictly feasible LP result with correct shapes.\n\nImplementation details:\n- Use NumPy and SciPy only. No prints. Keep runtime under ~300s.\n- Multi-start is encouraged; use diverse seeds (triangular/edge-biased/hex-like) + small jitter.\n- Gradients:\n  * Overlap penalty: hinge h(d_ij - (r_i + r_j)) on violations via smooth square (e.g., ReLU or softplus with small tau).\n  * Boundary penalty: hinge on r_i - min_bdry_i.\n  * Stabilize only in division: use d_ij + eps in denominators, but not inside the hinge value.\n- After optimization, always run the LP to get final radii and report the sum.\n- Ensure strict feasibility: subtract 1e-6 in pair constraints in LP.\n- Vectorize pairwise computations; avoid Python loops for O(n^2) terms.\n- Seed RNG deterministically for reproducibility.\n\nReturn format:\n- centers: np.ndarray (26, 2), radii: np.ndarray (26,), sum_radii: float\n\n\nDesign:\n- Variables: z = [x(26), y(26), r(26)], bounds: x,y in [0,1], r in [0,1].\n- Objective: -sum(r) + mu * (pair_overlap_penalty + boundary_penalty).\n- Pair overlap penalty: for i<j, v_ij = max(0, r_i + r_j - d_ij); penalty = sum(w_ij * v_ij^2).\n  * d_ij = ||c_i - c_j||, compute with vectorized pairwise operations.\n  * w_ij adaptive: inverse local density proxy via kNN distances (k=8..12): w_ij ~ 1 / (epsilon + avg_kNNdist(c_i, c_j)).\n- Boundary penalty: for each i, b_i = r_i - min(x_i, y_i, 1-x_i, 1-y_i); penalty += sum(max(0, b_i)^2).\n- Eps only in denominators: use (d_ij + 1e-12) in gradient divisions but not in v_ij or b_i.\n- Optimization:\n  * Anneal mu over restarts: [3e2, 3e3, 3e4], each with up to ~200 L-BFGS-B iterations.\n  * Multi-start: 20 seeds mixing triangular/hex-like and edge/corner-biased layouts with small jitter.\n  * Keep best penalized objective per mu stage; pass its solution to next mu stage.\n- Finalization: Run LP (no caps by candidate radii) and return strictly feasible solution.\n\nImplement construct_packing() with the above. Ensure gradients are fully vectorized and correct.\n",
  "response": "```python\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.optimize import linprog\n\n# ---------------------------- Utilities ---------------------------- #\n\ndef _pairwise_distances(centers):\n    # centers: (N,2)\n    diff = centers[:, None, :] - centers[None, :, :]\n    d = np.sqrt(np.sum(diff * diff, axis=2))\n    return d, diff[..., 0], diff[..., 1]\n\ndef _adaptive_pair_weights(centers, k=8, alpha=1.0, eps=1e-12, clip=(0.3, 3.0)):\n    \"\"\"\n    Compute adaptive weights w_ij for pairs (i<j) based on local crowding.\n    For each point, compute mean distance to its k nearest neighbors (excluding itself).\n    Weight for pair (i,j):\n        w_ij = ( 1 / (eps + 0.5*(mean_kNN_i + mean_kNN_j)) )**alpha\n    Then normalize to mean 1 and clip to [clip[0], clip[1]] to stabilize.\n    Returns:\n        w_ij for upper-triangular index order (np.triu_indices(N,1))\n    \"\"\"\n    N = centers.shape[0]\n    d, _, _ = _pairwise_distances(centers)\n    # Exclude self-distances by setting to inf\n    with np.errstate(invalid='ignore'):\n        d2 = d.copy()\n        np.fill_diagonal(d2, np.inf)\n    k_eff = max(1, min(k, max(1, N - 1)))\n    # partial sort to k smallest\n    # Use np.partition for efficiency\n    kth = np.partition(d2, kth=k_eff-1, axis=1)[:, :k_eff]  # (N,k_eff)\n    mean_knn = np.mean(kth, axis=1)  # (N,)\n    iu = np.triu_indices(N, 1)\n    avg_local = 0.5 * (mean_knn[iu[0]] + mean_knn[iu[1]])\n    w = (1.0 / (eps + avg_local)) ** alpha\n    # normalize to mean 1\n    m = np.mean(w)\n    if m > 0:\n        w = w / m\n    # clip\n    if clip is not None:\n        w = np.clip(w, clip[0], clip[1])\n    return w\n\n# -------------------- Penalized objective with grads -------------------- #\n\ndef _objective_and_grad(z, N, mu, w_b=1.0, w_o=1.0, k_neigh=8, alpha_w=1.0):\n    \"\"\"\n    z = [x_0..x_{N-1}, y_0..y_{N-1}, r_0..r_{N-1}]\n    Objective:\n      f = -sum(r) + mu * ( w_b * boundary_penalty + w_o * overlap_penalty )\n    Boundary penalty uses squared ReLU on each of the four hinges per circle:\n      h1 = max(0, r - x), h2 = max(0, r - y), h3 = max(0, r + x - 1), h4 = max(0, r + y - 1)\n      pen_b = 0.5 * sum_i (h1^2 + h2^2 + h3^2 + h4^2)\n    Overlap penalty uses squared ReLU on violations with adaptive pair weights:\n      v_ij = max(0, r_i + r_j - d_ij)\n      pen_o = 0.5 * sum_{i<j} w_ij * v_ij^2\n    Notes:\n      - We recompute w_ij from current centers but treat it as constant for gradient (common in reweighting).\n      - Stabilize only in denominators with a tiny epsilon; no bias in hinges themselves.\n    Returns (f, grad)\n    \"\"\"\n    x = z[:N]\n    y = z[N:2*N]\n    r = z[2*N:]\n\n    # Boundary penalty\n    s1 = r - x\n    s2 = r - y\n    s3 = r + x - 1.0\n    s4 = r + y - 1.0\n\n    h1 = np.maximum(0.0, s1)\n    h2 = np.maximum(0.0, s2)\n    h3 = np.maximum(0.0, s3)\n    h4 = np.maximum(0.0, s4)\n\n    pen_b = 0.5 * np.sum(h1*h1 + h2*h2 + h3*h3 + h4*h4)\n\n    # Centers and pairwise distances\n    centers = np.stack([x, y], axis=1)\n    d, dx_mat, dy_mat = _pairwise_distances(centers)\n    iu = np.triu_indices(N, 1)\n    dab = d[iu]\n    dxab = dx_mat[iu]\n    dyab = dy_mat[iu]\n\n    # Adaptive weights for pairs\n    w_pairs = _adaptive_pair_weights(centers, k=k_neigh, alpha=alpha_w, eps=1e-12, clip=(0.4, 2.5))\n\n    # Overlap violations\n    hij = r[iu[0]] + r[iu[1]] - dab\n    hpos = np.maximum(0.0, hij)\n\n    pen_o = 0.5 * np.sum(w_pairs * hpos * hpos)\n\n    # Total objective\n    f = -np.sum(r) + mu * (w_b * pen_b + w_o * pen_o)\n\n    # Gradients\n    # Boundary gradients\n    dr_b = h1 + h2 + h3 + h4\n    dx_b = -h1 + h3\n    dy_b = -h2 + h4\n\n    # Overlap gradients\n    tiny = 1e-12\n    d_safe = np.maximum(dab, tiny)\n    coeff = (w_pairs * hpos) / d_safe  # shape (M,), M = N*(N-1)/2\n\n    dx_o = np.zeros(N, dtype=float)\n    dy_o = np.zeros(N, dtype=float)\n    dr_o = np.zeros(N, dtype=float)\n\n    cx = coeff * dxab\n    cy = coeff * dyab\n\n    # center grads: for pair (a,b): grad_x_a += -cx, grad_x_b += +cx\n    np.add.at(dx_o, iu[0], -cx)\n    np.add.at(dx_o, iu[1], +cx)\n    np.add.at(dy_o, iu[0], -cy)\n    np.add.at(dy_o, iu[1], +cy)\n\n    # radii grads: grad_r_a += w*hpos, grad_r_b += w*hpos\n    np.add.at(dr_o, iu[0], w_pairs * hpos)\n    np.add.at(dr_o, iu[1], w_pairs * hpos)\n\n    # Combine\n    gx = mu * (w_b * dx_b + w_o * dx_o)\n    gy = mu * (w_b * dy_b + w_o * dy_o)\n    gr = -1.0 + mu * (w_b * dr_b + w_o * dr_o)\n\n    g = np.concatenate([gx, gy, gr])\n    return f, g\n\n# ------------------------ Linear Programming ------------------------ #\n\ndef _lp_maximize_radii(centers, eps_pair=1e-6):\n    \"\"\"\n    Solve LP: maximize sum r_i\n    subject to:\n      0 <= r_i <= 0.5\n      r_i <= x_i - eps, r_i <= y_i - eps, r_i <= 1-x_i - eps, r_i <= 1-y_i - eps\n      r_i + r_j <= ||c_i - c_j|| - eps for all i<j\n    Uses SciPy linprog with 'highs'.\n    Returns radii (N,) or zeros if LP fails (shouldn't).\n    \"\"\"\n    N = centers.shape[0]\n    x = centers[:, 0]\n    y = centers[:, 1]\n\n    A_rows = []\n    b_vals = []\n\n    # Boundary constraints\n    for i in range(N):\n        e = np.zeros(N); e[i] = 1.0\n        A_rows.append(e.copy()); b_vals.append(x[i] - eps_pair)\n        A_rows.append(e.copy()); b_vals.append(y[i] - eps_pair)\n        A_rows.append(e.copy()); b_vals.append(1.0 - x[i] - eps_pair)\n        A_rows.append(e.copy()); b_vals.append(1.0 - y[i] - eps_pair)\n\n    # Pairwise constraints\n    d, _, _ = _pairwise_distances(centers)\n    for i in range(N):\n        for j in range(i+1, N):\n            e = np.zeros(N); e[i] = 1.0; e[j] = 1.0\n            A_rows.append(e); b_vals.append(d[i, j] - eps_pair)\n\n    A_ub = np.array(A_rows, dtype=float)\n    b_ub = np.array(b_vals, dtype=float)\n\n    bounds = [(0.0, 0.5) for _ in range(N)]\n    c = -np.ones(N, dtype=float)  # maximize sum r -> minimize -sum r\n\n    res = linprog(c, A_ub=A_ub, b_ub=b_ub, bounds=bounds, method='highs')\n    if res.success:\n        r = np.clip(res.x, 0.0, 0.5)\n    else:\n        # Fallback (should rarely happen): return zeros\n        r = np.zeros(N, dtype=float)\n    return r\n\ndef _verify(centers, radii, tol_pair=1e-6):\n    \"\"\"\n    Strict feasibility check with tolerance tol_pair in pairwise constraints.\n    \"\"\"\n    N = centers.shape[0]\n    if centers.shape != (N, 2): return False\n    if radii.shape != (N,): return False\n    if np.any(radii < -1e-12): return False\n    x = centers[:, 0]; y = centers[:, 1]\n    # boundary\n    if np.any(x - radii < tol_pair): return False\n    if np.any(y - radii < tol_pair): return False\n    if np.any(x + radii > 1.0 - tol_pair): return False\n    if np.any(y + radii > 1.0 - tol_pair): return False\n    # pairs\n    d, _, _ = _pairwise_distances(centers)\n    iu = np.triu_indices(N, 1)\n    lhs = radii[:, None] + radii[None, :]\n    if np.any(lhs[iu] > d[iu] - tol_pair + 1e-12): return False\n    return True\n\n# ------------------------ Initializations ------------------------ #\n\ndef _hex_initial_centers_from_rows(rows, margin=0.05, jitter_scale=0.02, seed=0):\n    \"\"\"\n    Build centers using a hex-like row pattern with optional jitter and margin.\n    rows: list of ints summing to 26.\n    \"\"\"\n    assert sum(rows) == 26\n    max_cols = max(rows)\n    nrows = len(rows)\n    if max_cols > 1:\n        dx = (1.0 - 2*margin) / (max_cols - 1)\n    else:\n        dx = 0.0\n    if nrows > 1:\n        dy = (1.0 - 2*margin) / (nrows - 1)\n    else:\n        dy = 0.0\n\n    c = []\n    for r_i, cols in enumerate(rows):\n        y = margin + r_i * dy if nrows > 1 else 0.5\n        if cols > 1:\n            width = (cols - 1) * dx\n            startx = 0.5 - 0.5 * width\n            if r_i % 2 == 1 and max_cols > 1:\n                startx += 0.5 * dx\n        else:\n            startx = 0.5\n        for k in range(cols):\n            x = startx + (k * dx if cols > 1 else 0.0)\n            c.append([x, y])\n    c = np.array(c, dtype=float)\n    rng = np.random.default_rng(seed)\n    c = c + rng.normal(scale=jitter_scale, size=c.shape)\n    c = np.clip(c, 1e-3, 1.0 - 1e-3)\n    return c\n\ndef _triangular_lattice_init(n, margin=0.04, jitter=0.02, seed=0):\n    \"\"\"\n    Approximate triangular-lattice fill of the square.\n    Returns exactly n points by cropping/striding.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    # grid dims\n    rows = int(np.sqrt(n)) + 3\n    cols = rows + 2\n    if cols < 3: cols = 3\n    dx = (1.0 - 2*margin) / (cols - 1)\n    dy = (1.0 - 2*margin) / (rows - 1)\n    pts = []\n    for i in range(rows):\n        y = margin + i * dy\n        off = 0.5 * dx if (i % 2 == 1) else 0.0\n        for j in range(cols):\n            x = margin + j * dx + off\n            if 0.0 <= x <= 1.0:\n                pts.append([x, y])\n    pts = np.array(pts, dtype=float)\n    # pick n points with a spread: take every stride\n    if pts.shape[0] > n:\n        idx = np.linspace(0, pts.shape[0]-1, n, dtype=int)\n        pts = pts[idx]\n    elif pts.shape[0] < n:\n        # oversample with random duplicates slightly jittered\n        extra = n - pts.shape[0]\n        add_idx = rng.integers(0, pts.shape[0], size=extra)\n        add = pts[add_idx] + rng.normal(scale=0.5*jitter, size=(extra,2))\n        pts = np.vstack([pts, add])\n    pts = pts + rng.normal(scale=jitter, size=pts.shape)\n    pts = np.clip(pts, 1e-3, 1.0 - 1e-3)\n    return pts\n\ndef _edge_biased_init(n, margin=0.02, jitter=0.02, seed=0):\n    \"\"\"\n    Place some points near edges/corners to encourage boundary hugging.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    m = margin\n    pts = []\n    # corners and edge midpoints bias\n    anchors = [\n        (m, m), (1-m, m), (m, 1-m), (1-m, 1-m),\n        (0.5, m), (0.5, 1-m), (m, 0.5), (1-m, 0.5)\n    ]\n    k = min(len(anchors), n)\n    pts.extend(anchors[:k])\n    # Fill rest with a hex-like background\n    need = n - len(pts)\n    if need > 0:\n        rows = [7, 6, 7, 6]  # sums to 26 if used alone\n        base = _hex_initial_centers_from_rows(rows, margin=0.06, jitter_scale=0.0, seed=seed+1)\n        idx = rng.choice(base.shape[0], size=need, replace=False)\n        pts.extend(base[idx].tolist())\n    pts = np.array(pts, dtype=float)\n    pts = pts + rng.normal(scale=jitter, size=pts.shape)\n    pts = np.clip(pts, 1e-3, 1.0 - 1e-3)\n    return pts\n\ndef _multi_start_inits(N=26):\n    inits = []\n    seeds = list(range(32))\n    patterns = [\n        [6, 5, 6, 5, 4],\n        [7, 6, 7, 6],\n        [6, 7, 6, 7],\n        [6, 6, 7, 7],\n        [5, 6, 5, 5, 5],\n        [6, 6, 6, 4, 4],\n    ]\n    params = [\n        (0.03, 0.02), (0.05, 0.02), (0.06, 0.03), (0.04, 0.025)\n    ]\n    cnt = 0\n    for p in patterns:\n        for (m, j) in params:\n            inits.append(_hex_initial_centers_from_rows(p, margin=m, jitter_scale=j, seed=seeds[cnt % len(seeds)]))\n            cnt += 1\n    # Add lattice and edge-biased variants\n    for s in range(6):\n        inits.append(_triangular_lattice_init(N, margin=0.045 + 0.005*s, jitter=0.02, seed=100+s))\n    for s in range(6):\n        inits.append(_edge_biased_init(N, margin=0.02 + 0.004*s, jitter=0.02, seed=200+s))\n    # Ensure exactly N points for each init and clip\n    ret = []\n    for c in inits:\n        if c.shape[0] != N:\n            # if mismatch (shouldn't), fix via stride or padding\n            if c.shape[0] > N:\n                idx = np.linspace(0, c.shape[0]-1, N, dtype=int)\n                c = c[idx]\n            else:\n                # pad by random picks with tiny jitter\n                rng = np.random.default_rng(999)\n                add_idx = rng.integers(0, c.shape[0], size=N - c.shape[0])\n                add = c[add_idx] + rng.normal(scale=1e-3, size=(N - c.shape[0], 2))\n                c = np.vstack([c, add])\n        ret.append(np.clip(c, 1e-3, 1.0 - 1e-3))\n    return ret\n\n# ------------------------ Main construction ------------------------ #\n\ndef construct_packing():\n    \"\"\"\n    Returns (centers, radii, sum_radii) for N=26 circles.\n    Strategy:\n      - Multi-start centers.\n      - Penalized L-BFGS-B on (x,y,r) with staged annealing on mu and adaptive neighbor weights.\n      - Final LP to maximize radii strictly feasibly.\n      - Elite refinement of top candidates.\n    \"\"\"\n    N = 26\n    # Bounds: x,y in [0,1]; r in [0,0.5]\n    bounds = [(0.0, 1.0)] * (2*N) + [(0.0, 0.5)] * N\n\n    # Annealing schedules\n    mu_schedule_primary = [300.0, 3000.0, 30000.0, 120000.0]\n    k_schedule_primary = [10, 8, 6, 6]\n    iters_primary = [80, 90, 100, 120]\n\n    mu_schedule_refine = [5000.0, 50000.0, 200000.0]\n    k_schedule_refine = [8, 6, 5]\n    iters_refine = [90, 110, 130]\n\n    w_b = 1.0\n    w_o = 1.0\n\n    inits = _multi_start_inits(N)\n    best_candidates = []  # list of dicts with 'centers','r','sum'\n\n    # Primary multi-start optimization\n    for idx, centers0 in enumerate(inits):\n        x0 = centers0[:, 0]\n        y0 = centers0[:, 1]\n        base_bound = np.minimum.reduce([x0, y0, 1 - x0, 1 - y0])\n        r0 = 0.04 * base_bound  # small initial radii\n\n        z = np.concatenate([x0, y0, r0])\n\n        for si, mu in enumerate(mu_schedule_primary):\n            k_neigh = k_schedule_primary[si]\n            max_it = iters_primary[si]\n            res = minimize(\n                fun=lambda zz, mu=mu, k_neigh=k_neigh: _objective_and_grad(\n                    zz, N, mu, w_b=w_b, w_o=w_o, k_neigh=k_neigh, alpha_w=1.0\n                ),\n                x0=z,\n                method='L-BFGS-B',\n                jac=True,\n                bounds=bounds,\n                options={'maxiter': max_it, 'ftol': 1e-12, 'gtol': 1e-8, 'maxcor': 30}\n            )\n            z = res.x  # continue from here\n\n        centers_opt = np.stack([z[:N], z[N:2*N]], axis=1)\n        # Finalize by LP\n        r_lp = _lp_maximize_radii(centers_opt, eps_pair=1e-6)\n        s_lp = float(np.sum(r_lp))\n        best_candidates.append({'centers': centers_opt.copy(), 'r': r_lp.copy(), 'sum': s_lp})\n\n    # Select elite candidates\n    best_candidates.sort(key=lambda d: d['sum'], reverse=True)\n    elites = best_candidates[:6] if len(best_candidates) >= 6 else best_candidates\n\n    # Elite refinement\n    refined = []\n    for cand in elites:\n        centers = cand['centers']\n        r = cand['r']\n        # Start from slightly shrunken radii to allow movement\n        r_start = 0.98 * r\n        z = np.concatenate([centers[:, 0], centers[:, 1], r_start])\n        for si, mu in enumerate(mu_schedule_refine):\n            k_neigh = k_schedule_refine[si]\n            max_it = iters_refine[si]\n            res = minimize(\n                fun=lambda zz, mu=mu, k_neigh=k_neigh: _objective_and_grad(\n                    zz, N, mu, w_b=w_b, w_o=w_o, k_neigh=k_neigh, alpha_w=1.0\n                ),\n                x0=z,\n                method='L-BFGS-B',\n                jac=True,\n                bounds=bounds,\n                options={'maxiter': max_it, 'ftol': 1e-12, 'gtol': 1e-8, 'maxcor': 30}\n            )\n            z = res.x\n\n        centers_ref = np.stack([z[:N], z[N:2*N]], axis=1)\n        r_ref = _lp_maximize_radii(centers_ref, eps_pair=1e-6)\n        s_ref = float(np.sum(r_ref))\n        refined.append({'centers': centers_ref.copy(), 'r': r_ref.copy(), 'sum': s_ref})\n\n    # Choose the best feasible solution\n    all_solutions = refined if refined else best_candidates\n    all_solutions.sort(key=lambda d: d['sum'], reverse=True)\n\n    best = None\n    for sol in all_solutions:\n        if _verify(sol['centers'], sol['r'], tol_pair=1e-6):\n            best = sol\n            break\n    if best is None:\n        # As a safe fallback, take the best LP radii (first one) and ensure feasibility by tiny scaling\n        sol = all_solutions[0]\n        centers = sol['centers']\n        r = sol['r']\n        # Hardening: rerun LP with slightly larger epsilon\n        r2 = _lp_maximize_radii(centers, eps_pair=1.1e-6)\n        if _verify(centers, r2, tol_pair=1e-6):\n            best = {'centers': centers, 'r': r2, 'sum': float(np.sum(r2))}\n        else:\n            # tiny global scale\n            d, _, _ = _pairwise_distances(centers)\n            lhs = r[:, None] + r[None, :]\n            iu = np.triu_indices(N, 1)\n            with np.errstate(divide='ignore', invalid='ignore'):\n                s_pairs = np.min((d[iu] - 1e-6) / np.maximum(lhs[iu], 1e-12))\n            s_pairs = float(np.clip(s_pairs, 0.0, 1.0))\n            bnd = np.minimum.reduce([centers[:, 0], centers[:, 1], 1-centers[:, 0], 1-centers[:, 1]])\n            with np.errstate(divide='ignore', invalid='ignore'):\n                s_b = np.min((bnd - 1e-6) / np.maximum(r, 1e-12))\n            s_b = float(np.clip(s_b if np.isfinite(s_b) else 1.0, 0.0, 1.0))\n            s = min(s_pairs, s_b)\n            r3 = np.clip(s * r, 0.0, 0.5)\n            best = {'centers': centers, 'r': r3, 'sum': float(np.sum(r3))}\n\n    centers = best['centers'].astype(float)\n    radii = np.clip(best['r'], 0.0, 0.5).astype(float)\n    # Final strict LP pass to maximize under strict constraints\n    radii_final = _lp_maximize_radii(centers, eps_pair=1.0e-6)\n    if _verify(centers, radii_final, tol_pair=1.0e-6):\n        radii = radii_final\n\n    sum_radii = float(np.sum(radii))\n    return centers, radii, sum_radii\n\ndef run_packing():\n    return construct_packing()\n```",
  "reasoning": "",
  "timestamp": "2026-02-11T03:37:18.231109",
  "cost_data": null,
  "model_config": {
    "model": "openai/gpt-5",
    "temperature": 0.3
  },
  "llm_call_duration_s": 162.81674204498995,
  "eval_duration_s": 5.161795661086217
}