{
  "trial_id": "trial_3_0",
  "generation": 3,
  "parent_id": "trial_2_0",
  "code": "import numpy as np\nfrom scipy.optimize import minimize, linprog\n\n# ============ Geometry utilities ============\n\ndef _pairwise_distances(centers):\n    diff = centers[:, None, :] - centers[None, :, :]\n    d = np.sqrt(np.sum(diff**2, axis=2))\n    return d, diff[..., 0], diff[..., 1]\n\n# ============ Smooth penalties (softplus with temperature) ============\n\ndef _softplus_scaled(x, tau):\n    # t*log(1+exp(x/t)) computed stably\n    u = x / tau\n    # softplus(u) = log1p(exp(-|u|)) + max(u,0)\n    return tau * (np.log1p(np.exp(-np.abs(u))) + np.maximum(u, 0.0))\n\ndef _sigmoid_scaled(x, tau):\n    # sigma(x/tau) computed stably\n    u = x / tau\n    # Stable sigmoid\n    out = np.empty_like(u)\n    pos = u >= 0\n    out[pos] = 1.0 / (1.0 + np.exp(-u[pos]))\n    expu = np.exp(u[~pos])\n    out[~pos] = expu / (1.0 + expu)\n    return out\n\n# ============ Objective and gradient (vectorized) ============\n\ndef _objective_and_grad_soft(z, N, lam_b, lam_o, tau):\n    # z = [x(0..N-1), y(0..N-1), r(0..N-1)]\n    x = z[:N]\n    y = z[N:2*N]\n    r = z[2*N:]\n    tiny = 1e-12\n\n    # Boundary soft hinges s = r - x, r - y, r + x - 1, r + y - 1\n    s1 = r - x\n    s2 = r - y\n    s3 = r + x - 1.0\n    s4 = r + y - 1.0\n\n    sp1 = _softplus_scaled(s1, tau)\n    sp2 = _softplus_scaled(s2, tau)\n    sp3 = _softplus_scaled(s3, tau)\n    sp4 = _softplus_scaled(s4, tau)\n\n    pen_b = lam_b * np.sum(sp1 + sp2 + sp3 + sp4)\n\n    g1 = lam_b * _sigmoid_scaled(s1, tau)\n    g2 = lam_b * _sigmoid_scaled(s2, tau)\n    g3 = lam_b * _sigmoid_scaled(s3, tau)\n    g4 = lam_b * _sigmoid_scaled(s4, tau)\n\n    # Overlap soft hinges over i<j pairs\n    centers = np.stack([x, y], axis=1)\n    d, dx_mat, dy_mat = _pairwise_distances(centers)\n    d_safe = d + tiny  # only for gradients; do not bias the penalty\n\n    hinge_full = r[:, None] + r[None, :] - d\n    iu = np.triu_indices(N, 1)\n    hinge_u = hinge_full[iu]\n\n    sp_o = _softplus_scaled(hinge_u, tau)\n    pen_o = lam_o * np.sum(sp_o)\n\n    sigma_u = _sigmoid_scaled(hinge_u, tau)  # shape (num_pairs,)\n    # Build an upper-triangular matrix S with sigma on (i<j), zeros elsewhere\n    S = np.zeros_like(d)\n    S[iu] = sigma_u\n\n    # Gradients for overlaps\n    # Qx = S * (dx_mat / d_safe), Qy = S * (dy_mat / d_safe)\n    Qx = S * (dx_mat / d_safe)\n    Qy = S * (dy_mat / d_safe)\n\n    # For variable i: -sum_j Q[i,j] + sum_k Q[k,i] (since ds/dx_i = -(x_i-x_j)/d)\n    dx_o = lam_o * (-np.sum(Qx, axis=1) + np.sum(Qx, axis=0))\n    dy_o = lam_o * (-np.sum(Qy, axis=1) + np.sum(Qy, axis=0))\n\n    # dr contribution: ds/dr_i = 1 for each pair containing i\n    # Sum of S over row i (i<j) plus over column i (k<i)\n    dr_o = lam_o * (np.sum(S, axis=1) + np.sum(S, axis=0))\n\n    # Boundary gradients accumulation\n    dx_b = -g1 + g3\n    dy_b = -g2 + g4\n    dr_b = g1 + g2 + g3 + g4\n\n    # Total gradients\n    dx_total = dx_b + dx_o\n    dy_total = dy_b + dy_o\n    dr_total = -1.0 + dr_b + dr_o  # objective has -sum(r)\n\n    grad = np.concatenate([dx_total, dy_total, dr_total])\n    f = -np.sum(r) + pen_b + pen_o\n    return f, grad\n\n# ============ Linear-program feasibilizer ============\n\ndef _lp_feasibilize(centers, r_cap=None, eps_pair=1e-6):\n    \"\"\"\n    Maximize sum(r) subject to:\n      0 <= r_i <= r_cap[i] (if provided, else <= 0.5)\n      r_i <= x_i, r_i <= y_i, r_i <= 1-x_i, r_i <= 1-y_i\n      r_i + r_j <= dist(c_i,c_j) - eps_pair\n    Implemented as minimizing -sum(r) with linprog (HiGHS).\n    Includes a fallback with eps_pair=0 and post-scaling if necessary.\n    \"\"\"\n    N = centers.shape[0]\n    x = centers[:, 0]\n    y = centers[:, 1]\n\n    # Boundary constraints (vectorized construction)\n    # For each bound r_i <= b: a row with 1 at i\n    A_rows = []\n    b_vals = []\n\n    eyeN = np.eye(N)\n    # r_i <= x_i\n    A_rows.append(eyeN.copy()); b_vals.append(x.copy())\n    # r_i <= y_i\n    A_rows.append(eyeN.copy()); b_vals.append(y.copy())\n    # r_i <= 1 - x_i\n    A_rows.append(eyeN.copy()); b_vals.append(1.0 - x)\n    # r_i <= 1 - y_i\n    A_rows.append(eyeN.copy()); b_vals.append(1.0 - y)\n\n    # Pairwise constraints\n    d, _, _ = _pairwise_distances(centers)\n    iu = np.triu_indices(N, 1)\n    num_pairs = iu[0].size\n    if num_pairs > 0:\n        A_pairs = np.zeros((num_pairs, N))\n        A_pairs[np.arange(num_pairs), iu[0]] = 1.0\n        A_pairs[np.arange(num_pairs), iu[1]] = 1.0\n        b_pairs = d[iu] - eps_pair\n        A_rows.append(A_pairs); b_vals.append(b_pairs)\n\n    # Optional caps r_i <= r_cap[i]\n    if r_cap is not None:\n        A_rows.append(eyeN.copy()); b_vals.append(np.asarray(r_cap).copy())\n\n    A_ub = np.vstack(A_rows) if len(A_rows) > 0 else None\n    b_ub = np.concatenate(b_vals) if len(b_vals) > 0 else None\n\n    # Bounds 0 <= r_i <= 0.5\n    bounds = [(0.0, 0.5) for _ in range(N)]\n    c = -np.ones(N)\n\n    res = linprog(c, A_ub=A_ub, b_ub=b_ub, bounds=bounds, method='highs')\n    if res.success:\n        r = res.x\n        return np.clip(r, 0.0, 0.5)\n\n    # Fallback: relax pairwise epsilon to 0, then scale to meet eps_pair\n    if num_pairs > 0:\n        b_pairs0 = d[iu]\n        b_ub2 = b_ub.copy()\n        # Replace the tail (pairwise block) of b_ub with b_pairs0\n        # Compute sizes\n        nb = 4*N\n        if r_cap is not None:\n            nb += N\n        # b_ub layout: [4N boundary][num_pairs pairs][N caps?] depending on append order above.\n        # Above, order: boundaries (4 blocks), pairs, caps (optional)\n        # So we replace the block starting at 4N with b_pairs0\n        start = 4*N\n        b_ub2[start:start+num_pairs] = b_pairs0\n        res2 = linprog(c, A_ub=A_ub, b_ub=b_ub2, bounds=bounds, method='highs')\n        if res2.success:\n            r_base = np.clip(res2.x, 0.0, 0.5)\n            # Scale radii to satisfy stricter eps_pair\n            denom = r_base[:, None] + r_base[None, :]\n            with np.errstate(divide='ignore', invalid='ignore'):\n                ratio = np.where(denom > 0, (d - eps_pair) / denom, np.inf)\n            s_pairs = np.min(ratio[iu]) if ratio[iu].size > 0 else 1.0\n            if not np.isfinite(s_pairs):\n                s_pairs = 1.0\n            s = float(np.clip(s_pairs, 0.0, 1.0))\n            r = s * r_base\n            # Boundary safety\n            bx = np.minimum.reduce([x, y, 1 - x, 1 - y])\n            with np.errstate(divide='ignore', invalid='ignore'):\n                scale_b = np.min(np.where(r > 0, bx / r, 1.0))\n            if not np.isfinite(scale_b):\n                scale_b = 1.0\n            r = np.clip(scale_b, 0.0, 1.0) * r\n            return np.clip(r, 0.0, 0.5)\n\n    # Give up with zeros (should not happen)\n    return np.zeros(N)\n\n# ============ Verification ============\n\ndef _verify(centers, radii, tol_pair=1e-6):\n    N = len(radii)\n    x = centers[:, 0]\n    y = centers[:, 1]\n    if centers.shape != (N, 2):\n        return False\n    if np.any(radii < -1e-12):\n        return False\n    if np.any(x - radii < -1e-9):\n        return False\n    if np.any(y - radii < -1e-9):\n        return False\n    if np.any(x + radii > 1.0 + 1e-9):\n        return False\n    if np.any(y + radii > 1.0 + 1e-9):\n        return False\n    d, _, _ = _pairwise_distances(centers)\n    iu = np.triu_indices(N, 1)\n    lhs = radii[:, None] + radii[None, :]\n    viol = lhs[iu] - d[iu] + tol_pair\n    if np.any(viol > 1e-12):\n        return False\n    return True\n\n# ============ Initializations ============\n\ndef _hex_initial_centers(N, margin=0.04, jitter_scale=0.03, seed=0):\n    # Fixed row pattern summing to 26\n    rows = [6, 5, 6, 5, 4]\n    assert sum(rows) == N\n    max_cols = max(rows)\n    nrows = len(rows)\n    dx = (1.0 - 2*margin) / (max_cols - 1)\n    dy = (1.0 - 2*margin) / (nrows - 1) if nrows > 1 else 0.0\n    centers = []\n    for ri, cols in enumerate(rows):\n        y = margin + ri * dy\n        if cols == max_cols:\n            startx = margin\n        else:\n            startx = margin + 0.5 * dx\n        for k in range(cols):\n            x = startx + k * dx\n            centers.append([x, y])\n    centers = np.array(centers)\n    rng = np.random.default_rng(seed)\n    jitter = rng.normal(scale=jitter_scale, size=centers.shape)\n    centers = np.clip(centers + jitter, 1e-3, 1 - 1e-3)\n    return centers\n\ndef _lhs_minsep(N, min_sep=0.14, jitter=0.015, seed=0, relax_iters=20, step=0.25):\n    \"\"\"\n    Latin-hypercube-like centers with a push-away heuristic to enforce min separation approximately.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    grid = (np.arange(N) + 0.5) / N\n    x = grid.copy(); rng.shuffle(x)\n    y = grid.copy(); rng.shuffle(y)\n    centers = np.stack([x, y], axis=1)\n    centers += rng.uniform(-jitter, jitter, size=centers.shape)\n    centers = np.clip(centers, 1e-3, 1 - 1e-3)\n\n    for _ in range(relax_iters):\n        d, dx, dy = _pairwise_distances(centers)\n        mask = (d > 0) & (d < min_sep)\n        if not np.any(mask):\n            break\n        # Repulsive displacements\n        with np.errstate(divide='ignore', invalid='ignore'):\n            rep = (min_sep - d) / (d + 1e-12)\n        rx = np.sum((dx * rep) * mask, axis=1)\n        ry = np.sum((dy * rep) * mask, axis=1)\n        disp = np.stack([rx, ry], axis=1)\n        centers = centers + step * disp / (np.linalg.norm(disp, axis=1, keepdims=True) + 1e-12) * (1.0 / N)\n        centers = np.clip(centers, 1e-3, 1 - 1e-3)\n    return centers\n\ndef _random_push(N, seed=0, min_sep=0.12, iters=60, step=0.15):\n    rng = np.random.default_rng(seed)\n    centers = rng.uniform(0.05, 0.95, size=(N, 2))\n    for _ in range(iters):\n        d, dx, dy = _pairwise_distances(centers)\n        mask = (d > 0) & (d < min_sep)\n        if not np.any(mask):\n            break\n        with np.errstate(divide='ignore', invalid='ignore'):\n            rep = (min_sep - d) / (d + 1e-12)\n        rx = np.sum((dx * rep) * mask, axis=1)\n        ry = np.sum((dy * rep) * mask, axis=1)\n        disp = np.stack([rx, ry], axis=1)\n        centers = centers + step * disp / (np.linalg.norm(disp, axis=1, keepdims=True) + 1e-12)\n        centers = np.clip(centers, 1e-3, 1 - 1e-3)\n    return centers\n\ndef _multi_start_inits(N):\n    inits = []\n    # Hexagonal-like grids with varying margins/jitters\n    params_hex = [\n        (0.03, 0.02, 0),\n        (0.05, 0.03, 1),\n        (0.07, 0.03, 2),\n        (0.04, 0.04, 3),\n        (0.06, 0.02, 4),\n        (0.05, 0.04, 5),\n        (0.03, 0.03, 6),\n        (0.07, 0.02, 7),\n    ]\n    for m, j, s in params_hex:\n        inits.append(_hex_initial_centers(N, margin=m, jitter_scale=j, seed=s))\n    # Latin-hypercube-like with min-sep variants\n    params_lhs = [\n        (0.13, 0.018, 10),\n        (0.15, 0.018, 11),\n        (0.17, 0.020, 12),\n        (0.14, 0.015, 13),\n        (0.16, 0.020, 14),\n        (0.18, 0.020, 15),\n    ]\n    for ms, jit, s in params_lhs:\n        inits.append(_lhs_minsep(N, min_sep=ms, jitter=jit, seed=s, relax_iters=25, step=0.3))\n    # Random with push-away\n    params_rand = [\n        (0.12, 20, 0.20, 21),\n        (0.14, 30, 0.20, 22),\n        (0.16, 40, 0.15, 23),\n    ]\n    for ms, it, st, s in params_rand:\n        inits.append(_random_push(N, seed=s, min_sep=ms, iters=it, step=st))\n    return inits\n\n# ============ Main construction ============\n\ndef construct_packing():\n    N = 26\n    best = {'centers': None, 'radii': None, 'sum': -np.inf}\n\n    # Annealing schedule: (lambda_bound, lambda_overlap, tau)\n    schedule = [\n        (25.0,   25.0,   0.05),\n        (90.0,  180.0,   0.03),\n        (350.0, 700.0,   0.02),\n        (1400.0, 2200.0, 0.015),\n        (6000.0, 9000.0, 0.010),\n    ]\n    maxiter_stage = 90\n    bounds = [(0.0, 1.0)] * (2*N) + [(0.0, 0.5)] * N\n\n    # Initializations\n    inits = _multi_start_inits(N)\n    # We'll add jittered inherits later if we discover a good layout\n    extra_inits = []\n\n    def _optimize_from_init(centers0, extra_boost=False):\n        nonlocal best\n        x0 = centers0[:, 0]\n        y0 = centers0[:, 1]\n        # Initial radii: small fraction of boundary clearance\n        clear = np.minimum.reduce([x0, y0, 1 - x0, 1 - y0])\n        r0 = 0.08 * clear\n        z = np.concatenate([x0, y0, r0])\n\n        # Annealing optimization with periodic LP tightening\n        for (lam_b, lam_o, tau) in schedule:\n            res = minimize(\n                fun=lambda zz: _objective_and_grad_soft(zz, N, lam_b, lam_o, tau),\n                x0=z,\n                method='L-BFGS-B',\n                jac=True,\n                bounds=bounds,\n                options={'maxiter': maxiter_stage if not extra_boost else int(1.2*maxiter_stage),\n                         'ftol': 1e-12, 'gtol': 1e-8, 'maxcor': 20}\n            )\n            z = res.x\n            centers_tmp = np.stack([z[:N], z[N:2*N]], axis=1)\n            r_cur = np.clip(z[2*N:], 0.0, 0.5)\n            # Conservative LP update, cap by slight expansion over current radii\n            r_cap = np.minimum(1.10 * r_cur, np.minimum.reduce([\n                centers_tmp[:, 0], centers_tmp[:, 1], 1 - centers_tmp[:, 0], 1 - centers_tmp[:, 1]\n            ]))\n            r_lp = _lp_feasibilize(centers_tmp, r_cap=r_cap, eps_pair=1e-6)\n            z[2*N:] = np.clip(r_lp, 0.0, 0.5)\n\n        # Final LP without cap to fully utilize geometry\n        centers_cur = np.stack([z[:N], z[N:2*N]], axis=1)\n        r_final = _lp_feasibilize(centers_cur, r_cap=None, eps_pair=1e-6)\n        # Verify and update best\n        if _verify(centers_cur, r_final, tol_pair=1e-6):\n            s = float(np.sum(r_final))\n            if s > best['sum'] + 1e-8:\n                best['centers'] = centers_cur.copy()\n                best['radii'] = r_final.copy()\n                best['sum'] = s\n                # Add jittered inherits to explore neighborhood\n                rng = np.random.default_rng(int(1e6 * s) % 2**32)\n                for sc in [0.010, 0.015, 0.020]:\n                    jit = rng.normal(scale=sc, size=centers_cur.shape)\n                    c_new = np.clip(centers_cur + jit, 1e-3, 1 - 1e-3)\n                    extra_inits.append(c_new)\n\n    # Run through base initializations\n    for centers0 in inits:\n        _optimize_from_init(centers0, extra_boost=False)\n\n    # Process a limited number of inherited jittered starts\n    # Limit count to keep runtime bounded\n    for i in range(min(6, len(extra_inits))):\n        _optimize_from_init(extra_inits[i], extra_boost=True)\n\n    # Fallback if nothing verified (unlikely)\n    if best['centers'] is None:\n        centers_fb = _hex_initial_centers(N, margin=0.06, jitter_scale=0.02, seed=42)\n        r_fb = _lp_feasibilize(centers_fb, r_cap=None, eps_pair=1e-6)\n        if not _verify(centers_fb, r_fb, tol_pair=1e-6):\n            # Try slight shrink and re-LP (robustify)\n            r_fb = 0.95 * r_fb\n            r_fb = _lp_feasibilize(centers_fb, r_cap=r_fb, eps_pair=1e-6)\n        best['centers'] = centers_fb\n        best['radii'] = r_fb\n        best['sum'] = float(np.sum(r_fb))\n\n    # Final strict LP hardening on best centers\n    centers = best['centers']\n    radii = _lp_feasibilize(centers, r_cap=None, eps_pair=1.0e-6)\n    # Ensure strict feasibility; if solver failed, scale conservatively\n    if not _verify(centers, radii, tol_pair=1e-6):\n        d, _, _ = _pairwise_distances(centers)\n        iu = np.triu_indices(N, 1)\n        denom = radii[:, None] + radii[None, :]\n        with np.errstate(divide='ignore', invalid='ignore'):\n            ratio = np.where(denom > 0, (d - 1.0e-6) / denom, np.inf)\n        s_pairs = np.min(ratio[iu]) if ratio[iu].size > 0 else 1.0\n        if not np.isfinite(s_pairs):\n            s_pairs = 1.0\n        s_pairs = float(np.clip(s_pairs, 0.0, 1.0))\n        radii = s_pairs * radii\n        bx = np.minimum.reduce([centers[:, 0], centers[:, 1], 1 - centers[:, 0], 1 - centers[:, 1]])\n        with np.errstate(divide='ignore', invalid='ignore'):\n            scale_b = np.min(np.where(radii > 0, bx / radii, 1.0))\n        if not np.isfinite(scale_b):\n            scale_b = 1.0\n        radii = np.clip(scale_b, 0.0, 1.0) * radii\n        radii = np.clip(radii, 0.0, 0.5)\n\n    sum_radii = float(np.sum(radii))\n    return centers.astype(float), radii.astype(float), sum_radii\n\ndef run_packing():\n    centers, radii, sum_radii = construct_packing()\n    return centers, radii, sum_radii",
  "metrics": {
    "valid": true,
    "score": 2.3602217729410295,
    "eval_time": 1.9483473300933838,
    "error": null
  },
  "prompt": "\nImprove this high-scoring approach by adding multi-start initialization diversity and stronger penalty annealing.\nKey ideas:\n- Optimize centers and provisional radii with L-BFGS-B on a smooth penalty objective:\n  objective = -sum(r) + lambda_overlap * sum(softplus(overlap_ij)) + lambda_bound * sum(softplus(bound_violation_i))\n- Use vectorized gradients. For distances d_ij, use stable grads with denom d_ij+eps (no bias in penalty).\n- Anneal penalties: start small, increase across 4-6 stages; keep best-so-far centers.\n- Seeds: combine (a) perturbed triangular grid, (b) Latin hypercube-like random centers with min-sep heuristic,\n  (c) inherit best centers if available, jittered.\n- After optimization, run LP feasibilization to produce a strictly feasible solution.\n\nIf helpful, you may adapt the structure of this prior best code:\nimport numpy as np\nfrom scipy.optimize import minimize, linprog\n\ndef _pairwise_distances(centers):\n    diff = centers[:, None, :] - centers[None, :, :]\n    d = np.sqrt(np.sum(diff**2, axis=2))\n    return d, diff[..., 0], diff[..., 1]\n\ndef _objective_and_grad(z, N, P):\n    # z = [x(0..N-1), y(0..N-1), r(0..N-1)]\n    x = z[:N]\n    y = z[N:2*N]\n    r = z[2*N:]\n    tiny = 1e-12\n\n    # Boundary hinges s = r - x, r - y, r - (1-x), r - (1-y)\n    s1 = r - x\n    s2 = r - y\n    s3 = r + x - 1.0\n    s4 = r + y - 1.0\n\n    h1 = np.maximum(0.0, s1)\n    h2 = np.maximum(0.0, s2)\n    h3 = np.maximum(0.0, s3)\n    h4 = np.maximum(0.0, s4)\n\n    pen_b = 0.5 * np.sum(h1*h1 + h2*h2 + h3*h3 + h4*h4)\n\n    # Overlap hinges\n    centers = np.stack([x, y], axis=1)\n    d, dx_mat, dy_mat = _pairwise_distances(centers)\n    d_safe = np.maximum(d, tiny)\n    hinge_full = r[:, None] + r[None, :] - d\n    H = np.triu(np.maximum(0.0, hinge_full), 1)\n\n    pen_o = 0.5 * np.sum(H*H)\n\n    # Objective\n    f = -np.sum(r) + P * (pen_b + pen_o)\n\n    # Gradients for boundary penalties: grad 0.5*h^2 = h * grad(arg)\n    dr_b = h1 + h2 + h3 + h4\n    dx_b = -h1 + h3\n    dy_b = -h2 + h4\n\n    # Overlap gradients\n    Qx = H * (dx_mat / d_safe)\n    Qy = H * (dy_mat / d_safe)\n    # For variable i: -sum_j Q[i,j] + sum_k Q[k,i]\n    dx_o = -np.sum(Qx, axis=1) + np.sum(Qx, axis=0)\n    dy_o = -np.sum(Qy, axis=1) + np.sum(Qy, axis=0)\n    dr_o = np.sum(H, axis=1) + np.sum(H, axis=0)\n\n    dx_total = P * (dx_b + dx_o)\n    dy_total = P * (dy_b + dy_o)\n    dr_total = -1.0 + P * (dr_b + dr_o)\n\n    grad = np.concatenate([dx_total, dy_total, dr_total])\n    return f, grad\n\ndef _lp_feasibilize(centers, r_max=None, eps_pair=1e-6):\n    \"\"\"\n    Maximize sum(r) subject to:\n      r_i >= 0\n      r_i <= r_max[i] (if provided)\n      r_i <= x_i, r_i <= y_i, r_i <= 1-x_i, r_i <= 1-y_i\n      r_i + r_j <= dist(c_i,c_j) - eps_pair\n    Use linprog highs. If infeasible with eps_pair, retry with eps_pair=0 and then scale radii down.\n    \"\"\"\n    N = centers.shape[0]\n    x = centers[:, 0]\n    y = centers[:, 1]\n\n    # Build A_ub, b_ub\n    A_rows = []\n    b_vals = []\n\n    # Boundary constraints\n    for i in range(N):\n        e = np.zeros(N)\n        e[i] = 1.0\n        A_rows.append(e.copy()); b_vals.append(x[i])\n        A_rows.append(e.copy()); b_vals.append(y[i])\n        A_rows.append(e.copy()); b_vals.append(1.0 - x[i])\n        A_rows.append(e.copy()); b_vals.append(1.0 - y[i])\n\n    # Pairwise constraints\n    d, _, _ = _pairwise_distances(centers)\n    for i in range(N):\n        for j in range(i+1, N):\n            e = np.zeros(N)\n            e[i] = 1.0; e[j] = 1.0\n            A_rows.append(e)\n            b_vals.append(d[i, j] - eps_pair)\n\n    # Optional upper bounds r_i <= r_max[i]\n    if r_max is not None:\n        for i in range(N):\n            e = np.zeros(N)\n            e[i] = 1.0\n            A_rows.append(e)\n            b_vals.append(r_max[i])\n\n    A_ub = np.array(A_rows)\n    b_ub = np.array(b_vals)\n\n    # Bounds on r: [0, 0.5]\n    bounds = [(0.0, 0.5) for _ in range(N)]\n    c = -np.ones(N)\n\n    res = linprog(c, A_ub=A_ub, b_ub=b_ub, bounds=bounds, method='highs')\n    if res.success:\n        r = res.x\n        return r\n\n    # Retry with eps_pair = 0 to get a feasible base, then scale to satisfy eps_pair\n    b_vals2 = []\n    A_rows2 = []\n    for i in range(N):\n        e = np.zeros(N); e[i] = 1.0\n        A_rows2.append(e.copy()); b_vals2.append(x[i])\n        A_rows2.append(e.copy()); b_vals2.append(y[i])\n        A_rows2.append(e.copy()); b_vals2.append(1.0 - x[i])\n        A_rows2.append(e.copy()); b_vals2.append(1.0 - y[i])\n\n    for i in range(N):\n        for j in range(i+1, N):\n            e = np.zeros(N); e[i] = 1.0; e[j] = 1.0\n            A_rows2.append(e); b_vals2.append(d[i, j])\n\n    if r_max is not None:\n        for i in range(N):\n            e = np.zeros(N); e[i] = 1.0\n            A_rows2.append(e); b_vals2.append(r_max[i])\n\n    A_ub2 = np.array(A_rows2)\n    b_ub2 = np.array(b_vals2)\n\n    res2 = linprog(c, A_ub=A_ub2, b_ub=b_ub2, bounds=bounds, method='highs')\n    if not res2.success:\n        return np.zeros(N)\n\n    r_base = res2.x\n    # Scale radii to satisfy stricter pairwise with eps_pair\n    denom = r_base[:, None] + r_base[None, :]\n    with np.errstate(divide='ignore', invalid='ignore'):\n        ratio = np.where(denom > 0, (d - eps_pair) / denom, np.inf)\n    iu = np.triu_indices(N, 1)\n    if ratio[iu].size > 0:\n        s_pairs = np.min(ratio[iu])\n        if not np.isfinite(s_pairs):\n            s_pairs = 1.0\n        s = min(1.0, s_pairs)\n    else:\n        s = 1.0\n    s = max(0.0, s)\n    r = s * r_base\n    return r\n\ndef _verify(centers, radii, tol_pair=1e-6):\n    N = len(radii)\n    x = centers[:, 0]\n    y = centers[:, 1]\n    if np.any(radii < -1e-12):\n        return False\n    if np.any(x - radii < -1e-9):\n        return False\n    if np.any(y - radii < -1e-9):\n        return False\n    if np.any(x + radii > 1.0 + 1e-9):\n        return False\n    if np.any(y + radii > 1.0 + 1e-9):\n        return False\n    d, _, _ = _pairwise_distances(centers)\n    iu = np.triu_indices(N, 1)\n    lhs = radii[:, None] + radii[None, :]\n    viol = lhs[iu] - d[iu] + tol_pair\n    if np.any(viol > 1e-12):\n        return False\n    return True\n\ndef _hex_initial_centers(N, margin=0.04, jitter_scale=0.04, seed=0):\n    # Fixed row pattern to sum 26\n    rows = [6, 5, 6, 5, 4]\n    assert sum(rows) == 26\n    max_cols = max(rows)\n    nrows = len(rows)\n    dx = (1.0 - 2*margin) / (max_cols - 1)\n    dy = (1.0 - 2*margin) / (nrows - 1) if nrows > 1 else 0.0\n    centers = []\n    rng = np.random.default_rng(seed)\n    for ri, cols in enumerate(rows):\n        y = margin + ri * dy\n        if cols == max_cols:\n            startx = margin\n        else:\n            startx = margin + 0.5 * dx\n        for k in range(cols):\n            x = startx + k * dx\n            centers.append([x, y])\n    centers = np.array(centers)\n    # Jitter\n    jitter = rng.normal(scale=jitter_scale, size=centers.shape)\n    centers = np.clip(centers + jitter, 1e-3, 1 - 1e-3)\n    return centers\n\ndef _multi_start_inits(N):\n    inits = []\n    params = [\n        (0.03, 0.02, 0),\n        (0.05, 0.03, 1),\n        (0.07, 0.03, 2),\n        (0.04, 0.04, 3),\n        (0.06, 0.02, 4),\n        (0.05, 0.04, 5),\n        (0.03, 0.03, 6),\n        (0.07, 0.02, 7),\n    ]\n    for m, j, s in params:\n        inits.append(_hex_initial_centers(N, margin=m, jitter_scale=j, seed=s))\n    return inits\n\ndef construct_packing():\n    N = 26\n    best = {'centers': None, 'radii': None, 'sum': -np.inf}\n\n    # Optimization settings\n    P_schedule = [10.0, 100.0, 1000.0, 5000.0]\n    maxiter_stage = 120\n    bounds = [(0.0, 1.0)] * (2*N) + [(0.0, 0.6)] * N\n\n    # Initializations\n    inits = _multi_start_inits(N)\n\n    for centers0 in inits:\n        # Initial radii small, within boundary\n        x0 = centers0[:, 0]\n        y0 = centers0[:, 1]\n        r0 = 0.05 * np.minimum.reduce([x0, y0, 1 - x0, 1 - y0])\n        z = np.concatenate([x0, y0, r0])\n\n        # Annealing optimization with LP tightening\n        for P in P_schedule:\n            res = minimize(\n                fun=lambda zz: _objective_and_grad(zz, N, P),\n                x0=z,\n                method='L-BFGS-B',\n                jac=True,\n                bounds=bounds,\n                options={'maxiter': maxiter_stage, 'ftol': 1e-12, 'gtol': 1e-8, 'maxcor': 20}\n            )\n            z = res.x\n            centers_tmp = np.stack([z[:N], z[N:2*N]], axis=1)\n            # Conservative LP update, cap by slight expansion over current radii\n            r_cur = np.clip(z[2*N:], 0.0, 0.5)\n            r_cap = 1.05 * r_cur\n            r_lp = _lp_feasibilize(centers_tmp, r_max=r_cap, eps_pair=1e-6)\n            z[2*N:] = np.clip(r_lp, 0.0, 0.5)\n\n        # Final LP without cap to fully utilize geometry\n        centers_cur = np.stack([z[:N], z[N:2*N]], axis=1)\n        r_final = _lp_feasibilize(centers_cur, r_max=None, eps_pair=1e-6)\n\n        # Verify and update best\n        if _verify(centers_cur, r_final, tol_pair=1e-6):\n            s = float(np.sum(r_final))\n            if s > best['sum']:\n                best['centers'] = centers_cur.copy()\n                best['radii'] = r_final.copy()\n                best['sum'] = s\n        else:\n            # Try slight shrink and re-LP to fix tiny violations\n            d, _, _ = _pairwise_distances(centers_cur)\n            denom = r_final[:, None] + r_final[None, :]\n            with np.errstate(divide='ignore', invalid='ignore'):\n                ratio = np.where(denom > 0, (d - 1.0e-6) / denom, np.inf)\n            iu = np.triu_indices(N, 1)\n            s_pairs = np.min(ratio[iu]) if ratio[iu].size > 0 else 1.0\n            if not np.isfinite(s_pairs):\n                s_pairs = 1.0\n            s_pairs = max(0.0, min(1.0, s_pairs))\n            r_try = s_pairs * r_final\n            # Boundaries scale check\n            bx = np.minimum.reduce([centers_cur[:,0], centers_cur[:,1], 1-centers_cur[:,0], 1-centers_cur[:,1]])\n            scale_b = np.min(np.where(r_try > 0, bx / r_try, 1.0))\n            scale_b = min(1.0, scale_b if np.isfinite(scale_b) else 1.0)\n            r_try = scale_b * r_try\n            if _verify(centers_cur, r_try, tol_pair=1e-6):\n                s = float(np.sum(r_try))\n                if s > best['sum']:\n                    best['centers'] = centers_cur.copy()\n                    best['radii'] = r_try.copy()\n                    best['sum'] = s\n\n    # Fallback if nothing verified\n    if best['centers'] is None:\n        centers_fb = _hex_initial_centers(N, margin=0.06, jitter_scale=0.02, seed=42)\n        r_fb = _lp_feasibilize(centers_fb, r_max=None, eps_pair=1e-6)\n        if not _verify(centers_fb, r_fb, tol_pair=1e-6):\n            r_fb = 0.95 * r_fb\n        best['centers'] = centers_fb\n        best['radii'] = r_fb\n        best['sum'] = float(np.sum(r_fb))\n\n    # Ensure strict feasibility and final verify\n    radii_strict = _lp_feasibilize(best['centers'], r_max=None, eps_pair=1.1e-6)\n    if _verify(best['centers'], radii_strict, tol_pair=1e-6):\n        radii = radii_strict\n    else:\n        radii = best['radii']\n\n    centers = best['centers']\n    # Final hardening: if any violation persists, scale radii conservatively\n    if not _verify(centers, radii, tol_pair=1e-6):\n        d, _, _ = _pairwise_distances(centers)\n        denom = radii[:, None] + radii[None, :]\n        with np.errstate(divide='ignore', invalid='ignore'):\n            ratio = np.where(denom > 0, (d - 1e-6) / denom, np.inf)\n        iu = np.triu_indices(N, 1)\n        s_pairs = np.min(ratio[iu]) if ratio[iu].size > 0 else 1.0\n        s_pairs = min(1.0, s_pairs if np.isfinite(s_pairs) else 1.0)\n        s_pairs = max(0.0, s_pairs)\n        radii = s_pairs * radii\n        bx = np.minimum.reduce([centers[:,0], centers[:,1], 1-centers[:,0], 1-centers[:,1]])\n        scale_b = np.min(np.where(radii > 0, bx / radii, 1.0))\n        scale_b = min(1.0, scale_b if np.isfinite(scale_b) else 1.0)\n        radii = scale_b * radii\n\n    # Clip tiny negatives and ensure within [0,0.5]\n    radii = np.clip(radii, 0.0, 0.5)\n    sum_radii = float(np.sum(radii))\n    return centers.astype(float), radii.astype(float), sum_radii\n\ndef run_packing():\n    centers, radii, sum_radii = construct_packing()\n    return centers, radii, sum_radii\n\n\nRequirements:\n- Implement construct_packing() that returns (centers, radii, sum_radii).\n- centers: np.array shape (26, 2), radii: np.array shape (26,), sum_radii: float.\n- Use only numpy and scipy.\n- Exactly 26 circles; all inside [0,1]^2; no overlaps (tolerance 1e-6); radii >= 0.\n- Runtime <= 300s; no prints.\n- Use a linear program (scipy.optimize.linprog) for final feasibilization to strictly satisfy constraints:\n  maximize sum(r) subject to:\n    0 <= r_i <= r_cap_i\n    r_i <= x_i, r_i <= y_i, r_i <= 1-x_i, r_i <= 1-y_i\n    r_i + r_j <= dist_ij - 1e-6 for all i<j\n  where r_cap_i are current candidate radii (upper bounds) and dist_ij are pairwise distances.\n  Implement as minimizing -sum(r).\n- Provide robust, vectorized computations; avoid numerical bias (apply eps only to denominators).\n- Return the strictly feasible LP output.\n\n\nWrite a single code block with:\n- construct_packing()\n- Optional run_packing() helper\n",
  "response": "```python\nimport numpy as np\nfrom scipy.optimize import minimize, linprog\n\n# ============ Geometry utilities ============\n\ndef _pairwise_distances(centers):\n    diff = centers[:, None, :] - centers[None, :, :]\n    d = np.sqrt(np.sum(diff**2, axis=2))\n    return d, diff[..., 0], diff[..., 1]\n\n# ============ Smooth penalties (softplus with temperature) ============\n\ndef _softplus_scaled(x, tau):\n    # t*log(1+exp(x/t)) computed stably\n    u = x / tau\n    # softplus(u) = log1p(exp(-|u|)) + max(u,0)\n    return tau * (np.log1p(np.exp(-np.abs(u))) + np.maximum(u, 0.0))\n\ndef _sigmoid_scaled(x, tau):\n    # sigma(x/tau) computed stably\n    u = x / tau\n    # Stable sigmoid\n    out = np.empty_like(u)\n    pos = u >= 0\n    out[pos] = 1.0 / (1.0 + np.exp(-u[pos]))\n    expu = np.exp(u[~pos])\n    out[~pos] = expu / (1.0 + expu)\n    return out\n\n# ============ Objective and gradient (vectorized) ============\n\ndef _objective_and_grad_soft(z, N, lam_b, lam_o, tau):\n    # z = [x(0..N-1), y(0..N-1), r(0..N-1)]\n    x = z[:N]\n    y = z[N:2*N]\n    r = z[2*N:]\n    tiny = 1e-12\n\n    # Boundary soft hinges s = r - x, r - y, r + x - 1, r + y - 1\n    s1 = r - x\n    s2 = r - y\n    s3 = r + x - 1.0\n    s4 = r + y - 1.0\n\n    sp1 = _softplus_scaled(s1, tau)\n    sp2 = _softplus_scaled(s2, tau)\n    sp3 = _softplus_scaled(s3, tau)\n    sp4 = _softplus_scaled(s4, tau)\n\n    pen_b = lam_b * np.sum(sp1 + sp2 + sp3 + sp4)\n\n    g1 = lam_b * _sigmoid_scaled(s1, tau)\n    g2 = lam_b * _sigmoid_scaled(s2, tau)\n    g3 = lam_b * _sigmoid_scaled(s3, tau)\n    g4 = lam_b * _sigmoid_scaled(s4, tau)\n\n    # Overlap soft hinges over i<j pairs\n    centers = np.stack([x, y], axis=1)\n    d, dx_mat, dy_mat = _pairwise_distances(centers)\n    d_safe = d + tiny  # only for gradients; do not bias the penalty\n\n    hinge_full = r[:, None] + r[None, :] - d\n    iu = np.triu_indices(N, 1)\n    hinge_u = hinge_full[iu]\n\n    sp_o = _softplus_scaled(hinge_u, tau)\n    pen_o = lam_o * np.sum(sp_o)\n\n    sigma_u = _sigmoid_scaled(hinge_u, tau)  # shape (num_pairs,)\n    # Build an upper-triangular matrix S with sigma on (i<j), zeros elsewhere\n    S = np.zeros_like(d)\n    S[iu] = sigma_u\n\n    # Gradients for overlaps\n    # Qx = S * (dx_mat / d_safe), Qy = S * (dy_mat / d_safe)\n    Qx = S * (dx_mat / d_safe)\n    Qy = S * (dy_mat / d_safe)\n\n    # For variable i: -sum_j Q[i,j] + sum_k Q[k,i] (since ds/dx_i = -(x_i-x_j)/d)\n    dx_o = lam_o * (-np.sum(Qx, axis=1) + np.sum(Qx, axis=0))\n    dy_o = lam_o * (-np.sum(Qy, axis=1) + np.sum(Qy, axis=0))\n\n    # dr contribution: ds/dr_i = 1 for each pair containing i\n    # Sum of S over row i (i<j) plus over column i (k<i)\n    dr_o = lam_o * (np.sum(S, axis=1) + np.sum(S, axis=0))\n\n    # Boundary gradients accumulation\n    dx_b = -g1 + g3\n    dy_b = -g2 + g4\n    dr_b = g1 + g2 + g3 + g4\n\n    # Total gradients\n    dx_total = dx_b + dx_o\n    dy_total = dy_b + dy_o\n    dr_total = -1.0 + dr_b + dr_o  # objective has -sum(r)\n\n    grad = np.concatenate([dx_total, dy_total, dr_total])\n    f = -np.sum(r) + pen_b + pen_o\n    return f, grad\n\n# ============ Linear-program feasibilizer ============\n\ndef _lp_feasibilize(centers, r_cap=None, eps_pair=1e-6):\n    \"\"\"\n    Maximize sum(r) subject to:\n      0 <= r_i <= r_cap[i] (if provided, else <= 0.5)\n      r_i <= x_i, r_i <= y_i, r_i <= 1-x_i, r_i <= 1-y_i\n      r_i + r_j <= dist(c_i,c_j) - eps_pair\n    Implemented as minimizing -sum(r) with linprog (HiGHS).\n    Includes a fallback with eps_pair=0 and post-scaling if necessary.\n    \"\"\"\n    N = centers.shape[0]\n    x = centers[:, 0]\n    y = centers[:, 1]\n\n    # Boundary constraints (vectorized construction)\n    # For each bound r_i <= b: a row with 1 at i\n    A_rows = []\n    b_vals = []\n\n    eyeN = np.eye(N)\n    # r_i <= x_i\n    A_rows.append(eyeN.copy()); b_vals.append(x.copy())\n    # r_i <= y_i\n    A_rows.append(eyeN.copy()); b_vals.append(y.copy())\n    # r_i <= 1 - x_i\n    A_rows.append(eyeN.copy()); b_vals.append(1.0 - x)\n    # r_i <= 1 - y_i\n    A_rows.append(eyeN.copy()); b_vals.append(1.0 - y)\n\n    # Pairwise constraints\n    d, _, _ = _pairwise_distances(centers)\n    iu = np.triu_indices(N, 1)\n    num_pairs = iu[0].size\n    if num_pairs > 0:\n        A_pairs = np.zeros((num_pairs, N))\n        A_pairs[np.arange(num_pairs), iu[0]] = 1.0\n        A_pairs[np.arange(num_pairs), iu[1]] = 1.0\n        b_pairs = d[iu] - eps_pair\n        A_rows.append(A_pairs); b_vals.append(b_pairs)\n\n    # Optional caps r_i <= r_cap[i]\n    if r_cap is not None:\n        A_rows.append(eyeN.copy()); b_vals.append(np.asarray(r_cap).copy())\n\n    A_ub = np.vstack(A_rows) if len(A_rows) > 0 else None\n    b_ub = np.concatenate(b_vals) if len(b_vals) > 0 else None\n\n    # Bounds 0 <= r_i <= 0.5\n    bounds = [(0.0, 0.5) for _ in range(N)]\n    c = -np.ones(N)\n\n    res = linprog(c, A_ub=A_ub, b_ub=b_ub, bounds=bounds, method='highs')\n    if res.success:\n        r = res.x\n        return np.clip(r, 0.0, 0.5)\n\n    # Fallback: relax pairwise epsilon to 0, then scale to meet eps_pair\n    if num_pairs > 0:\n        b_pairs0 = d[iu]\n        b_ub2 = b_ub.copy()\n        # Replace the tail (pairwise block) of b_ub with b_pairs0\n        # Compute sizes\n        nb = 4*N\n        if r_cap is not None:\n            nb += N\n        # b_ub layout: [4N boundary][num_pairs pairs][N caps?] depending on append order above.\n        # Above, order: boundaries (4 blocks), pairs, caps (optional)\n        # So we replace the block starting at 4N with b_pairs0\n        start = 4*N\n        b_ub2[start:start+num_pairs] = b_pairs0\n        res2 = linprog(c, A_ub=A_ub, b_ub=b_ub2, bounds=bounds, method='highs')\n        if res2.success:\n            r_base = np.clip(res2.x, 0.0, 0.5)\n            # Scale radii to satisfy stricter eps_pair\n            denom = r_base[:, None] + r_base[None, :]\n            with np.errstate(divide='ignore', invalid='ignore'):\n                ratio = np.where(denom > 0, (d - eps_pair) / denom, np.inf)\n            s_pairs = np.min(ratio[iu]) if ratio[iu].size > 0 else 1.0\n            if not np.isfinite(s_pairs):\n                s_pairs = 1.0\n            s = float(np.clip(s_pairs, 0.0, 1.0))\n            r = s * r_base\n            # Boundary safety\n            bx = np.minimum.reduce([x, y, 1 - x, 1 - y])\n            with np.errstate(divide='ignore', invalid='ignore'):\n                scale_b = np.min(np.where(r > 0, bx / r, 1.0))\n            if not np.isfinite(scale_b):\n                scale_b = 1.0\n            r = np.clip(scale_b, 0.0, 1.0) * r\n            return np.clip(r, 0.0, 0.5)\n\n    # Give up with zeros (should not happen)\n    return np.zeros(N)\n\n# ============ Verification ============\n\ndef _verify(centers, radii, tol_pair=1e-6):\n    N = len(radii)\n    x = centers[:, 0]\n    y = centers[:, 1]\n    if centers.shape != (N, 2):\n        return False\n    if np.any(radii < -1e-12):\n        return False\n    if np.any(x - radii < -1e-9):\n        return False\n    if np.any(y - radii < -1e-9):\n        return False\n    if np.any(x + radii > 1.0 + 1e-9):\n        return False\n    if np.any(y + radii > 1.0 + 1e-9):\n        return False\n    d, _, _ = _pairwise_distances(centers)\n    iu = np.triu_indices(N, 1)\n    lhs = radii[:, None] + radii[None, :]\n    viol = lhs[iu] - d[iu] + tol_pair\n    if np.any(viol > 1e-12):\n        return False\n    return True\n\n# ============ Initializations ============\n\ndef _hex_initial_centers(N, margin=0.04, jitter_scale=0.03, seed=0):\n    # Fixed row pattern summing to 26\n    rows = [6, 5, 6, 5, 4]\n    assert sum(rows) == N\n    max_cols = max(rows)\n    nrows = len(rows)\n    dx = (1.0 - 2*margin) / (max_cols - 1)\n    dy = (1.0 - 2*margin) / (nrows - 1) if nrows > 1 else 0.0\n    centers = []\n    for ri, cols in enumerate(rows):\n        y = margin + ri * dy\n        if cols == max_cols:\n            startx = margin\n        else:\n            startx = margin + 0.5 * dx\n        for k in range(cols):\n            x = startx + k * dx\n            centers.append([x, y])\n    centers = np.array(centers)\n    rng = np.random.default_rng(seed)\n    jitter = rng.normal(scale=jitter_scale, size=centers.shape)\n    centers = np.clip(centers + jitter, 1e-3, 1 - 1e-3)\n    return centers\n\ndef _lhs_minsep(N, min_sep=0.14, jitter=0.015, seed=0, relax_iters=20, step=0.25):\n    \"\"\"\n    Latin-hypercube-like centers with a push-away heuristic to enforce min separation approximately.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    grid = (np.arange(N) + 0.5) / N\n    x = grid.copy(); rng.shuffle(x)\n    y = grid.copy(); rng.shuffle(y)\n    centers = np.stack([x, y], axis=1)\n    centers += rng.uniform(-jitter, jitter, size=centers.shape)\n    centers = np.clip(centers, 1e-3, 1 - 1e-3)\n\n    for _ in range(relax_iters):\n        d, dx, dy = _pairwise_distances(centers)\n        mask = (d > 0) & (d < min_sep)\n        if not np.any(mask):\n            break\n        # Repulsive displacements\n        with np.errstate(divide='ignore', invalid='ignore'):\n            rep = (min_sep - d) / (d + 1e-12)\n        rx = np.sum((dx * rep) * mask, axis=1)\n        ry = np.sum((dy * rep) * mask, axis=1)\n        disp = np.stack([rx, ry], axis=1)\n        centers = centers + step * disp / (np.linalg.norm(disp, axis=1, keepdims=True) + 1e-12) * (1.0 / N)\n        centers = np.clip(centers, 1e-3, 1 - 1e-3)\n    return centers\n\ndef _random_push(N, seed=0, min_sep=0.12, iters=60, step=0.15):\n    rng = np.random.default_rng(seed)\n    centers = rng.uniform(0.05, 0.95, size=(N, 2))\n    for _ in range(iters):\n        d, dx, dy = _pairwise_distances(centers)\n        mask = (d > 0) & (d < min_sep)\n        if not np.any(mask):\n            break\n        with np.errstate(divide='ignore', invalid='ignore'):\n            rep = (min_sep - d) / (d + 1e-12)\n        rx = np.sum((dx * rep) * mask, axis=1)\n        ry = np.sum((dy * rep) * mask, axis=1)\n        disp = np.stack([rx, ry], axis=1)\n        centers = centers + step * disp / (np.linalg.norm(disp, axis=1, keepdims=True) + 1e-12)\n        centers = np.clip(centers, 1e-3, 1 - 1e-3)\n    return centers\n\ndef _multi_start_inits(N):\n    inits = []\n    # Hexagonal-like grids with varying margins/jitters\n    params_hex = [\n        (0.03, 0.02, 0),\n        (0.05, 0.03, 1),\n        (0.07, 0.03, 2),\n        (0.04, 0.04, 3),\n        (0.06, 0.02, 4),\n        (0.05, 0.04, 5),\n        (0.03, 0.03, 6),\n        (0.07, 0.02, 7),\n    ]\n    for m, j, s in params_hex:\n        inits.append(_hex_initial_centers(N, margin=m, jitter_scale=j, seed=s))\n    # Latin-hypercube-like with min-sep variants\n    params_lhs = [\n        (0.13, 0.018, 10),\n        (0.15, 0.018, 11),\n        (0.17, 0.020, 12),\n        (0.14, 0.015, 13),\n        (0.16, 0.020, 14),\n        (0.18, 0.020, 15),\n    ]\n    for ms, jit, s in params_lhs:\n        inits.append(_lhs_minsep(N, min_sep=ms, jitter=jit, seed=s, relax_iters=25, step=0.3))\n    # Random with push-away\n    params_rand = [\n        (0.12, 20, 0.20, 21),\n        (0.14, 30, 0.20, 22),\n        (0.16, 40, 0.15, 23),\n    ]\n    for ms, it, st, s in params_rand:\n        inits.append(_random_push(N, seed=s, min_sep=ms, iters=it, step=st))\n    return inits\n\n# ============ Main construction ============\n\ndef construct_packing():\n    N = 26\n    best = {'centers': None, 'radii': None, 'sum': -np.inf}\n\n    # Annealing schedule: (lambda_bound, lambda_overlap, tau)\n    schedule = [\n        (25.0,   25.0,   0.05),\n        (90.0,  180.0,   0.03),\n        (350.0, 700.0,   0.02),\n        (1400.0, 2200.0, 0.015),\n        (6000.0, 9000.0, 0.010),\n    ]\n    maxiter_stage = 90\n    bounds = [(0.0, 1.0)] * (2*N) + [(0.0, 0.5)] * N\n\n    # Initializations\n    inits = _multi_start_inits(N)\n    # We'll add jittered inherits later if we discover a good layout\n    extra_inits = []\n\n    def _optimize_from_init(centers0, extra_boost=False):\n        nonlocal best\n        x0 = centers0[:, 0]\n        y0 = centers0[:, 1]\n        # Initial radii: small fraction of boundary clearance\n        clear = np.minimum.reduce([x0, y0, 1 - x0, 1 - y0])\n        r0 = 0.08 * clear\n        z = np.concatenate([x0, y0, r0])\n\n        # Annealing optimization with periodic LP tightening\n        for (lam_b, lam_o, tau) in schedule:\n            res = minimize(\n                fun=lambda zz: _objective_and_grad_soft(zz, N, lam_b, lam_o, tau),\n                x0=z,\n                method='L-BFGS-B',\n                jac=True,\n                bounds=bounds,\n                options={'maxiter': maxiter_stage if not extra_boost else int(1.2*maxiter_stage),\n                         'ftol': 1e-12, 'gtol': 1e-8, 'maxcor': 20}\n            )\n            z = res.x\n            centers_tmp = np.stack([z[:N], z[N:2*N]], axis=1)\n            r_cur = np.clip(z[2*N:], 0.0, 0.5)\n            # Conservative LP update, cap by slight expansion over current radii\n            r_cap = np.minimum(1.10 * r_cur, np.minimum.reduce([\n                centers_tmp[:, 0], centers_tmp[:, 1], 1 - centers_tmp[:, 0], 1 - centers_tmp[:, 1]\n            ]))\n            r_lp = _lp_feasibilize(centers_tmp, r_cap=r_cap, eps_pair=1e-6)\n            z[2*N:] = np.clip(r_lp, 0.0, 0.5)\n\n        # Final LP without cap to fully utilize geometry\n        centers_cur = np.stack([z[:N], z[N:2*N]], axis=1)\n        r_final = _lp_feasibilize(centers_cur, r_cap=None, eps_pair=1e-6)\n        # Verify and update best\n        if _verify(centers_cur, r_final, tol_pair=1e-6):\n            s = float(np.sum(r_final))\n            if s > best['sum'] + 1e-8:\n                best['centers'] = centers_cur.copy()\n                best['radii'] = r_final.copy()\n                best['sum'] = s\n                # Add jittered inherits to explore neighborhood\n                rng = np.random.default_rng(int(1e6 * s) % 2**32)\n                for sc in [0.010, 0.015, 0.020]:\n                    jit = rng.normal(scale=sc, size=centers_cur.shape)\n                    c_new = np.clip(centers_cur + jit, 1e-3, 1 - 1e-3)\n                    extra_inits.append(c_new)\n\n    # Run through base initializations\n    for centers0 in inits:\n        _optimize_from_init(centers0, extra_boost=False)\n\n    # Process a limited number of inherited jittered starts\n    # Limit count to keep runtime bounded\n    for i in range(min(6, len(extra_inits))):\n        _optimize_from_init(extra_inits[i], extra_boost=True)\n\n    # Fallback if nothing verified (unlikely)\n    if best['centers'] is None:\n        centers_fb = _hex_initial_centers(N, margin=0.06, jitter_scale=0.02, seed=42)\n        r_fb = _lp_feasibilize(centers_fb, r_cap=None, eps_pair=1e-6)\n        if not _verify(centers_fb, r_fb, tol_pair=1e-6):\n            # Try slight shrink and re-LP (robustify)\n            r_fb = 0.95 * r_fb\n            r_fb = _lp_feasibilize(centers_fb, r_cap=r_fb, eps_pair=1e-6)\n        best['centers'] = centers_fb\n        best['radii'] = r_fb\n        best['sum'] = float(np.sum(r_fb))\n\n    # Final strict LP hardening on best centers\n    centers = best['centers']\n    radii = _lp_feasibilize(centers, r_cap=None, eps_pair=1.0e-6)\n    # Ensure strict feasibility; if solver failed, scale conservatively\n    if not _verify(centers, radii, tol_pair=1e-6):\n        d, _, _ = _pairwise_distances(centers)\n        iu = np.triu_indices(N, 1)\n        denom = radii[:, None] + radii[None, :]\n        with np.errstate(divide='ignore', invalid='ignore'):\n            ratio = np.where(denom > 0, (d - 1.0e-6) / denom, np.inf)\n        s_pairs = np.min(ratio[iu]) if ratio[iu].size > 0 else 1.0\n        if not np.isfinite(s_pairs):\n            s_pairs = 1.0\n        s_pairs = float(np.clip(s_pairs, 0.0, 1.0))\n        radii = s_pairs * radii\n        bx = np.minimum.reduce([centers[:, 0], centers[:, 1], 1 - centers[:, 0], 1 - centers[:, 1]])\n        with np.errstate(divide='ignore', invalid='ignore'):\n            scale_b = np.min(np.where(radii > 0, bx / radii, 1.0))\n        if not np.isfinite(scale_b):\n            scale_b = 1.0\n        radii = np.clip(scale_b, 0.0, 1.0) * radii\n        radii = np.clip(radii, 0.0, 0.5)\n\n    sum_radii = float(np.sum(radii))\n    return centers.astype(float), radii.astype(float), sum_radii\n\ndef run_packing():\n    centers, radii, sum_radii = construct_packing()\n    return centers, radii, sum_radii\n```",
  "reasoning": "",
  "timestamp": "2026-02-11T01:12:43.284840",
  "cost_data": null,
  "model_config": {
    "model": "openai/gpt-5",
    "temperature": 0.4
  },
  "llm_call_duration_s": 198.7887767079519,
  "eval_duration_s": 1.9483528060372919
}