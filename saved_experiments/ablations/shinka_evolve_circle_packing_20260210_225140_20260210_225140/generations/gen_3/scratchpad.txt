============================================================
SCRATCHPAD FOR GENERATION 3
============================================================


## Calibration Observations (Round 2 — Gradient implementations)

Summary:
- All models complied with “single code block, no prose” and produced fully vectorized NumPy gradients with FD checks.
- Everyone handled boundary subgradients via hinge indicators and overlap grads via masked pairwise ops; stabilization used either d+eps or sqrt(d^2+eps/eps^2).

Per-model notes:
- gpt5: Best reference. Uses d for penalty and d+eps only for division (no bias), symmetric active mask, clean typing, central FD checks. Ideal for gradient-sensitive code.
- o4-mini: Compact, correct, neat masking; stabilization via sqrt(d^2+eps). Clear FD error summaries. Great for precise, format-strict utilities.
- gemini-flash: Clear structure, thorough FD harness, sqrt(d^2+eps^2) stabilization; easy to read/maintain.
- gemini-pro: Correct math; uses d+eps in both penalty and division (tiny bias). Strong docstring and central FD checks; production-friendly.
- sonnet: Correct; computes overlaps on upper triangle then symmetrizes via row/col sums; stabilization sqrt(d^2+eps); slightly verbose but precise.
- gpt5-mini: Mirrors gpt5’s pattern (no penalty bias), cost-effective, good FD harness; strong choice under budget.
- gpt5-nano: Correct, minimal; uses d+eps in penalty (small bias), succinct FD outputs; fine for lightweight tasks.

Takeaways:
- Prefer gpt5 or o4-mini for critical gradient code; gpt5-mini for budget.
- To avoid penalty bias, apply eps only to division (gpt5/gpt5-mini style).
- All are suitable for vectorized constraint/penalty work; keep temperatures low for strict format/compliance with sonnet/gemini-*.

## Generation 0 Plan
Goal: Maximize sum of radii for 26 circles in unit square with strict feasibility.
Strategies (8):
1) gpt5 — Analytic-gradient penalized L-BFGS-B (centers+radii), annealed penalties, LP feasibilization.
2) o4-mini — Force-directed centers (smooth repulsion + boundary) via L-BFGS-B, radii via LP; multi-start.
3) gemini-flash — Basin-hopping on penalized objective with analytic gradient; LP feasibilization.
4) sonnet — Alternating local coordinate search on centers with objective = LP(sum of radii); derivative-free; step annealing.
5) gpt5-mini — Multi-stage penalty L-BFGS-B with robust vectorized gradients; multiple restarts; LP feasibilization.
6) gpt5-nano — Hex/triangular-grid seeding; radii via LP; brief gradient polish; LP finalization.
7) gemini-pro — SLSQP with explicit nonlinear constraints and Jacobians (pairwise and boundary); initialized from structured layout.
8) gemini-flash — Unconstrained reparameterization (sigmoid/tanh) for (x,y,r); L-BFGS-B on unconstrained vars; LP cleanup.

Key implementation details to emphasize across children:
- Vectorized gradients; apply epsilon only in division (not in penalty) to avoid bias (per calibration).
- LP feasibilization: Given centers and candidate radii, solve a linear program to maximize sum(r) subject to r_i >= 0, r_i <= current r_i, r_i <= x_i, r_i <= y_i, r_i <= 1-x_i, r_i <= 1-y_i, and r_i + r_j <= dist_ij - 1e-6.
- Return strictly feasible solution with shapes: centers (26,2), radii (26,), sum_radii float.
- No prints; runtime-conscious (<=300s).

## Generation 2 Insights
- Best score this gen: 2.614178549664672 (trial_2_0)
- Tried 8 diverse strategies:
  1) Annealed L-BFGS-B with analytic gradients + LP
  2) Force-directed centers with softmin surrogate + LP
  3) Basin-hopping with analytic gradients + LP
  4) Alternating coordinate search on LP objective
  5) Multi-stage penalty L-BFGS-B (budget-friendly)
  6) Hex grid + LP + surrogate polish
  7) SLSQP with full constraints and Jacobians
  8) Unconstrained reparam with softmin clamp + LP
- Keep: strongest penalty-annealed variants and any that exceed 2.63; maintain LP-first methods for stability.
- Next: hybridize best penalty run's final centers with alt-coordinate LP local search; add deterministic refined grid initializations; try pairwise ADMM-style updates for centers; consider trust-constr for constrained formulation.
