{
  "trial_id": "trial_5_3",
  "generation": 5,
  "parent_id": "trial_2_0",
  "code": "import numpy as np\nfrom scipy.optimize import minimize, linprog\n\ndef _pairwise_distances(centers):\n    diff = centers[:, None, :] - centers[None, :, :]\n    d = np.sqrt(np.sum(diff**2, axis=2))\n    return d, diff[..., 0], diff[..., 1]\n\ndef _objective_and_grad(z, N, P):\n    x = z[:N]\n    y = z[N:2*N]\n    r = z[2*N:]\n    tiny = 1e-12\n\n    # Boundary hinges\n    h1 = np.maximum(0.0, r - x)\n    h2 = np.maximum(0.0, r - y)\n    h3 = np.maximum(0.0, r + x - 1.0)\n    h4 = np.maximum(0.0, r + y - 1.0)\n    pen_b = 0.5 * np.sum(h1*h1 + h2*h2 + h3*h3 + h4*h4)\n\n    # Overlap hinges\n    centers = np.stack([x, y], axis=1)\n    d, dx_mat, dy_mat = _pairwise_distances(centers)\n    d_safe = np.maximum(d, tiny)\n    hinge_full = r[:, None] + r[None, :] - d\n    H = np.triu(np.maximum(0.0, hinge_full), 1)\n    pen_o = 0.5 * np.sum(H*H)\n\n    f = -np.sum(r) + P * (pen_b + pen_o)\n\n    # Gradients\n    dr_b = h1 + h2 + h3 + h4\n    dx_b = -h1 + h3\n    dy_b = -h2 + h4\n\n    Qx = H * (dx_mat / d_safe)\n    Qy = H * (dy_mat / d_safe)\n    dx_o = -np.sum(Qx, axis=1) + np.sum(Qx, axis=0)\n    dy_o = -np.sum(Qy, axis=1) + np.sum(Qy, axis=0)\n    dr_o = np.sum(H, axis=1) + np.sum(H, axis=0)\n\n    dx_total = P * (dx_b + dx_o)\n    dy_total = P * (dy_b + dy_o)\n    dr_total = -1.0 + P * (dr_b + dr_o)\n\n    grad = np.concatenate([dx_total, dy_total, dr_total])\n    return f, grad\n\ndef _lp_feasibilize(centers, r_max=None, eps_pair=1e-6):\n    N = centers.shape[0]\n    x = centers[:, 0]\n    y = centers[:, 1]\n\n    A_rows = []\n    b_vals = []\n\n    # Boundary constraints\n    for i in range(N):\n        e = np.zeros(N)\n        e[i] = 1.0\n        A_rows.append(e.copy()); b_vals.append(x[i])\n        A_rows.append(e.copy()); b_vals.append(y[i])\n        A_rows.append(e.copy()); b_vals.append(1.0 - x[i])\n        A_rows.append(e.copy()); b_vals.append(1.0 - y[i])\n\n    # Pairwise constraints\n    d, _, _ = _pairwise_distances(centers)\n    for i in range(N):\n        for j in range(i+1, N):\n            e = np.zeros(N)\n            e[i] = 1.0; e[j] = 1.0\n            A_rows.append(e)\n            b_vals.append(d[i, j] - eps_pair)\n\n    if r_max is not None:\n        for i in range(N):\n            e = np.zeros(N)\n            e[i] = 1.0\n            A_rows.append(e)\n            b_vals.append(r_max[i])\n\n    A_ub = np.array(A_rows)\n    b_ub = np.array(b_vals)\n    bounds = [(0.0, 0.5) for _ in range(N)]\n    c = -np.ones(N)\n\n    res = linprog(c, A_ub=A_ub, b_ub=b_ub, bounds=bounds, method='highs')\n    if res.success:\n        return res.x\n\n    # Retry with eps_pair = 0\n    b_vals2 = []\n    A_rows2 = []\n    for i in range(N):\n        e = np.zeros(N); e[i] = 1.0\n        A_rows2.append(e.copy()); b_vals2.append(x[i])\n        A_rows2.append(e.copy()); b_vals2.append(y[i])\n        A_rows2.append(e.copy()); b_vals2.append(1.0 - x[i])\n        A_rows2.append(e.copy()); b_vals2.append(1.0 - y[i])\n\n    for i in range(N):\n        for j in range(i+1, N):\n            e = np.zeros(N); e[i] = 1.0; e[j] = 1.0\n            A_rows2.append(e); b_vals2.append(d[i, j])\n\n    if r_max is not None:\n        for i in range(N):\n            e = np.zeros(N); e[i] = 1.0\n            A_rows2.append(e); b_vals2.append(r_max[i])\n\n    A_ub2 = np.array(A_rows2)\n    b_ub2 = np.array(b_vals2)\n\n    res2 = linprog(c, A_ub=A_ub2, b_ub=b_ub2, bounds=bounds, method='highs')\n    if not res2.success:\n        return np.zeros(N)\n\n    r_base = res2.x\n    denom = r_base[:, None] + r_base[None, :]\n    with np.errstate(divide='ignore', invalid='ignore'):\n        ratio = np.where(denom > 0, (d - eps_pair) / denom, np.inf)\n    iu = np.triu_indices(N, 1)\n    if ratio[iu].size > 0:\n        s_pairs = np.min(ratio[iu])\n        if not np.isfinite(s_pairs):\n            s_pairs = 1.0\n        s = min(1.0, s_pairs)\n    else:\n        s = 1.0\n    s = max(0.0, s)\n    return s * r_base\n\ndef _verify(centers, radii, tol_pair=1e-6):\n    N = len(radii)\n    x = centers[:, 0]\n    y = centers[:, 1]\n    if np.any(radii < -1e-12):\n        return False\n    if np.any(x - radii < -1e-9):\n        return False\n    if np.any(y - radii < -1e-9):\n        return False\n    if np.any(x + radii > 1.0 + 1e-9):\n        return False\n    if np.any(y + radii > 1.0 + 1e-9):\n        return False\n    d, _, _ = _pairwise_distances(centers)\n    iu = np.triu_indices(N, 1)\n    lhs = radii[:, None] + radii[None, :]\n    viol = lhs[iu] - d[iu] + tol_pair\n    if np.any(viol > 1e-12):\n        return False\n    return True\n\ndef _hex_initial_centers(N, margin=0.04, jitter_scale=0.04, seed=0):\n    rows = [6, 5, 6, 5, 4]\n    assert sum(rows) == 26\n    max_cols = max(rows)\n    nrows = len(rows)\n    dx = (1.0 - 2*margin) / (max_cols - 1)\n    dy = (1.0 - 2*margin) / (nrows - 1) if nrows > 1 else 0.0\n    centers = []\n    rng = np.random.default_rng(seed)\n    for ri, cols in enumerate(rows):\n        y = margin + ri * dy\n        if cols == max_cols:\n            startx = margin\n        else:\n            startx = margin + 0.5 * dx\n        for k in range(cols):\n            x = startx + k * dx\n            centers.append([x, y])\n    centers = np.array(centers)\n    jitter = rng.normal(scale=jitter_scale, size=centers.shape)\n    centers = np.clip(centers + jitter, 1e-3, 1 - 1e-3)\n    return centers\n\ndef _force_directed_phase(centers, n_iter=50, dt=0.001):\n    N = centers.shape[0]\n    for _ in range(n_iter):\n        forces = np.zeros_like(centers)\n        \n        # Pairwise repulsion\n        for i in range(N):\n            for j in range(i+1, N):\n                diff = centers[i] - centers[j]\n                dist = np.linalg.norm(diff)\n                if dist > 1e-12:\n                    force_mag = 0.01 / (dist**2 + 1e-6)\n                    force_dir = diff / dist\n                    forces[i] += force_mag * force_dir\n                    forces[j] -= force_mag * force_dir\n        \n        # Boundary repulsion\n        for i in range(N):\n            x, y = centers[i]\n            if x < 0.1:\n                forces[i, 0] += 0.005 / (x + 1e-6)\n            if x > 0.9:\n                forces[i, 0] -= 0.005 / (1.0 - x + 1e-6)\n            if y < 0.1:\n                forces[i, 1] += 0.005 / (y + 1e-6)\n            if y > 0.9:\n                forces[i, 1] -= 0.005 / (1.0 - y + 1e-6)\n        \n        centers += dt * forces\n        centers = np.clip(centers, 0.02, 0.98)\n    \n    return centers\n\ndef construct_packing():\n    N = 26\n    best = {'centers': None, 'radii': None, 'sum': -np.inf}\n\n    P_schedule = [20.0, 200.0, 2000.0]\n    maxiter_stage = 150\n    bounds = [(0.0, 1.0)] * (2*N) + [(0.0, 0.5)] * N\n\n    # Multiple initializations\n    inits = []\n    for seed in range(8):\n        centers0 = _hex_initial_centers(N, margin=0.03 + 0.01*seed, jitter_scale=0.02 + 0.01*seed, seed=seed)\n        # Force-directed spreading\n        centers0 = _force_directed_phase(centers0, n_iter=30)\n        inits.append(centers0)\n\n    for centers0 in inits:\n        x0 = centers0[:, 0]\n        y0 = centers0[:, 1]\n        r0 = 0.08 * np.minimum.reduce([x0, y0, 1 - x0, 1 - y0])\n        z = np.concatenate([x0, y0, r0])\n\n        # Penalty annealing\n        for P in P_schedule:\n            res = minimize(\n                fun=lambda zz: _objective_and_grad(zz, N, P),\n                x0=z,\n                method='L-BFGS-B',\n                jac=True,\n                bounds=bounds,\n                options={'maxiter': maxiter_stage, 'ftol': 1e-12, 'gtol': 1e-8}\n            )\n            z = res.x\n            \n            # Conservative LP update\n            centers_tmp = np.stack([z[:N], z[N:2*N]], axis=1)\n            r_cur = np.clip(z[2*N:], 0.0, 0.5)\n            r_cap = 1.02 * r_cur\n            r_lp = _lp_feasibilize(centers_tmp, r_max=r_cap, eps_pair=1e-6)\n            z[2*N:] = np.clip(r_lp, 0.0, 0.5)\n\n        # Final LP without cap\n        centers_cur = np.stack([z[:N], z[N:2*N]], axis=1)\n        r_final = _lp_feasibilize(centers_cur, r_max=None, eps_pair=1e-6)\n\n        if _verify(centers_cur, r_final, tol_pair=1e-6):\n            s = float(np.sum(r_final))\n            if s > best['sum']:\n                best['centers'] = centers_cur.copy()\n                best['radii'] = r_final.copy()\n                best['sum'] = s\n\n    # Fallback\n    if best['centers'] is None:\n        centers_fb = _hex_initial_centers(N, margin=0.06, jitter_scale=0.02, seed=42)\n        r_fb = _lp_feasibilize(centers_fb, r_max=None, eps_pair=1e-6)\n        best['centers'] = centers_fb\n        best['radii'] = r_fb\n        best['sum'] = float(np.sum(r_fb))\n\n    # Final strict feasibilization\n    centers = best['centers']\n    radii = _lp_feasibilize(centers, r_max=None, eps_pair=1.1e-6)\n    \n    if not _verify(centers, radii, tol_pair=1e-6):\n        d, _, _ = _pairwise_distances(centers)\n        denom = radii[:, None] + radii[None, :]\n        with np.errstate(divide='ignore', invalid='ignore'):\n            ratio = np.where(denom > 0, (d - 1e-6) / denom, np.inf)\n        iu = np.triu_indices(N, 1)\n        s_pairs = np.min(ratio[iu]) if ratio[iu].size > 0 else 1.0\n        s_pairs = max(0.0, min(1.0, s_pairs if np.isfinite(s_pairs) else 1.0))\n        radii = s_pairs * radii\n        \n        bx = np.minimum.reduce([centers[:,0], centers[:,1], 1-centers[:,0], 1-centers[:,1]])\n        scale_b = np.min(np.where(radii > 0, bx / radii, 1.0))\n        scale_b = min(1.0, scale_b if np.isfinite(scale_b) else 1.0)\n        radii = scale_b * radii\n\n    radii = np.clip(radii, 0.0, 0.5)\n    sum_radii = float(np.sum(radii))\n    return centers.astype(float), radii.astype(float), sum_radii",
  "metrics": {
    "valid": true,
    "score": 2.612632185774156,
    "eval_time": 1.1258208751678467,
    "error": null
  },
  "prompt": "\nYou are designing an algorithm to pack 26 non-overlapping circles into [0,1]x[0,1] maximizing sum of radii.\n\nRequirements:\n- Provide a single Python code block only, no prose.\n- Allowed modules: numpy, scipy\n- Implement function: construct_packing()\n- Optional: run_packing()\n- Return: (centers, radii, sum_radii)\n  - centers: np.array shape (26,2)\n  - radii:   np.array shape (26,)\n  - sum_radii: float\n- All circles must be strictly inside the unit square with non-overlap (tolerance 1e-6).\n- Exactly 26 circles; all radii >= 0.\n- Runtime <= 300s.\n\nCritical implementation notes:\n- Use analytic, fully vectorized gradients where applicable.\n- For distance-based penalties, apply epsilon only in divisions (avoid adding eps in the penalty itself).\n- Always finalize with a Linear Program to enforce feasibility and maximize sum of radii:\n  Maximize sum(r_i) subject to:\n    r_i >= 0\n    r_i <= min(current_r_i, x_i, y_i, 1-x_i, 1-y_i)\n    r_i + r_j <= dist_ij - 1e-6 for all i<j\n- If intermediate methods produce infeasible centers or radii, the LP must return a strictly feasible solution.\n\nScoring target:\n- Known best ~2.635. Aim to exceed 2.62+ reliably.\n\nCombine these two approaches:\n# A (trial_2_0): penalty-annealed L-BFGS-B on centers+radii\n# B (trial_2_4): force-directed centers + LP\n\nHybrid plan:\n- Run a brief force-directed phase to spread centers.\n- Switch to penalty-annealed L-BFGS-B on (centers,radii) with analytic gradients.\n- Multi-start across 3 seeds; pick best.\n- Always finalize with LP feasibilization.\n- Ensure strict feasibility and shape correctness.\n\n\nImplement an LP feasibilizer function:\n- Input: centers (N,2), candidate radii (N,) or upper bounds (N,)\n- Build pairwise constraints r_i + r_j <= d_ij - 1e-6, where d_ij = ||c_i - c_j||_2\n- Bound by box: r_i <= x_i, y_i, 1-x_i, 1-y_i\n- Also enforce r_i <= r_ub_i (from candidate) if provided\n- Use scipy.optimize.linprog to maximize sum(r_i)\n- Return strictly feasible radii; on failure, back off margins slightly (e.g., add 1e-6 slack in constraints)\n\nReferences to integrate and improve:\n# Approach A:\nimport numpy as np\nfrom scipy.optimize import minimize, linprog\n\ndef _pairwise_distances(centers):\n    diff = centers[:, None, :] - centers[None, :, :]\n    d = np.sqrt(np.sum(diff**2, axis=2))\n    return d, diff[..., 0], diff[..., 1]\n\ndef _objective_and_grad(z, N, P):\n    # z = [x(0..N-1), y(0..N-1), r(0..N-1)]\n    x = z[:N]\n    y = z[N:2*N]\n    r = z[2*N:]\n    tiny = 1e-12\n\n    # Boundary hinges s = r - x, r - y, r - (1-x), r - (1-y)\n    s1 = r - x\n    s2 = r - y\n    s3 = r + x - 1.0\n    s4 = r + y - 1.0\n\n    h1 = np.maximum(0.0, s1)\n    h2 = np.maximum(0.0, s2)\n    h3 = np.maximum(0.0, s3)\n    h4 = np.maximum(0.0, s4)\n\n    pen_b = 0.5 * np.sum(h1*h1 + h2*h2 + h3*h3 + h4*h4)\n\n    # Overlap hinges\n    centers = np.stack([x, y], axis=1)\n    d, dx_mat, dy_mat = _pairwise_distances(centers)\n    d_safe = np.maximum(d, tiny)\n    hinge_full = r[:, None] + r[None, :] - d\n    H = np.triu(np.maximum(0.0, hinge_full), 1)\n\n    pen_o = 0.5 * np.sum(H*H)\n\n    # Objective\n    f = -np.sum(r) + P * (pen_b + pen_o)\n\n    # Gradients for boundary penalties: grad 0.5*h^2 = h * grad(arg)\n    dr_b = h1 + h2 + h3 + h4\n    dx_b = -h1 + h3\n    dy_b = -h2 + h4\n\n    # Overlap gradients\n    Qx = H * (dx_mat / d_safe)\n    Qy = H * (dy_mat / d_safe)\n    # For variable i: -sum_j Q[i,j] + sum_k Q[k,i]\n    dx_o = -np.sum(Qx, axis=1) + np.sum(Qx, axis=0)\n    dy_o = -np.sum(Qy, axis=1) + np.sum(Qy, axis=0)\n    dr_o = np.sum(H, axis=1) + np.sum(H, axis=0)\n\n    dx_total = P * (dx_b + dx_o)\n    dy_total = P * (dy_b + dy_o)\n    dr_total = -1.0 + P * (dr_b + dr_o)\n\n    grad = np.concatenate([dx_total, dy_total, dr_total])\n    return f, grad\n\ndef _lp_feasibilize(centers, r_max=None, eps_pair=1e-6):\n    \"\"\"\n    Maximize sum(r) subject to:\n      r_i >= 0\n      r_i <= r_max[i] (if provided)\n      r_i <= x_i, r_i <= y_i, r_i <= 1-x_i, r_i <= 1-y_i\n      r_i + r_j <= dist(c_i,c_j) - eps_pair\n    Use linprog highs. If infeasible with eps_pair, retry with eps_pair=0 and then scale radii down.\n    \"\"\"\n    N = centers.shape[0]\n    x = centers[:, 0]\n    y = centers[:, 1]\n\n    # Build A_ub, b_ub\n    A_rows = []\n    b_vals = []\n\n    # Boundary constraints\n    for i in range(N):\n        e = np.zeros(N)\n        e[i] = 1.0\n        A_rows.append(e.copy()); b_vals.append(x[i])\n        A_rows.append(e.copy()); b_vals.append(y[i])\n        A_rows.append(e.copy()); b_vals.append(1.0 - x[i])\n        A_rows.append(e.copy()); b_vals.append(1.0 - y[i])\n\n    # Pairwise constraints\n    d, _, _ = _pairwise_distances(centers)\n    for i in range(N):\n        for j in range(i+1, N):\n            e = np.zeros(N)\n            e[i] = 1.0; e[j] = 1.0\n            A_rows.append(e)\n            b_vals.append(d[i, j] - eps_pair)\n\n    # Optional upper bounds r_i <= r_max[i]\n    if r_max is not None:\n        for i in range(N):\n            e = np.zeros(N)\n            e[i] = 1.0\n            A_rows.append(e)\n            b_vals.append(r_max[i])\n\n    A_ub = np.array(A_rows)\n    b_ub = np.array(b_vals)\n\n    # Bounds on r: [0, 0.5]\n    bounds = [(0.0, 0.5) for _ in range(N)]\n    c = -np.ones(N)\n\n    res = linprog(c, A_ub=A_ub, b_ub=b_ub, bounds=bounds, method='highs')\n    if res.success:\n        r = res.x\n        return r\n\n    # Retry with eps_pair = 0 to get a feasible base, then scale to satisfy eps_pair\n    b_vals2 = []\n    A_rows2 = []\n    for i in range(N):\n        e = np.zeros(N); e[i] = 1.0\n        A_rows2.append(e.copy()); b_vals2.append(x[i])\n        A_rows2.append(e.copy()); b_vals2.append(y[i])\n        A_rows2.append(e.copy()); b_vals2.append(1.0 - x[i])\n        A_rows2.append(e.copy()); b_vals2.append(1.0 - y[i])\n\n    for i in range(N):\n        for j in range(i+1, N):\n            e = np.zeros(N); e[i] = 1.0; e[j] = 1.0\n            A_rows2.append(e); b_vals2.append(d[i, j])\n\n    if r_max is not None:\n        for i in range(N):\n            e = np.zeros(N); e[i] = 1.0\n            A_rows2.append(e); b_vals2.append(r_max[i])\n\n    A_ub2 = np.array(A_rows2)\n    b_ub2 = np.array(b_vals2)\n\n    res2 = linprog(c, A_ub=A_ub2, b_ub=b_ub2, bounds=bounds, method='highs')\n    if not res2.success:\n        return np.zeros(N)\n\n    r_base = res2.x\n    # Scale radii to satisfy stricter pairwise with eps_pair\n    denom = r_base[:, None] + r_base[None, :]\n    with np.errstate(divide='ignore', invalid='ignore'):\n        ratio = np.where(denom > 0, (d - eps_pair) / denom, np.inf)\n    iu = np.triu_indices(N, 1)\n    if ratio[iu].size > 0:\n        s_pairs = np.min(ratio[iu])\n        if not np.isfinite(s_pairs):\n            s_pairs = 1.0\n        s = min(1.0, s_pairs)\n    else:\n        s = 1.0\n    s = max(0.0, s)\n    r = s * r_base\n    return r\n\ndef _verify(centers, radii, tol_pair=1e-6):\n    N = len(radii)\n    x = centers[:, 0]\n    y = centers[:, 1]\n    if np.any(radii < -1e-12):\n        return False\n    if np.any(x - radii < -1e-9):\n        return False\n    if np.any(y - radii < -1e-9):\n        return False\n    if np.any(x + radii > 1.0 + 1e-9):\n        return False\n    if np.any(y + radii > 1.0 + 1e-9):\n        return False\n    d, _, _ = _pairwise_distances(centers)\n    iu = np.triu_indices(N, 1)\n    lhs = radii[:, None] + radii[None, :]\n    viol = lhs[iu] - d[iu] + tol_pair\n    if np.any(viol > 1e-12):\n        return False\n    return True\n\ndef _hex_initial_centers(N, margin=0.04, jitter_scale=0.04, seed=0):\n    # Fixed row pattern to sum 26\n    rows = [6, 5, 6, 5, 4]\n    assert sum(rows) == 26\n    max_cols = max(rows)\n    nrows = len(rows)\n    dx = (1.0 - 2*margin) / (max_cols - 1)\n    dy = (1.0 - 2*margin) / (nrows - 1) if nrows > 1 else 0.0\n    centers = []\n    rng = np.random.default_rng(seed)\n    for ri, cols in enumerate(rows):\n        y = margin + ri * dy\n        if cols == max_cols:\n            startx = margin\n        else:\n            startx = margin + 0.5 * dx\n        for k in range(cols):\n            x = startx + k * dx\n            centers.append([x, y])\n    centers = np.array(centers)\n    # Jitter\n    jitter = rng.normal(scale=jitter_scale, size=centers.shape)\n    centers = np.clip(centers + jitter, 1e-3, 1 - 1e-3)\n    return centers\n\ndef _multi_start_inits(N):\n    inits = []\n    params = [\n        (0.03, 0.02, 0),\n        (0.05, 0.03, 1),\n        (0.07, 0.03, 2),\n        (0.04, 0.04, 3),\n        (0.06, 0.02, 4),\n        (0.05, 0.04, 5),\n        (0.03, 0.03, 6),\n        (0.07, 0.02, 7),\n    ]\n    for m, j, s in params:\n        inits.append(_hex_initial_centers(N, margin=m, jitter_scale=j, seed=s))\n    return inits\n\ndef construct_packing():\n    N = 26\n    best = {'centers': None, 'radii': None, 'sum': -np.inf}\n\n    # Optimization settings\n    P_schedule = [10.0, 100.0, 1000.0, 5000.0]\n    maxiter_stage = 120\n    bounds = [(0.0, 1.0)] * (2*N) + [(0.0, 0.6)] * N\n\n    # Initializations\n    inits = _multi_start_inits(N)\n\n    for centers0 in inits:\n        # Initial radii small, within boundary\n        x0 = centers0[:, 0]\n        y0 = centers0[:, 1]\n        r0 = 0.05 * np.minimum.reduce([x0, y0, 1 - x0, 1 - y0])\n        z = np.concatenate([x0, y0, r0])\n\n        # Annealing optimization with LP tightening\n        for P in P_schedule:\n            res = minimize(\n                fun=lambda zz: _objective_and_grad(zz, N, P),\n                x0=z,\n                method='L-BFGS-B',\n                jac=True,\n                bounds=bounds,\n                options={'maxiter': maxiter_stage, 'ftol': 1e-12, 'gtol': 1e-8, 'maxcor': 20}\n            )\n            z = res.x\n            centers_tmp = np.stack([z[:N], z[N:2*N]], axis=1)\n            # Conservative LP update, cap by slight expansion over current radii\n            r_cur = np.clip(z[2*N:], 0.0, 0.5)\n            r_cap = 1.05 * r_cur\n            r_lp = _lp_feasibilize(centers_tmp, r_max=r_cap, eps_pair=1e-6)\n            z[2*N:] = np.clip(r_lp, 0.0, 0.5)\n\n        # Final LP without cap to fully utilize geometry\n        centers_cur = np.stack([z[:N], z[N:2*N]], axis=1)\n        r_final = _lp_feasibilize(centers_cur, r_max=None, eps_pair=1e-6)\n\n        # Verify and update best\n        if _verify(centers_cur, r_final, tol_pair=1e-6):\n            s = float(np.sum(r_final))\n            if s > best['sum']:\n                best['centers'] = centers_cur.copy()\n                best['radii'] = r_final.copy()\n                best['sum'] = s\n        else:\n            # Try slight shrink and re-LP to fix tiny violations\n            d, _, _ = _pairwise_distances(centers_cur)\n            denom = r_final[:, None] + r_final[None, :]\n            with np.errstate(divide='ignore', invalid='ignore'):\n                ratio = np.where(denom > 0, (d - 1.0e-6) / denom, np.inf)\n            iu = np.triu_indices(N, 1)\n            s_pairs = np.min(ratio[iu]) if ratio[iu].size > 0 else 1.0\n            if not np.isfinite(s_pairs):\n                s_pairs = 1.0\n            s_pairs = max(0.0, min(1.0, s_pairs))\n            r_try = s_pairs * r_final\n            # Boundaries scale check\n            bx = np.minimum.reduce([centers_cur[:,0], centers_cur[:,1], 1-centers_cur[:,0], 1-centers_cur[:,1]])\n            scale_b = np.min(np.where(r_try > 0, bx / r_try, 1.0))\n            scale_b = min(1.0, scale_b if np.isfinite(scale_b) else 1.0)\n            r_try = scale_b * r_try\n            if _verify(centers_cur, r_try, tol_pair=1e-6):\n                s = float(np.sum(r_try))\n                if s > best['sum']:\n                    best['centers'] = centers_cur.copy()\n                    best['radii'] = r_try.copy()\n                    best['sum'] = s\n\n    # Fallback if nothing verified\n    if best['centers'] is None:\n        centers_fb = _hex_initial_centers(N, margin=0.06, jitter_scale=0.02, seed=42)\n        r_fb = _lp_feasibilize(centers_fb, r_max=None, eps_pair=1e-6)\n        if not _verify(centers_fb, r_fb, tol_pair=1e-6):\n            r_fb = 0.95 * r_fb\n        best['centers'] = centers_fb\n        best['radii'] = r_fb\n        best['sum'] = float(np.sum(r_fb))\n\n    # Ensure strict feasibility and final verify\n    radii_strict = _lp_feasibilize(best['centers'], r_max=None, eps_pair=1.1e-6)\n    if _verify(best['centers'], radii_strict, tol_pair=1e-6):\n        radii = radii_strict\n    else:\n        radii = best['radii']\n\n    centers = best['centers']\n    # Final hardening: if any violation persists, scale radii conservatively\n    if not _verify(centers, radii, tol_pair=1e-6):\n        d, _, _ = _pairwise_distances(centers)\n        denom = radii[:, None] + radii[None, :]\n        with np.errstate(divide='ignore', invalid='ignore'):\n            ratio = np.where(denom > 0, (d - 1e-6) / denom, np.inf)\n        iu = np.triu_indices(N, 1)\n        s_pairs = np.min(ratio[iu]) if ratio[iu].size > 0 else 1.0\n        s_pairs = min(1.0, s_pairs if np.isfinite(s_pairs) else 1.0)\n        s_pairs = max(0.0, s_pairs)\n        radii = s_pairs * radii\n        bx = np.minimum.reduce([centers[:,0], centers[:,1], 1-centers[:,0], 1-centers[:,1]])\n        scale_b = np.min(np.where(radii > 0, bx / radii, 1.0))\n        scale_b = min(1.0, scale_b if np.isfinite(scale_b) else 1.0)\n        radii = scale_b * radii\n\n    # Clip tiny negatives and ensure within [0,0.5]\n    radii = np.clip(radii, 0.0, 0.5)\n    sum_radii = float(np.sum(radii))\n    return centers.astype(float), radii.astype(float), sum_radii\n\ndef run_packing():\n    centers, radii, sum_radii = construct_packing()\n    return centers, radii, sum_radii\n\n# Approach B:\nimport numpy as np\nfrom scipy.optimize import minimize, linprog\n\ndef _pairwise_distances(centers):\n    diff = centers[:, None, :] - centers[None, :, :]\n    d = np.sqrt(np.sum(diff**2, axis=2))\n    return d, diff[..., 0], diff[..., 1]\n\ndef _objective_and_grad(z, N, P):\n    # z = [x(0..N-1), y(0..N-1), r(0..N-1)]\n    x = z[:N]\n    y = z[N:2*N]\n    r = z[2*N:]\n    tiny = 1e-12\n\n    # Boundary hinges: h = max(0, violation)\n    h1 = np.maximum(0.0, r - x)        # left\n    h2 = np.maximum(0.0, r - y)        # bottom\n    h3 = np.maximum(0.0, r + x - 1.0)  # right\n    h4 = np.maximum(0.0, r + y - 1.0)  # top\n\n    pen_b = np.sum(h1*h1 + h2*h2 + h3*h3 + h4*h4)\n\n    # Pairwise overlap hinges\n    centers = np.stack([x, y], axis=1)\n    d, dx_mat, dy_mat = _pairwise_distances(centers)\n    d_safe = np.maximum(d, tiny)\n    # hinge = max(0, r_i + r_j - d_ij)\n    rr = r[:, None] + r[None, :]\n    hinge = np.maximum(0.0, rr - d)\n    H = np.triu(hinge, 1)  # only upper triangle to avoid double counting\n    pen_o = np.sum(H*H)\n\n    # Objective: maximize sum(r) -> minimize -sum(r)\n    f = -np.sum(r) + P * (pen_b + pen_o)\n\n    # Gradients\n    # Boundary gradients\n    dr_b = 2.0 * (h1 + h2 + h3 + h4)\n    dx_b = -2.0 * h1 + 2.0 * h3\n    dy_b = -2.0 * h2 + 2.0 * h4\n\n    # Overlap gradients\n    # For distances, safe division only\n    with np.errstate(divide='ignore', invalid='ignore'):\n        invd = 1.0 / d_safe\n        fx = dx_mat * invd\n        fy = dy_mat * invd\n\n    # Qx = H * (dx_mat / d_safe), Qy similar\n    Qx = H * fx\n    Qy = H * fy\n    # dx_o for i: -2*sum_j Q[i,j] + 2*sum_k Q[k,i]\n    dx_o = -2.0 * np.sum(Qx, axis=1) + 2.0 * np.sum(Qx, axis=0)\n    dy_o = -2.0 * np.sum(Qy, axis=1) + 2.0 * np.sum(Qy, axis=0)\n    # dr_o: each H(i,j) contributes 2*H to both r_i and r_j\n    dr_o = 2.0 * (np.sum(H, axis=1) + np.sum(H, axis=0))\n\n    dx_total = P * dx_b + P * dx_o\n    dy_total = P * dy_b + P * dy_o\n    dr_total = -1.0 + P * dr_b + P * dr_o\n\n    grad = np.concatenate([dx_total, dy_total, dr_total])\n    return f, grad\n\ndef _lp_feasibilize(centers, r_max=None, eps_pair=1e-6):\n    \"\"\"\n    Maximize sum(r) subject to:\n      r_i >= 0\n      r_i <= r_max[i] (if provided)\n      r_i <= x_i, r_i <= y_i, r_i <= 1-x_i, r_i <= 1-y_i\n      r_i + r_j <= dist(c_i,c_j) - eps_pair\n    Uses linprog with method 'highs'. Retries with eps_pair=0 if needed and scales.\n    \"\"\"\n    N = centers.shape[0]\n    x = centers[:, 0].astype(float)\n    y = centers[:, 1].astype(float)\n\n    # Build A_ub, b_ub\n    A_rows = []\n    b_vals = []\n\n    # Boundary constraints r_i <= x_i, y_i, 1-x_i, 1-y_i\n    for i in range(N):\n        e = np.zeros(N, dtype=float)\n        e[i] = 1.0\n        A_rows.append(e.copy()); b_vals.append(x[i])\n        A_rows.append(e.copy()); b_vals.append(y[i])\n        A_rows.append(e.copy()); b_vals.append(1.0 - x[i])\n        A_rows.append(e.copy()); b_vals.append(1.0 - y[i])\n\n    # Pairwise constraints\n    d, _, _ = _pairwise_distances(centers)\n    for i in range(N):\n        for j in range(i+1, N):\n            e = np.zeros(N, dtype=float)\n            e[i] = 1.0; e[j] = 1.0\n            A_rows.append(e)\n            b_vals.append(d[i, j] - eps_pair)\n\n    # Optional upper bounds r_i <= r_max[i]\n    if r_max is not None:\n        for i in range(N):\n            e = np.zeros(N, dtype=float)\n            e[i] = 1.0\n            A_rows.append(e)\n            b_vals.append(float(r_max[i]))\n\n    if len(A_rows) == 0:\n        A_ub = None\n        b_ub = None\n    else:\n        A_ub = np.array(A_rows, dtype=float)\n        b_ub = np.array(b_vals, dtype=float)\n\n    # Bounds on r: [0, 0.6]\n    bounds = [(0.0, 0.6) for _ in range(N)]\n    c = -np.ones(N, dtype=float)  # maximize sum -> minimize -sum\n\n    res = linprog(c, A_ub=A_ub, b_ub=b_ub, bounds=bounds, method='highs')\n    if res.success:\n        r = res.x\n        return r\n\n    # Retry with eps_pair = 0 to get feasible base, then scale down to satisfy eps_pair\n    # Build without eps\n    A_rows2 = []\n    b_vals2 = []\n    for i in range(N):\n        e = np.zeros(N, dtype=float); e[i] = 1.0\n        A_rows2.append(e.copy()); b_vals2.append(x[i])\n        A_rows2.append(e.copy()); b_vals2.append(y[i])\n        A_rows2.append(e.copy()); b_vals2.append(1.0 - x[i])\n        A_rows2.append(e.copy()); b_vals2.append(1.0 - y[i])\n\n    for i in range(N):\n        for j in range(i+1, N):\n            e = np.zeros(N, dtype=float); e[i] = 1.0; e[j] = 1.0\n            A_rows2.append(e); b_vals2.append(d[i, j])  # no epsilon\n\n    if r_max is not None:\n        for i in range(N):\n            e = np.zeros(N, dtype=float); e[i] = 1.0\n            A_rows2.append(e); b_vals2.append(float(r_max[i]))\n\n    A_ub2 = np.array(A_rows2, dtype=float)\n    b_ub2 = np.array(b_vals2, dtype=float)\n\n    res2 = linprog(c, A_ub=A_ub2, b_ub=b_ub2, bounds=bounds, method='highs')\n    if not res2.success:\n        # As last resort, attempt trivial feasible vector: small radii respecting boundaries and pairwise\n        # Start with min(boundary) then shrink to avoid overlaps\n        r_try = np.minimum.reduce([x, y, 1-x, 1-y]) * 0.1\n        r_try = np.clip(r_try, 0.0, 0.6)\n        # If violates pairwise, shrink uniformly\n        denom = r_try[:, None] + r_try[None, :]\n        with np.errstate(divide='ignore', invalid='ignore'):\n            ratio = np.where(denom > 0, (d - eps_pair) / denom, np.inf)\n        iu = np.triu_indices(N, 1)\n        s_pairs = np.min(ratio[iu]) if ratio[iu].size > 0 else 1.0\n        if not np.isfinite(s_pairs):\n            s_pairs = 0.0\n        s_pairs = max(0.0, min(1.0, s_pairs))\n        r_try = s_pairs * r_try\n        return r_try\n\n    r_base = res2.x\n    # Scale radii to satisfy stricter pairwise with eps_pair\n    s = 1.0\n    denom = r_base[:, None] + r_base[None, :]\n    with np.errstate(divide='ignore', invalid='ignore'):\n        ratio = np.where(denom > 0, (d - eps_pair) / denom, np.inf)\n    iu = np.triu_indices(N, 1)\n    if ratio[iu].size > 0:\n        s_pairs = np.min(ratio[iu])\n        if not np.isfinite(s_pairs):\n            s_pairs = 1.0\n        s = min(1.0, s_pairs)\n    s = max(0.0, s)\n    r = s * r_base\n    return r\n\ndef _verify(centers, radii, tol_pair=1e-6):\n    N = len(radii)\n    x = centers[:, 0]\n    y = centers[:, 1]\n    if np.any(radii < -1e-12):\n        return False\n    if np.any(x - radii < -1e-9):\n        return False\n    if np.any(y - radii < -1e-9):\n        return False\n    if np.any(x + radii > 1.0 + 1e-9):\n        return False\n    if np.any(y + radii > 1.0 + 1e-9):\n        return False\n    d, _, _ = _pairwise_distances(centers)\n    iu = np.triu_indices(N, 1)\n    lhs = radii[:, None] + radii[None, :]\n    viol = lhs[iu] - d[iu] + tol_pair\n    if np.any(viol > 1e-12):\n        return False\n    return True\n\ndef _hex_initial_centers(N, margin=0.03, seed=0):\n    # Arrange rows to sum to 26\n    rows = [6,5,6,5,4]\n    assert sum(rows) == N\n    max_cols = max(rows)\n    nrows = len(rows)\n    dx = (1.0 - 2*margin) / (max_cols - 1) if max_cols > 1 else 1.0\n    dy = (1.0 - 2*margin) / (nrows - 1) if nrows > 1 else 1.0\n    centers = []\n    rng = np.random.default_rng(seed)\n    for ri, cols in enumerate(rows):\n        y = margin + ri * dy\n        if cols == max_cols:\n            startx = margin\n        else:\n            startx = margin + 0.5 * dx\n        for k in range(cols):\n            x = startx + k * dx\n            centers.append([x, y])\n    centers = np.array(centers, dtype=float)\n    # Jitter via Latin hypercube style small random shift\n    jitter_scale = min(dx, dy) * 0.12 if nrows > 1 else 0.02\n    jitter = rng.normal(scale=jitter_scale, size=centers.shape)\n    centers = np.clip(centers + jitter, 1e-5, 1.0 - 1e-5)\n    return centers\n\ndef _lhs_initial_centers(N, seed=0):\n    rng = np.random.default_rng(seed)\n    # Latin hypercube sampling in 2D\n    cut = np.linspace(0, 1, N+1)\n    u = rng.uniform(size=N)\n    a = cut[:N] + u*(cut[1]-cut[0])  # incorrect for full LHS - better do permuted per dim\n    # Better LHS: generate N strata per dim and permute\n    xs = (np.arange(N) + rng.uniform(size=N)) / N\n    ys = (np.arange(N) + rng.uniform(size=N)) / N\n    rng.shuffle(xs); rng.shuffle(ys)\n    centers = np.vstack([xs, ys]).T\n    # Move slightly inward\n    centers = np.clip(centers, 0.02, 0.98)\n    return centers\n\ndef construct_packing():\n    N = 26\n    best = {'centers': None, 'radii': None, 'sum': -np.inf}\n\n    # Optimization settings\n    P_schedule = [20.0, 200.0, 1500.0]\n    maxiter_stage = 300\n    bounds = [(0.0, 1.0)] * (2*N) + [(0.0, 0.6)] * N\n\n    # Prepare initial center sets: a few hex variants + several random/LHS seeds\n    inits = []\n    inits.append(_hex_initial_centers(N, margin=0.03, seed=0))\n    inits.append(_hex_initial_centers(N, margin=0.06, seed=1))\n    inits.append(_hex_initial_centers(N, margin=0.08, seed=2))\n    # Add LHS/random initializations\n    for s in range(9):\n        if s % 2 == 0:\n            inits.append(_lhs_initial_centers(N, seed=10+s))\n        else:\n            rng = np.random.default_rng(20 + s)\n            c = rng.uniform(0.06, 0.94, size=(N, 2))\n            inits.append(c)\n\n    # Ensure exactly 12 inits (3 hex + 9 random = 12)\n    inits = inits[:12]\n\n    for centers0 in inits:\n        # Initial radii small respecting boundaries\n        x0 = centers0[:, 0].astype(float)\n        y0 = centers0[:, 1].astype(float)\n        r0 = np.minimum.reduce([x0, y0, 1 - x0, 1 - y0]) * 0.15\n        z = np.concatenate([x0, y0, r0])\n\n        # Multi-stage penalty optimization\n        for P in P_schedule:\n            res = minimize(\n                fun=lambda zz: _objective_and_grad(zz, N, P),\n                x0=z,\n                method='L-BFGS-B',\n                jac=True,\n                bounds=bounds,\n                options={'maxiter': maxiter_stage, 'ftol': 1e-12, 'gtol': 1e-8, 'maxcor': 20}\n            )\n            z = res.x\n            # After each stage, LP-feasibilize but do not allow radii to grow beyond current (conservative)\n            centers_tmp = np.stack([z[:N], z[N:2*N]], axis=1)\n            r_current = np.clip(z[2*N:], 0.0, 0.6)\n            r_lp = _lp_feasibilize(centers_tmp, r_max=r_current, eps_pair=1e-6)\n            z[2*N:] = np.clip(r_lp, 0.0, 0.6)\n\n        centers_cur = np.stack([z[:N], z[N:2*N]], axis=1)\n        # Final LP: allow growth beyond current radii to tighten optimum\n        r_final = _lp_feasibilize(centers_cur, r_max=None, eps_pair=1e-6)\n\n        # If LP returns infeasible zeros or violates, attempt small uniform shrink and retry\n        if not _verify(centers_cur, r_final, tol_pair=1e-6):\n            # Try shrink current radii slightly and re-run LP\n            r_try = np.clip(r_final, 0.0, 0.6)\n            if np.sum(r_try) <= 0:\n                # scale down slightly from z radii\n                r_try = np.clip(z[2*N:] * 0.95, 0.0, 0.6)\n            r_try2 = _lp_feasibilize(centers_cur, r_max=r_try, eps_pair=1e-6)\n            if _verify(centers_cur, r_try2, tol_pair=1e-6):\n                r_final = r_try2\n            else:\n                # As last resort, scale radii until feasible\n                d, _, _ = _pairwise_distances(centers_cur)\n                denom = r_try2[:, None] + r_try2[None, :]\n                with np.errstate(divide='ignore', invalid='ignore'):\n                    ratio = np.where(denom > 0, (d - 1e-6) / denom, np.inf)\n                iu = np.triu_indices(N, 1)\n                s_pairs = np.min(ratio[iu]) if ratio[iu].size > 0 else 1.0\n                if not np.isfinite(s_pairs):\n                    s_pairs = 0.0\n                s_pairs = max(0.0, min(1.0, s_pairs))\n                r_final = np.clip(s_pairs * r_try2, 0.0, 0.6)\n\n        # Final verification and acceptance\n        if _verify(centers_cur, r_final, tol_pair=1e-6):\n            s = float(np.sum(r_final))\n            if s > best['sum']:\n                best['centers'] = centers_cur.copy()\n                best['radii'] = r_final.copy()\n                best['sum'] = s\n\n    # Fallback if nothing verified\n    if best['centers'] is None:\n        centers_fb = _hex_initial_centers(N, margin=0.08, seed=42)\n        r_fb = _lp_feasibilize(centers_fb, r_max=None, eps_pair=1e-6)\n        if not _verify(centers_fb, r_fb, tol_pair=1e-6):\n            r_fb = 0.9 * r_fb\n        best['centers'] = centers_fb\n        best['radii'] = r_fb\n        best['sum'] = float(np.sum(r_fb))\n\n    # One final tightening LP with slightly stricter eps; if fails, use previous best\n    r_strict = _lp_feasibilize(best['centers'], r_max=None, eps_pair=1.1e-6)\n    if _verify(best['centers'], r_strict, tol_pair=1e-6):\n        radii = r_strict\n    else:\n        radii = best['radii']\n\n    centers = best['centers'].astype(float)\n\n    # Final safety: ensure strict non-overlap with 1e-6 by uniform shrink if needed\n    if not _verify(centers, radii, tol_pair=1e-6):\n        d, _, _ = _pairwise_distances(centers)\n        denom = radii[:, None] + radii[None, :]\n        with np.errstate(divide='ignore', invalid='ignore'):\n            ratio = np.where(denom > 0, (d - 1e-6) / denom, np.inf)\n        iu = np.triu_indices(N, 1)\n        s_pairs = np.min(ratio[iu]) if ratio[iu].size > 0 else 1.0\n        if not np.isfinite(s_pairs):\n            s_pairs = 0.0\n        s_pairs = max(0.0, min(1.0, s_pairs))\n        radii = s_pairs * radii\n        # also ensure boundary\n        bx = np.minimum.reduce([centers[:,0], centers[:,1], 1-centers[:,0], 1-centers[:,1]])\n        with np.errstate(divide='ignore', invalid='ignore'):\n            scale_b = np.min(np.where(radii > 0, bx / radii, np.inf))\n        if not np.isfinite(scale_b):\n            scale_b = 1.0\n        scale_b = max(0.0, min(1.0, scale_b))\n        radii = np.clip(scale_b * radii, 0.0, 0.6)\n\n    sum_radii = float(np.sum(radii))\n    return centers.astype(float), radii.astype(float), sum_radii\n\ndef run_packing():\n    return construct_packing()\n",
  "response": "```python\nimport numpy as np\nfrom scipy.optimize import minimize, linprog\n\ndef _pairwise_distances(centers):\n    diff = centers[:, None, :] - centers[None, :, :]\n    d = np.sqrt(np.sum(diff**2, axis=2))\n    return d, diff[..., 0], diff[..., 1]\n\ndef _objective_and_grad(z, N, P):\n    x = z[:N]\n    y = z[N:2*N]\n    r = z[2*N:]\n    tiny = 1e-12\n\n    # Boundary hinges\n    h1 = np.maximum(0.0, r - x)\n    h2 = np.maximum(0.0, r - y)\n    h3 = np.maximum(0.0, r + x - 1.0)\n    h4 = np.maximum(0.0, r + y - 1.0)\n    pen_b = 0.5 * np.sum(h1*h1 + h2*h2 + h3*h3 + h4*h4)\n\n    # Overlap hinges\n    centers = np.stack([x, y], axis=1)\n    d, dx_mat, dy_mat = _pairwise_distances(centers)\n    d_safe = np.maximum(d, tiny)\n    hinge_full = r[:, None] + r[None, :] - d\n    H = np.triu(np.maximum(0.0, hinge_full), 1)\n    pen_o = 0.5 * np.sum(H*H)\n\n    f = -np.sum(r) + P * (pen_b + pen_o)\n\n    # Gradients\n    dr_b = h1 + h2 + h3 + h4\n    dx_b = -h1 + h3\n    dy_b = -h2 + h4\n\n    Qx = H * (dx_mat / d_safe)\n    Qy = H * (dy_mat / d_safe)\n    dx_o = -np.sum(Qx, axis=1) + np.sum(Qx, axis=0)\n    dy_o = -np.sum(Qy, axis=1) + np.sum(Qy, axis=0)\n    dr_o = np.sum(H, axis=1) + np.sum(H, axis=0)\n\n    dx_total = P * (dx_b + dx_o)\n    dy_total = P * (dy_b + dy_o)\n    dr_total = -1.0 + P * (dr_b + dr_o)\n\n    grad = np.concatenate([dx_total, dy_total, dr_total])\n    return f, grad\n\ndef _lp_feasibilize(centers, r_max=None, eps_pair=1e-6):\n    N = centers.shape[0]\n    x = centers[:, 0]\n    y = centers[:, 1]\n\n    A_rows = []\n    b_vals = []\n\n    # Boundary constraints\n    for i in range(N):\n        e = np.zeros(N)\n        e[i] = 1.0\n        A_rows.append(e.copy()); b_vals.append(x[i])\n        A_rows.append(e.copy()); b_vals.append(y[i])\n        A_rows.append(e.copy()); b_vals.append(1.0 - x[i])\n        A_rows.append(e.copy()); b_vals.append(1.0 - y[i])\n\n    # Pairwise constraints\n    d, _, _ = _pairwise_distances(centers)\n    for i in range(N):\n        for j in range(i+1, N):\n            e = np.zeros(N)\n            e[i] = 1.0; e[j] = 1.0\n            A_rows.append(e)\n            b_vals.append(d[i, j] - eps_pair)\n\n    if r_max is not None:\n        for i in range(N):\n            e = np.zeros(N)\n            e[i] = 1.0\n            A_rows.append(e)\n            b_vals.append(r_max[i])\n\n    A_ub = np.array(A_rows)\n    b_ub = np.array(b_vals)\n    bounds = [(0.0, 0.5) for _ in range(N)]\n    c = -np.ones(N)\n\n    res = linprog(c, A_ub=A_ub, b_ub=b_ub, bounds=bounds, method='highs')\n    if res.success:\n        return res.x\n\n    # Retry with eps_pair = 0\n    b_vals2 = []\n    A_rows2 = []\n    for i in range(N):\n        e = np.zeros(N); e[i] = 1.0\n        A_rows2.append(e.copy()); b_vals2.append(x[i])\n        A_rows2.append(e.copy()); b_vals2.append(y[i])\n        A_rows2.append(e.copy()); b_vals2.append(1.0 - x[i])\n        A_rows2.append(e.copy()); b_vals2.append(1.0 - y[i])\n\n    for i in range(N):\n        for j in range(i+1, N):\n            e = np.zeros(N); e[i] = 1.0; e[j] = 1.0\n            A_rows2.append(e); b_vals2.append(d[i, j])\n\n    if r_max is not None:\n        for i in range(N):\n            e = np.zeros(N); e[i] = 1.0\n            A_rows2.append(e); b_vals2.append(r_max[i])\n\n    A_ub2 = np.array(A_rows2)\n    b_ub2 = np.array(b_vals2)\n\n    res2 = linprog(c, A_ub=A_ub2, b_ub=b_ub2, bounds=bounds, method='highs')\n    if not res2.success:\n        return np.zeros(N)\n\n    r_base = res2.x\n    denom = r_base[:, None] + r_base[None, :]\n    with np.errstate(divide='ignore', invalid='ignore'):\n        ratio = np.where(denom > 0, (d - eps_pair) / denom, np.inf)\n    iu = np.triu_indices(N, 1)\n    if ratio[iu].size > 0:\n        s_pairs = np.min(ratio[iu])\n        if not np.isfinite(s_pairs):\n            s_pairs = 1.0\n        s = min(1.0, s_pairs)\n    else:\n        s = 1.0\n    s = max(0.0, s)\n    return s * r_base\n\ndef _verify(centers, radii, tol_pair=1e-6):\n    N = len(radii)\n    x = centers[:, 0]\n    y = centers[:, 1]\n    if np.any(radii < -1e-12):\n        return False\n    if np.any(x - radii < -1e-9):\n        return False\n    if np.any(y - radii < -1e-9):\n        return False\n    if np.any(x + radii > 1.0 + 1e-9):\n        return False\n    if np.any(y + radii > 1.0 + 1e-9):\n        return False\n    d, _, _ = _pairwise_distances(centers)\n    iu = np.triu_indices(N, 1)\n    lhs = radii[:, None] + radii[None, :]\n    viol = lhs[iu] - d[iu] + tol_pair\n    if np.any(viol > 1e-12):\n        return False\n    return True\n\ndef _hex_initial_centers(N, margin=0.04, jitter_scale=0.04, seed=0):\n    rows = [6, 5, 6, 5, 4]\n    assert sum(rows) == 26\n    max_cols = max(rows)\n    nrows = len(rows)\n    dx = (1.0 - 2*margin) / (max_cols - 1)\n    dy = (1.0 - 2*margin) / (nrows - 1) if nrows > 1 else 0.0\n    centers = []\n    rng = np.random.default_rng(seed)\n    for ri, cols in enumerate(rows):\n        y = margin + ri * dy\n        if cols == max_cols:\n            startx = margin\n        else:\n            startx = margin + 0.5 * dx\n        for k in range(cols):\n            x = startx + k * dx\n            centers.append([x, y])\n    centers = np.array(centers)\n    jitter = rng.normal(scale=jitter_scale, size=centers.shape)\n    centers = np.clip(centers + jitter, 1e-3, 1 - 1e-3)\n    return centers\n\ndef _force_directed_phase(centers, n_iter=50, dt=0.001):\n    N = centers.shape[0]\n    for _ in range(n_iter):\n        forces = np.zeros_like(centers)\n        \n        # Pairwise repulsion\n        for i in range(N):\n            for j in range(i+1, N):\n                diff = centers[i] - centers[j]\n                dist = np.linalg.norm(diff)\n                if dist > 1e-12:\n                    force_mag = 0.01 / (dist**2 + 1e-6)\n                    force_dir = diff / dist\n                    forces[i] += force_mag * force_dir\n                    forces[j] -= force_mag * force_dir\n        \n        # Boundary repulsion\n        for i in range(N):\n            x, y = centers[i]\n            if x < 0.1:\n                forces[i, 0] += 0.005 / (x + 1e-6)\n            if x > 0.9:\n                forces[i, 0] -= 0.005 / (1.0 - x + 1e-6)\n            if y < 0.1:\n                forces[i, 1] += 0.005 / (y + 1e-6)\n            if y > 0.9:\n                forces[i, 1] -= 0.005 / (1.0 - y + 1e-6)\n        \n        centers += dt * forces\n        centers = np.clip(centers, 0.02, 0.98)\n    \n    return centers\n\ndef construct_packing():\n    N = 26\n    best = {'centers': None, 'radii': None, 'sum': -np.inf}\n\n    P_schedule = [20.0, 200.0, 2000.0]\n    maxiter_stage = 150\n    bounds = [(0.0, 1.0)] * (2*N) + [(0.0, 0.5)] * N\n\n    # Multiple initializations\n    inits = []\n    for seed in range(8):\n        centers0 = _hex_initial_centers(N, margin=0.03 + 0.01*seed, jitter_scale=0.02 + 0.01*seed, seed=seed)\n        # Force-directed spreading\n        centers0 = _force_directed_phase(centers0, n_iter=30)\n        inits.append(centers0)\n\n    for centers0 in inits:\n        x0 = centers0[:, 0]\n        y0 = centers0[:, 1]\n        r0 = 0.08 * np.minimum.reduce([x0, y0, 1 - x0, 1 - y0])\n        z = np.concatenate([x0, y0, r0])\n\n        # Penalty annealing\n        for P in P_schedule:\n            res = minimize(\n                fun=lambda zz: _objective_and_grad(zz, N, P),\n                x0=z,\n                method='L-BFGS-B',\n                jac=True,\n                bounds=bounds,\n                options={'maxiter': maxiter_stage, 'ftol': 1e-12, 'gtol': 1e-8}\n            )\n            z = res.x\n            \n            # Conservative LP update\n            centers_tmp = np.stack([z[:N], z[N:2*N]], axis=1)\n            r_cur = np.clip(z[2*N:], 0.0, 0.5)\n            r_cap = 1.02 * r_cur\n            r_lp = _lp_feasibilize(centers_tmp, r_max=r_cap, eps_pair=1e-6)\n            z[2*N:] = np.clip(r_lp, 0.0, 0.5)\n\n        # Final LP without cap\n        centers_cur = np.stack([z[:N], z[N:2*N]], axis=1)\n        r_final = _lp_feasibilize(centers_cur, r_max=None, eps_pair=1e-6)\n\n        if _verify(centers_cur, r_final, tol_pair=1e-6):\n            s = float(np.sum(r_final))\n            if s > best['sum']:\n                best['centers'] = centers_cur.copy()\n                best['radii'] = r_final.copy()\n                best['sum'] = s\n\n    # Fallback\n    if best['centers'] is None:\n        centers_fb = _hex_initial_centers(N, margin=0.06, jitter_scale=0.02, seed=42)\n        r_fb = _lp_feasibilize(centers_fb, r_max=None, eps_pair=1e-6)\n        best['centers'] = centers_fb\n        best['radii'] = r_fb\n        best['sum'] = float(np.sum(r_fb))\n\n    # Final strict feasibilization\n    centers = best['centers']\n    radii = _lp_feasibilize(centers, r_max=None, eps_pair=1.1e-6)\n    \n    if not _verify(centers, radii, tol_pair=1e-6):\n        d, _, _ = _pairwise_distances(centers)\n        denom = radii[:, None] + radii[None, :]\n        with np.errstate(divide='ignore', invalid='ignore'):\n            ratio = np.where(denom > 0, (d - 1e-6) / denom, np.inf)\n        iu = np.triu_indices(N, 1)\n        s_pairs = np.min(ratio[iu]) if ratio[iu].size > 0 else 1.0\n        s_pairs = max(0.0, min(1.0, s_pairs if np.isfinite(s_pairs) else 1.0))\n        radii = s_pairs * radii\n        \n        bx = np.minimum.reduce([centers[:,0], centers[:,1], 1-centers[:,0], 1-centers[:,1]])\n        scale_b = np.min(np.where(radii > 0, bx / radii, 1.0))\n        scale_b = min(1.0, scale_b if np.isfinite(scale_b) else 1.0)\n        radii = scale_b * radii\n\n    radii = np.clip(radii, 0.0, 0.5)\n    sum_radii = float(np.sum(radii))\n    return centers.astype(float), radii.astype(float), sum_radii\n```",
  "reasoning": "",
  "timestamp": "2026-02-11T01:23:02.557686",
  "cost_data": null,
  "model_config": {
    "model": "anthropic/claude-sonnet-4",
    "temperature": 0.4
  },
  "llm_call_duration_s": 49.712011391995475,
  "eval_duration_s": 1.1258273160783574
}