{
  "trial_id": "trial_0_3",
  "generation": 0,
  "parent_id": null,
  "code": "import numpy as np\nfrom scipy.optimize import linprog\nimport time\n\ndef run_packing():\n    \"\"\"\n    Packs 26 circles into a unit square [0,1]x[0,1] to maximize the sum of their radii.\n    Uses a greedy expansion strategy with Linear Programming for radius optimization\n    and gradient ascent based on LP dual variables for center optimization.\n    \"\"\"\n    n = 26\n    start_time = time.time()\n    \n    # Pre-allocate the constraint matrix A_ub for the Linear Program\n    # Distance constraints: r_i + r_j <= d_ij (n*(n-1)//2 constraints)\n    # Boundary constraints: r_i <= x_i, r_i <= 1-x_i, r_i <= y_i, r_i <= 1-y_i (4*n constraints)\n    num_dist = n * (n - 1) // 2\n    num_bound = 4 * n\n    num_constraints = num_dist + num_bound\n    \n    A_ub = np.zeros((num_constraints, n))\n    pairs_i = []\n    pairs_j = []\n    curr = 0\n    for i in range(n):\n        for j in range(i + 1, n):\n            A_ub[curr, i] = 1\n            A_ub[curr, j] = 1\n            pairs_i.append(i)\n            pairs_j.append(j)\n            curr += 1\n    pairs_i = np.array(pairs_i)\n    pairs_j = np.array(pairs_j)\n    \n    # Fill in A_ub for boundary constraints\n    for i in range(n):\n        for k in range(4):\n            A_ub[curr, i] = 1\n            curr += 1\n            \n    # Objective: maximize sum(r_i) -> minimize sum(-r_i)\n    c = -np.ones(n)\n    \n    best_sum = -1\n    best_centers = None\n    best_radii = None\n    \n    # Run multiple restarts to explore different configurations\n    while time.time() - start_time < 285:\n        # Initialization: first try is grid-like, others are random\n        if best_sum < 0:\n            grid_c = []\n            for i in range(5):\n                for j in range(5):\n                    grid_c.append([0.1 + 0.2*i, 0.1 + 0.2*j])\n            grid_c.append([0.5, 0.5])\n            centers = np.array(grid_c) + np.random.normal(0, 0.01, (n, 2))\n        else:\n            centers = np.random.rand(n, 2)\n        \n        centers = np.clip(centers, 0, 1)\n        \n        # Gradient ascent using dual variables from LP\n        eta = 0.01  # Initial learning rate\n        for step in range(200):\n            if time.time() - start_time > 285:\n                break\n                \n            # Update the constraint bounds b_ub based on current centers\n            b_ub = np.zeros(num_constraints)\n            diff_vec = centers[pairs_i] - centers[pairs_j]\n            dists = np.sqrt(np.sum(diff_vec**2, axis=1))\n            b_ub[:num_dist] = dists\n            \n            # Boundary bounds: r_i <= dist_to_wall\n            b_ub[num_dist::4] = centers[:, 0]        # x_i\n            b_ub[num_dist+1::4] = 1 - centers[:, 0]  # 1-x_i\n            b_ub[num_dist+2::4] = centers[:, 1]      # y_i\n            b_ub[num_dist+3::4] = 1 - centers[:, 1]  # 1-y_i\n            \n            # Solve the LP to find optimal radii for these centers\n            res = linprog(c, A_ub=A_ub, b_ub=b_ub, method='highs', options={'presolve': True})\n            \n            if not res.success or res.x is None:\n                break\n            \n            current_sum = -res.fun\n            if current_sum > best_sum:\n                best_sum = current_sum\n                best_centers = centers.copy()\n                best_radii = res.x\n            \n            # Compute the gradient of the sum of radii with respect to centers\n            # The dual variables (marginals) tell us the sensitivity to each constraint\n            duals = res.ineqlin.marginals\n            grad = np.zeros((n, 2))\n            \n            # Distance constraint contribution to gradient\n            lam = duals[:num_dist]\n            d_eps = dists + 1e-9\n            grad_dist = lam[:, None] * diff_vec / d_eps[:, None]\n            np.add.at(grad, pairs_i, grad_dist)\n            np.subtract.at(grad, pairs_j, grad_dist)\n            \n            # Boundary constraint contribution to gradient\n            curr_idx = num_dist\n            grad[:, 0] += duals[curr_idx::4]      # r_i <= x_i\n            grad[:, 0] -= duals[curr_idx+1::4]    # r_i <= 1-x_i\n            grad[:, 1] += duals[curr_idx+2::4]    # r_i <= y_i\n            grad[:, 1] -= duals[curr_idx+3::4]    # r_i <= 1-y_i\n            \n            # Update center positions using normalized gradient and small perturbation\n            grad_mag = np.linalg.norm(grad)\n            if grad_mag > 1e-7:\n                centers += eta * grad / (grad_mag + 1e-9)\n            \n            # Add small random jitter to help escape local plateaus\n            centers += np.random.normal(0, 0.0002, centers.shape)\n            centers = np.clip(centers, 0, 1)\n            \n            # Decay the learning rate\n            eta *= 0.995\n            \n            # Early restart if the configuration is clearly poor\n            if step == 40 and current_sum < 2.45:\n                break\n                \n    return best_centers, best_radii, best_sum\n\nif __name__ == \"__main__\":\n    centers, radii, sum_radii = run_packing()",
  "metrics": {
    "valid": true,
    "score": 2.2166764832422343,
    "eval_time": 285.5552182197571,
    "error": null
  },
  "prompt": "Implement a 'greedy expansion' strategy. Start with 26 small circles at random positions. Iteratively: 1) Optimize radii for fixed centers using Linear Programming (scipy.optimize.linprog). 2) Perturb centers to allow for more growth. 3) Repeat. This decouples the linear and non-linear parts of the problem.",
  "response": "```python\nimport numpy as np\nfrom scipy.optimize import linprog\nimport time\n\ndef run_packing():\n    \"\"\"\n    Packs 26 circles into a unit square [0,1]x[0,1] to maximize the sum of their radii.\n    Uses a greedy expansion strategy with Linear Programming for radius optimization\n    and gradient ascent based on LP dual variables for center optimization.\n    \"\"\"\n    n = 26\n    start_time = time.time()\n    \n    # Pre-allocate the constraint matrix A_ub for the Linear Program\n    # Distance constraints: r_i + r_j <= d_ij (n*(n-1)//2 constraints)\n    # Boundary constraints: r_i <= x_i, r_i <= 1-x_i, r_i <= y_i, r_i <= 1-y_i (4*n constraints)\n    num_dist = n * (n - 1) // 2\n    num_bound = 4 * n\n    num_constraints = num_dist + num_bound\n    \n    A_ub = np.zeros((num_constraints, n))\n    pairs_i = []\n    pairs_j = []\n    curr = 0\n    for i in range(n):\n        for j in range(i + 1, n):\n            A_ub[curr, i] = 1\n            A_ub[curr, j] = 1\n            pairs_i.append(i)\n            pairs_j.append(j)\n            curr += 1\n    pairs_i = np.array(pairs_i)\n    pairs_j = np.array(pairs_j)\n    \n    # Fill in A_ub for boundary constraints\n    for i in range(n):\n        for k in range(4):\n            A_ub[curr, i] = 1\n            curr += 1\n            \n    # Objective: maximize sum(r_i) -> minimize sum(-r_i)\n    c = -np.ones(n)\n    \n    best_sum = -1\n    best_centers = None\n    best_radii = None\n    \n    # Run multiple restarts to explore different configurations\n    while time.time() - start_time < 285:\n        # Initialization: first try is grid-like, others are random\n        if best_sum < 0:\n            grid_c = []\n            for i in range(5):\n                for j in range(5):\n                    grid_c.append([0.1 + 0.2*i, 0.1 + 0.2*j])\n            grid_c.append([0.5, 0.5])\n            centers = np.array(grid_c) + np.random.normal(0, 0.01, (n, 2))\n        else:\n            centers = np.random.rand(n, 2)\n        \n        centers = np.clip(centers, 0, 1)\n        \n        # Gradient ascent using dual variables from LP\n        eta = 0.01  # Initial learning rate\n        for step in range(200):\n            if time.time() - start_time > 285:\n                break\n                \n            # Update the constraint bounds b_ub based on current centers\n            b_ub = np.zeros(num_constraints)\n            diff_vec = centers[pairs_i] - centers[pairs_j]\n            dists = np.sqrt(np.sum(diff_vec**2, axis=1))\n            b_ub[:num_dist] = dists\n            \n            # Boundary bounds: r_i <= dist_to_wall\n            b_ub[num_dist::4] = centers[:, 0]        # x_i\n            b_ub[num_dist+1::4] = 1 - centers[:, 0]  # 1-x_i\n            b_ub[num_dist+2::4] = centers[:, 1]      # y_i\n            b_ub[num_dist+3::4] = 1 - centers[:, 1]  # 1-y_i\n            \n            # Solve the LP to find optimal radii for these centers\n            res = linprog(c, A_ub=A_ub, b_ub=b_ub, method='highs', options={'presolve': True})\n            \n            if not res.success or res.x is None:\n                break\n            \n            current_sum = -res.fun\n            if current_sum > best_sum:\n                best_sum = current_sum\n                best_centers = centers.copy()\n                best_radii = res.x\n            \n            # Compute the gradient of the sum of radii with respect to centers\n            # The dual variables (marginals) tell us the sensitivity to each constraint\n            duals = res.ineqlin.marginals\n            grad = np.zeros((n, 2))\n            \n            # Distance constraint contribution to gradient\n            lam = duals[:num_dist]\n            d_eps = dists + 1e-9\n            grad_dist = lam[:, None] * diff_vec / d_eps[:, None]\n            np.add.at(grad, pairs_i, grad_dist)\n            np.subtract.at(grad, pairs_j, grad_dist)\n            \n            # Boundary constraint contribution to gradient\n            curr_idx = num_dist\n            grad[:, 0] += duals[curr_idx::4]      # r_i <= x_i\n            grad[:, 0] -= duals[curr_idx+1::4]    # r_i <= 1-x_i\n            grad[:, 1] += duals[curr_idx+2::4]    # r_i <= y_i\n            grad[:, 1] -= duals[curr_idx+3::4]    # r_i <= 1-y_i\n            \n            # Update center positions using normalized gradient and small perturbation\n            grad_mag = np.linalg.norm(grad)\n            if grad_mag > 1e-7:\n                centers += eta * grad / (grad_mag + 1e-9)\n            \n            # Add small random jitter to help escape local plateaus\n            centers += np.random.normal(0, 0.0002, centers.shape)\n            centers = np.clip(centers, 0, 1)\n            \n            # Decay the learning rate\n            eta *= 0.995\n            \n            # Early restart if the configuration is clearly poor\n            if step == 40 and current_sum < 2.45:\n                break\n                \n    return best_centers, best_radii, best_sum\n\nif __name__ == \"__main__\":\n    centers, radii, sum_radii = run_packing()\n```",
  "reasoning": "",
  "timestamp": "2026-02-10T22:58:42.959935",
  "cost_data": null,
  "model_config": {
    "model": "gemini-3-flash-preview",
    "temperature": 0.7,
    "provider_debug": {
      "finish_reason": "STOP",
      "usage_metadata": {
        "prompt_token_count": 301,
        "candidates_token_count": 1576,
        "thoughts_token_count": 18813
      },
      "response_text_len": 4965
    }
  },
  "llm_call_duration_s": 114.1157174280379,
  "eval_duration_s": 285.5552333079977
}