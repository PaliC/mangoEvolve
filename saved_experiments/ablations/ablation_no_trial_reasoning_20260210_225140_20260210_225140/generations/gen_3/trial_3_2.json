{
  "trial_id": "trial_3_2",
  "generation": 3,
  "parent_id": "trial_1_3",
  "code": "import numpy as np\nfrom scipy.optimize import minimize, linprog\nimport time\n\ndef run_packing():\n    \"\"\"\n    Packs 26 circles into a unit square [0,1] x [0,1] to maximize the sum of their radii.\n    Combines a Force-Directed initialization (penalty-based L-BFGS-B) with\n    Iterative Local Refinement (SLSQP with shaking and Linear Programming).\n    \"\"\"\n    n = 26\n    start_time = time.time()\n    \n    # Precompute indices for vectorized operations\n    i_idx, j_idx = np.triu_indices(n, k=1)\n    row_idx_overlap = 4 * n + np.arange(len(i_idx))\n    idx_n = np.arange(n)\n    \n    def objective(vars):\n        # Maximize sum of radii = Minimize -sum(radii)\n        return -vars[2::3].sum()\n\n    def grad_objective(vars):\n        grad = np.zeros(3 * n)\n        grad[2::3] = -1.0\n        return grad\n\n    def constraints(vars):\n        x, y, r = vars[0::3], vars[1::3], vars[2::3]\n        b_cons = np.zeros(4 * n)\n        b_cons[0::4] = x - r\n        b_cons[1::4] = 1.0 - x - r\n        b_cons[2::4] = y - r\n        b_cons[3::4] = 1.0 - y - r\n        o_cons = (x[i_idx] - x[j_idx])**2 + (y[i_idx] - y[j_idx])**2 - (r[i_idx] + r[j_idx])**2\n        return np.concatenate([b_cons, o_cons])\n\n    def constraints_jac(vars):\n        x, y, r = vars[0::3], vars[1::3], vars[2::3]\n        jac = np.zeros((4 * n + len(i_idx), 3 * n))\n        jac[4*idx_n, 3*idx_n] = 1.0\n        jac[4*idx_n, 3*idx_n+2] = -1.0\n        jac[4*idx_n+1, 3*idx_n] = -1.0\n        jac[4*idx_n+1, 3*idx_n+2] = -1.0\n        jac[4*idx_n+2, 3*idx_n+1] = 1.0\n        jac[4*idx_n+2, 3*idx_n+2] = -1.0\n        jac[4*idx_n+3, 3*idx_n+1] = -1.0\n        jac[4*idx_n+3, 3*idx_n+2] = -1.0\n        dx, dy, dr = x[i_idx] - x[j_idx], y[i_idx] - y[j_idx], r[i_idx] + r[j_idx]\n        jac[row_idx_overlap, 3*i_idx] = 2 * dx\n        jac[row_idx_overlap, 3*j_idx] = -2 * dx\n        jac[row_idx_overlap, 3*i_idx+1] = 2 * dy\n        jac[row_idx_overlap, 3*j_idx+1] = -2 * dy\n        jac[row_idx_overlap, 3*i_idx+2] = -2 * dr\n        jac[row_idx_overlap, 3*j_idx+2] = -2 * dr\n        return jac\n\n    def lp_refine(centers, n, i_idx, j_idx, current_r):\n        x, y = centers[:, 0], centers[:, 1]\n        c_lp = -np.ones(n)\n        A_ub = np.zeros((len(i_idx), n))\n        b_ub = np.zeros(len(i_idx))\n        for idx, (i, j) in enumerate(zip(i_idx, j_idx)):\n            A_ub[idx, i] = 1\n            A_ub[idx, j] = 1\n            b_ub[idx] = np.sqrt((x[i]-x[j])**2 + (y[i]-y[j])**2)\n        b_ub = np.maximum(0, b_ub + 1e-10)\n        r_bounds = [(0, max(0.0, min(x[i], 1.0 - x[i], y[i], 1.0 - y[i]))) for i in range(n)]\n        res_lp = linprog(c_lp, A_ub=A_ub, b_ub=b_ub, bounds=r_bounds, method='highs')\n        return res_lp.x if res_lp.success else current_r\n\n    def penalty_objective(vars, C):\n        x, y, r = vars[0::3], vars[1::3], vars[2::3]\n        b_viol = np.sum(np.maximum(0, r - x)**2 + np.maximum(0, x + r - 1)**2 + \n                        np.maximum(0, r - y)**2 + np.maximum(0, y + r - 1)**2)\n        dx, dy = x[i_idx] - x[j_idx], y[i_idx] - y[j_idx]\n        d = np.sqrt(dx*dx + dy*dy + 1e-9)\n        o_viol = np.sum(np.maximum(0, r[i_idx] + r[j_idx] - d)**2)\n        return -np.sum(r) + C * (b_viol + o_viol)\n\n    def penalty_grad(vars, C):\n        x, y, r = vars[0::3], vars[1::3], vars[2::3]\n        gx, gy, gr = np.zeros(n), np.zeros(n), -np.ones(n)\n        mask = r > x\n        gx[mask] -= 2*C*(r[mask]-x[mask]); gr[mask] += 2*C*(r[mask]-x[mask])\n        mask = x+r > 1\n        gx[mask] += 2*C*(x[mask]+r[mask]-1); gr[mask] += 2*C*(x[mask]+r[mask]-1)\n        mask = r > y\n        gy[mask] -= 2*C*(r[mask]-y[mask]); gr[mask] += 2*C*(r[mask]-y[mask])\n        mask = y+r > 1\n        gy[mask] += 2*C*(y[mask]+r[mask]-1); gr[mask] += 2*C*(y[mask]+r[mask]-1)\n        dx, dy = x[i_idx] - x[j_idx], y[i_idx] - y[j_idx]\n        d = np.sqrt(dx*dx + dy*dy + 1e-9)\n        overlap = r[i_idx] + r[j_idx] - d\n        mask = overlap > 0\n        common = 2 * C * overlap[mask]\n        d_mask, dx_mask, dy_mask = d[mask], dx[mask], dy[mask]\n        np.add.at(gr, i_idx[mask], common)\n        np.add.at(gr, j_idx[mask], common)\n        np.add.at(gx, i_idx[mask], -common * dx_mask / d_mask)\n        np.add.at(gx, j_idx[mask], common * dx_mask / d_mask)\n        np.add.at(gy, i_idx[mask], -common * dy_mask / d_mask)\n        np.add.at(gy, j_idx[mask], common * dy_mask / d_mask)\n        grad = np.zeros(3*n)\n        grad[0::3], grad[1::3], grad[2::3] = gx, gy, gr\n        return grad\n\n    def shake(current_vars):\n        vars_copy = current_vars.copy()\n        vars_copy[0::3] += np.random.normal(0, 0.003, n)\n        vars_copy[1::3] += np.random.normal(0, 0.003, n)\n        vars_copy[0::3] = np.clip(vars_copy[0::3], 0, 1)\n        vars_copy[1::3] = np.clip(vars_copy[1::3], 0, 1)\n        centers = vars_copy.reshape(n, 3)[:, :2]\n        vars_copy[2::3] = lp_refine(centers, n, i_idx, j_idx, vars_copy[2::3])\n        num_replace = np.random.randint(3, 6)\n        indices_to_replace = np.random.choice(n, size=num_replace, replace=False)\n        active_indices = [i for i in range(n) if i not in indices_to_replace]\n        samples = np.random.uniform(0, 1, (2000, 2))\n        d_b = np.min([samples[:,0], 1-samples[:,0], samples[:,1], 1-samples[:,1]], axis=0)\n        for idx in indices_to_replace:\n            ax, ay, ar = vars_copy[0::3][active_indices], vars_copy[1::3][active_indices], vars_copy[2::3][active_indices]\n            d_c = np.min(np.sqrt((samples[:, 0:1] - ax)**2 + (samples[:, 1:2] - ay)**2) - ar, axis=1)\n            r_cand = np.minimum(d_b, d_c)\n            best_sample_idx = np.argmax(r_cand)\n            vars_copy[3*idx] = samples[best_sample_idx, 0]\n            vars_copy[3*idx+1] = samples[best_sample_idx, 1]\n            vars_copy[3*idx+2] = max(0.01, r_cand[best_sample_idx])\n            active_indices.append(idx)\n        return vars_copy\n\n    bounds = [(0, 1), (0, 1), (0, 0.5)] * n\n    constraints_dict = {'type': 'ineq', 'fun': constraints, 'jac': constraints_jac}\n    best_sum, best_vars = -1.0, None\n\n    # Phase 1: Multiple starts\n    for start_idx in range(5):\n        vars0 = np.zeros(3 * n)\n        if start_idx == 0:\n            xs = np.linspace(0.1, 0.9, 5)\n            xi, yi = np.meshgrid(xs, xs)\n            vars0[0::3] = np.append(xi.flatten(), 0.5)\n            vars0[1::3] = np.append(yi.flatten(), 0.5)\n            vars0[2::3] = 0.08\n        else:\n            vars0[0::3], vars0[1::3], vars0[2::3] = np.random.rand(n), np.random.rand(n), 0.05\n        \n        # Penalty-based L-BFGS-B Initialization\n        res_p = minimize(penalty_objective, vars0, args=(100,), method='L-BFGS-B', \n                         jac=penalty_grad, bounds=bounds, options={'maxiter': 100})\n        # SLSQP Refinement\n        res = minimize(objective, res_p.x, method='SLSQP', jac=grad_objective,\n                       constraints=constraints_dict, bounds=bounds, options={'maxiter': 200, 'ftol': 1e-7})\n        # LP Radii Refinement\n        opt_r = lp_refine(res.x.reshape(n, 3)[:, :2], n, i_idx, j_idx, res.x[2::3])\n        current_sum = np.sum(opt_r)\n        if current_sum > best_sum:\n            best_sum, best_vars = current_sum, res.x.copy()\n            best_vars[2::3] = opt_r\n\n    # Phase 2: Shake and Refine\n    while time.time() - start_time < 285:\n        vars_shaken = shake(best_vars)\n        res = minimize(objective, vars_shaken, method='SLSQP', jac=grad_objective,\n                       constraints=constraints_dict, bounds=bounds, options={'maxiter': 200, 'ftol': 1e-7})\n        opt_r = lp_refine(res.x.reshape(n, 3)[:, :2], n, i_idx, j_idx, res.x[2::3])\n        current_sum = np.sum(opt_r)\n        if current_sum > best_sum:\n            best_sum, best_vars = current_sum, res.x.copy()\n            best_vars[2::3] = opt_r\n\n    final_centers = best_vars.reshape(n, 3)[:, :2]\n    final_radii = lp_refine(final_centers, n, i_idx, j_idx, best_vars[2::3])\n    return final_centers, final_radii, float(np.sum(final_radii))",
  "metrics": {
    "valid": true,
    "score": 2.630059586935439,
    "eval_time": 285.9916396141052,
    "error": null
  },
  "prompt": "Combine the Force-Directed initialization from trial_1_4 with the Iterative Local Refinement from trial_1_3. Use the force-directed approach to find a good starting topology, then apply the shaking and SLSQP refinement. Trial 1.4 code:\nimport numpy as np\nfrom scipy.optimize import minimize, linprog\nimport time\n\ndef run_packing():\n    \"\"\"\n    Packs 26 circles into a unit square [0,1] x [0,1] to maximize the sum of their radii.\n    Uses a combination of a penalty-based L-BFGS-B optimizer and Linear Programming\n    to find the optimal radii for a given set of centers.\n    \"\"\"\n    N = 26\n    start_time = time.time()\n    \n    def objective(params, C):\n        \"\"\"Penalty-based objective function for L-BFGS-B.\"\"\"\n        x = params[:N]\n        y = params[N:2*N]\n        r = params[2*N:3*N]\n        obj = -np.sum(r)\n        \n        penalty = 0\n        # Boundary violations: r_i <= x_i, r_i <= 1-x_i, etc.\n        penalty += np.sum(np.maximum(0, r - x)**2)\n        penalty += np.sum(np.maximum(0, x + r - 1)**2)\n        penalty += np.sum(np.maximum(0, r - y)**2)\n        penalty += np.sum(np.maximum(0, y + r - 1)**2)\n        \n        # Overlap violations: r_i + r_j <= d_ij\n        for i in range(N):\n            for j in range(i + 1, N):\n                dx = x[i] - x[j]\n                dy = y[i] - y[j]\n                d = np.sqrt(dx*dx + dy*dy)\n                if d < 1e-9:\n                    penalty += (r[i] + r[j])**2\n                elif r[i] + r[j] > d:\n                    penalty += (r[i] + r[j] - d)**2\n        return obj + C * penalty\n\n    def gradient(params, C):\n        \"\"\"Gradient of the penalty-based objective function.\"\"\"\n        x = params[:N]\n        y = params[N:2*N]\n        r = params[2*N:3*N]\n        \n        grad_x = np.zeros(N)\n        grad_y = np.zeros(N)\n        grad_r = -np.ones(N)\n        \n        # Boundary gradients\n        mask = r > x\n        grad_r[mask] += 2 * C * (r[mask] - x[mask])\n        grad_x[mask] -= 2 * C * (r[mask] - x[mask])\n        \n        mask = x + r > 1\n        grad_r[mask] += 2 * C * (x[mask] + r[mask] - 1)\n        grad_x[mask] += 2 * C * (x[mask] + r[mask] - 1)\n        \n        mask = r > y\n        grad_r[mask] += 2 * C * (r[mask] - y[mask])\n        grad_y[mask] -= 2 * C * (r[mask] - y[mask])\n        \n        mask = y + r > 1\n        grad_r[mask] += 2 * C * (y[mask] + r[mask] - 1)\n        grad_y[mask] += 2 * C * (y[mask] + r[mask] - 1)\n        \n        # Overlap gradients\n        for i in range(N):\n            for j in range(i + 1, N):\n                dx = x[i] - x[j]\n                dy = y[i] - y[j]\n                d = np.sqrt(dx*dx + dy*dy)\n                if r[i] + r[j] > d:\n                    if d < 1e-9:\n                        grad_r[i] += 2 * C * (r[i] + r[j])\n                        grad_r[j] += 2 * C * (r[i] + r[j])\n                        continue\n                    overlap = r[i] + r[j] - d\n                    common = 2 * C * overlap\n                    grad_r[i] += common\n                    grad_r[j] += common\n                    grad_x[i] -= common * (dx / d)\n                    grad_x[j] += common * (dx / d)\n                    grad_y[i] -= common * (dy / d)\n                    grad_y[j] += common * (dy / d)\n        \n        return np.concatenate([grad_x, grad_y, grad_r])\n\n    best_sum = 0\n    best_centers = np.zeros((N, 2))\n    best_radii = np.zeros(N)\n    \n    # Initialize several starting configurations\n    # 1. Near-grid start\n    grid_size = 5\n    xs = np.linspace(0.1, 0.9, grid_size)\n    ys = np.linspace(0.1, 0.9, grid_size)\n    xi, yi = np.meshgrid(xs, ys)\n    x_grid = np.append(xi.flatten(), 0.5)\n    y_grid = np.append(yi.flatten(), 0.5)\n    r_grid = np.full(N, 0.09)\n    \n    starts = [np.concatenate([x_grid, y_grid, r_grid])]\n    # 2. Random starts\n    for _ in range(15):\n        starts.append(np.concatenate([np.random.rand(N), np.random.rand(N), np.random.rand(N)*0.1]))\n\n    # Main optimization loop\n    for params in starts:\n        if time.time() - start_time > 280: # Time limit safety\n            break\n        \n        # Gradually increase penalty coefficient C\n        for C in [10, 100, 1000]:\n            res = minimize(objective, params, args=(C,), method='L-BFGS-B', \n                           jac=gradient, bounds=[(0, 1)]*(2*N) + [(0, 0.5)]*N, \n                           options={'maxiter': 200})\n            params = res.x\n        \n        # Refine radii using Linear Programming for the found centers\n        x, y = params[:N], params[N:2*N]\n        c_lp = -np.ones(N)\n        num_constraints = N * (N - 1) // 2\n        A_ub = np.zeros((num_constraints, N))\n        b_ub = np.zeros(num_constraints)\n        idx = 0\n        for i in range(N):\n            for j in range(i + 1, N):\n                A_ub[idx, i] = 1\n                A_ub[idx, j] = 1\n                b_ub[idx] = np.sqrt((x[i]-x[j])**2 + (y[i]-y[j])**2)\n                idx += 1\n        \n        # Radius bounds based on distance to square edges\n        r_bounds = []\n        for i in range(N):\n            limit = min(x[i], 1-x[i], y[i], 1-y[i])\n            r_bounds.append((0, max(0, limit)))\n            \n        res_lp = linprog(c_lp, A_ub=A_ub, b_ub=b_ub, bounds=r_bounds, method='highs')\n        \n        if res_lp.success:\n            current_sum = -res_lp.fun\n            if current_sum > best_sum:\n                best_sum = current_sum\n                best_radii = res_lp.x\n                best_centers = np.stack([x, y], axis=1)\n\n    return best_centers, best_radii, best_sum\nTrial 1.3 code:\nimport numpy as np\nfrom scipy.optimize import minimize\nimport time\n\ndef run_packing():\n    \"\"\"\n    Packs 26 circles into a unit square [0,1] x [0,1] to maximize the sum of their radii.\n    Uses a multi-start SLSQP followed by an iterative 'shake and refine' process.\n    \"\"\"\n    n = 26\n    start_time = time.time()\n    \n    # Precompute indices for vectorized constraint and Jacobian calculations\n    i_idx, j_idx = np.triu_indices(n, k=1)\n    # Overlap constraints start after 4*n boundary constraints\n    row_idx_overlap = 4 * n + np.arange(len(i_idx))\n    idx_n = np.arange(n)\n    \n    def objective(vars):\n        # Maximize sum of radii = Minimize -sum(radii)\n        return -vars[2::3].sum()\n\n    def grad_objective(vars):\n        # Gradient of the objective: -1 for all radii, 0 for centers\n        grad = np.zeros(3 * n)\n        grad[2::3] = -1.0\n        return grad\n\n    def constraints(vars):\n        x = vars[0::3]\n        y = vars[1::3]\n        r = vars[2::3]\n        \n        # Boundary constraints: x_i - r_i >= 0, 1 - x_i - r_i >= 0, etc.\n        b_cons = np.zeros(4 * n)\n        b_cons[0::4] = x - r\n        b_cons[1::4] = 1.0 - x - r\n        b_cons[2::4] = y - r\n        b_cons[3::4] = 1.0 - y - r\n        \n        # Overlap constraints: (x_i - x_j)**2 + (y_i - y_j)**2 - (r_i + r_j)**2 >= 0\n        o_cons = (x[i_idx] - x[j_idx])**2 + (y[i_idx] - y[j_idx])**2 - (r[i_idx] + r[j_idx])**2\n        return np.concatenate([b_cons, o_cons])\n\n    def constraints_jac(vars):\n        x = vars[0::3]\n        y = vars[1::3]\n        r = vars[2::3]\n        \n        # Jacobian matrix: 429 constraints x 78 variables\n        jac = np.zeros((4*n + len(i_idx), 3 * n))\n        \n        # Boundary constraints derivatives\n        jac[4*idx_n, 3*idx_n] = 1.0\n        jac[4*idx_n, 3*idx_n+2] = -1.0\n        jac[4*idx_n+1, 3*idx_n] = -1.0\n        jac[4*idx_n+1, 3*idx_n+2] = -1.0\n        jac[4*idx_n+2, 3*idx_n+1] = 1.0\n        jac[4*idx_n+2, 3*idx_n+2] = -1.0\n        jac[4*idx_n+3, 3*idx_n+1] = -1.0\n        jac[4*idx_n+3, 3*idx_n+2] = -1.0\n        \n        # Overlap constraints derivatives\n        dx = x[i_idx] - x[j_idx]\n        dy = y[i_idx] - y[j_idx]\n        dr = r[i_idx] + r[j_idx]\n        \n        jac[row_idx_overlap, 3*i_idx] = 2 * dx\n        jac[row_idx_overlap, 3*j_idx] = -2 * dx\n        jac[row_idx_overlap, 3*i_idx+1] = 2 * dy\n        jac[row_idx_overlap, 3*j_idx+1] = -2 * dy\n        jac[row_idx_overlap, 3*i_idx+2] = -2 * dr\n        jac[row_idx_overlap, 3*j_idx+2] = -2 * dr\n        \n        return jac\n\n    def shake(current_vars, n):\n        \"\"\"Randomly removes 2-4 circles and re-places them in the largest available gaps.\"\"\"\n        vars_copy = current_vars.copy()\n        \n        # Slight perturbation to existing centers to help escape local optima\n        vars_copy[0::3] += np.random.normal(0, 0.001, n)\n        vars_copy[1::3] += np.random.normal(0, 0.001, n)\n        vars_copy[0::3] = np.clip(vars_copy[0::3], 0, 1)\n        vars_copy[1::3] = np.clip(vars_copy[1::3], 0, 1)\n        \n        # Select 2-4 circles to replace\n        num_replace = np.random.randint(2, 5)\n        indices_to_replace = np.random.choice(n, size=num_replace, replace=False)\n        active_indices = [i for i in range(n) if i not in indices_to_replace]\n        \n        for idx in indices_to_replace:\n            # Sample many points to find a large gap\n            samples = np.random.uniform(0, 1, (2000, 2))\n            \n            # Distance to boundary\n            d_b = np.minimum.reduce([samples[:, 0], 1 - samples[:, 0], samples[:, 1], 1 - samples[:, 1]])\n            \n            # Distance to all other active circles\n            ax = vars_copy[0::3][active_indices]\n            ay = vars_copy[1::3][active_indices]\n            ar = vars_copy[2::3][active_indices]\n            \n            # Vectorized distance calculation\n            dx = samples[:, 0:1] - ax\n            dy = samples[:, 1:2] - ay\n            d_c = np.sqrt(dx**2 + dy**2) - ar\n            min_d_c = np.min(d_c, axis=1)\n            \n            # Gap radius candidate\n            r_cand = np.minimum(d_b, min_d_c)\n            best_sample_idx = np.argmax(r_cand)\n            \n            # Place the circle in the best gap found\n            vars_copy[3*idx] = samples[best_sample_idx, 0]\n            vars_copy[3*idx+1] = samples[best_sample_idx, 1]\n            vars_copy[3*idx+2] = 0.02 # Start with a small radius for refinement\n            \n            active_indices.append(idx)\n            \n        return vars_copy\n\n    bounds = [(0, 1), (0, 1), (0, 0.5)] * n\n    best_sum = -1.0\n    best_vars = None\n    \n    # Phase 1: Multi-start random initialization (60 seconds)\n    while time.time() - start_time < 60:\n        vars0 = np.empty(3 * n)\n        vars0[0::3] = np.random.uniform(0.1, 0.9, n)\n        vars0[1::3] = np.random.uniform(0.1, 0.9, n)\n        vars0[2::3] = np.random.uniform(0.01, 0.05, n)\n        \n        res = minimize(\n            objective, vars0, method='SLSQP', jac=grad_objective,\n            constraints={'type': 'ineq', 'fun': constraints, 'jac': constraints_jac},\n            bounds=bounds, options={'maxiter': 400, 'ftol': 1e-8}\n        )\n        \n        if -res.fun > best_sum:\n            if np.all(constraints(res.x) >= -1e-7):\n                best_sum = -res.fun\n                best_vars = res.x\n\n    # Phase 2: Iterative Local Refinement (until ~285 seconds)\n    if best_vars is not None:\n        while time.time() - start_time < 285:\n            vars0 = shake(best_vars, n)\n            res = minimize(\n                objective, vars0, method='SLSQP', jac=grad_objective,\n                constraints={'type': 'ineq', 'fun': constraints, 'jac': constraints_jac},\n                bounds=bounds, options={'maxiter': 250, 'ftol': 1e-8}\n            )\n            \n            if -res.fun > best_sum:\n                if np.all(constraints(res.x) >= -1e-7):\n                    best_sum = -res.fun\n                    best_vars = res.x\n                    \n    # Ensure a fallback if something went wrong\n    if best_vars is None:\n        best_vars = np.zeros(3 * n)\n        best_vars[0::3] = np.random.rand(n)\n        best_vars[1::3] = np.random.rand(n)\n        best_vars[2::3] = 0.01\n\n    # Format the final output\n    final_vars = best_vars.reshape(n, 3)\n    best_centers = final_vars[:, :2]\n    best_radii = np.maximum(0, final_vars[:, 2])\n    actual_sum = float(np.sum(best_radii))\n    \n    return best_centers, best_radii, actual_sum\n\nif __name__ == \"__main__\":\n    centers, radii, sum_radii = run_packing()",
  "response": "```python\nimport numpy as np\nfrom scipy.optimize import minimize, linprog\nimport time\n\ndef run_packing():\n    \"\"\"\n    Packs 26 circles into a unit square [0,1] x [0,1] to maximize the sum of their radii.\n    Combines a Force-Directed initialization (penalty-based L-BFGS-B) with\n    Iterative Local Refinement (SLSQP with shaking and Linear Programming).\n    \"\"\"\n    n = 26\n    start_time = time.time()\n    \n    # Precompute indices for vectorized operations\n    i_idx, j_idx = np.triu_indices(n, k=1)\n    row_idx_overlap = 4 * n + np.arange(len(i_idx))\n    idx_n = np.arange(n)\n    \n    def objective(vars):\n        # Maximize sum of radii = Minimize -sum(radii)\n        return -vars[2::3].sum()\n\n    def grad_objective(vars):\n        grad = np.zeros(3 * n)\n        grad[2::3] = -1.0\n        return grad\n\n    def constraints(vars):\n        x, y, r = vars[0::3], vars[1::3], vars[2::3]\n        b_cons = np.zeros(4 * n)\n        b_cons[0::4] = x - r\n        b_cons[1::4] = 1.0 - x - r\n        b_cons[2::4] = y - r\n        b_cons[3::4] = 1.0 - y - r\n        o_cons = (x[i_idx] - x[j_idx])**2 + (y[i_idx] - y[j_idx])**2 - (r[i_idx] + r[j_idx])**2\n        return np.concatenate([b_cons, o_cons])\n\n    def constraints_jac(vars):\n        x, y, r = vars[0::3], vars[1::3], vars[2::3]\n        jac = np.zeros((4 * n + len(i_idx), 3 * n))\n        jac[4*idx_n, 3*idx_n] = 1.0\n        jac[4*idx_n, 3*idx_n+2] = -1.0\n        jac[4*idx_n+1, 3*idx_n] = -1.0\n        jac[4*idx_n+1, 3*idx_n+2] = -1.0\n        jac[4*idx_n+2, 3*idx_n+1] = 1.0\n        jac[4*idx_n+2, 3*idx_n+2] = -1.0\n        jac[4*idx_n+3, 3*idx_n+1] = -1.0\n        jac[4*idx_n+3, 3*idx_n+2] = -1.0\n        dx, dy, dr = x[i_idx] - x[j_idx], y[i_idx] - y[j_idx], r[i_idx] + r[j_idx]\n        jac[row_idx_overlap, 3*i_idx] = 2 * dx\n        jac[row_idx_overlap, 3*j_idx] = -2 * dx\n        jac[row_idx_overlap, 3*i_idx+1] = 2 * dy\n        jac[row_idx_overlap, 3*j_idx+1] = -2 * dy\n        jac[row_idx_overlap, 3*i_idx+2] = -2 * dr\n        jac[row_idx_overlap, 3*j_idx+2] = -2 * dr\n        return jac\n\n    def lp_refine(centers, n, i_idx, j_idx, current_r):\n        x, y = centers[:, 0], centers[:, 1]\n        c_lp = -np.ones(n)\n        A_ub = np.zeros((len(i_idx), n))\n        b_ub = np.zeros(len(i_idx))\n        for idx, (i, j) in enumerate(zip(i_idx, j_idx)):\n            A_ub[idx, i] = 1\n            A_ub[idx, j] = 1\n            b_ub[idx] = np.sqrt((x[i]-x[j])**2 + (y[i]-y[j])**2)\n        b_ub = np.maximum(0, b_ub + 1e-10)\n        r_bounds = [(0, max(0.0, min(x[i], 1.0 - x[i], y[i], 1.0 - y[i]))) for i in range(n)]\n        res_lp = linprog(c_lp, A_ub=A_ub, b_ub=b_ub, bounds=r_bounds, method='highs')\n        return res_lp.x if res_lp.success else current_r\n\n    def penalty_objective(vars, C):\n        x, y, r = vars[0::3], vars[1::3], vars[2::3]\n        b_viol = np.sum(np.maximum(0, r - x)**2 + np.maximum(0, x + r - 1)**2 + \n                        np.maximum(0, r - y)**2 + np.maximum(0, y + r - 1)**2)\n        dx, dy = x[i_idx] - x[j_idx], y[i_idx] - y[j_idx]\n        d = np.sqrt(dx*dx + dy*dy + 1e-9)\n        o_viol = np.sum(np.maximum(0, r[i_idx] + r[j_idx] - d)**2)\n        return -np.sum(r) + C * (b_viol + o_viol)\n\n    def penalty_grad(vars, C):\n        x, y, r = vars[0::3], vars[1::3], vars[2::3]\n        gx, gy, gr = np.zeros(n), np.zeros(n), -np.ones(n)\n        mask = r > x\n        gx[mask] -= 2*C*(r[mask]-x[mask]); gr[mask] += 2*C*(r[mask]-x[mask])\n        mask = x+r > 1\n        gx[mask] += 2*C*(x[mask]+r[mask]-1); gr[mask] += 2*C*(x[mask]+r[mask]-1)\n        mask = r > y\n        gy[mask] -= 2*C*(r[mask]-y[mask]); gr[mask] += 2*C*(r[mask]-y[mask])\n        mask = y+r > 1\n        gy[mask] += 2*C*(y[mask]+r[mask]-1); gr[mask] += 2*C*(y[mask]+r[mask]-1)\n        dx, dy = x[i_idx] - x[j_idx], y[i_idx] - y[j_idx]\n        d = np.sqrt(dx*dx + dy*dy + 1e-9)\n        overlap = r[i_idx] + r[j_idx] - d\n        mask = overlap > 0\n        common = 2 * C * overlap[mask]\n        d_mask, dx_mask, dy_mask = d[mask], dx[mask], dy[mask]\n        np.add.at(gr, i_idx[mask], common)\n        np.add.at(gr, j_idx[mask], common)\n        np.add.at(gx, i_idx[mask], -common * dx_mask / d_mask)\n        np.add.at(gx, j_idx[mask], common * dx_mask / d_mask)\n        np.add.at(gy, i_idx[mask], -common * dy_mask / d_mask)\n        np.add.at(gy, j_idx[mask], common * dy_mask / d_mask)\n        grad = np.zeros(3*n)\n        grad[0::3], grad[1::3], grad[2::3] = gx, gy, gr\n        return grad\n\n    def shake(current_vars):\n        vars_copy = current_vars.copy()\n        vars_copy[0::3] += np.random.normal(0, 0.003, n)\n        vars_copy[1::3] += np.random.normal(0, 0.003, n)\n        vars_copy[0::3] = np.clip(vars_copy[0::3], 0, 1)\n        vars_copy[1::3] = np.clip(vars_copy[1::3], 0, 1)\n        centers = vars_copy.reshape(n, 3)[:, :2]\n        vars_copy[2::3] = lp_refine(centers, n, i_idx, j_idx, vars_copy[2::3])\n        num_replace = np.random.randint(3, 6)\n        indices_to_replace = np.random.choice(n, size=num_replace, replace=False)\n        active_indices = [i for i in range(n) if i not in indices_to_replace]\n        samples = np.random.uniform(0, 1, (2000, 2))\n        d_b = np.min([samples[:,0], 1-samples[:,0], samples[:,1], 1-samples[:,1]], axis=0)\n        for idx in indices_to_replace:\n            ax, ay, ar = vars_copy[0::3][active_indices], vars_copy[1::3][active_indices], vars_copy[2::3][active_indices]\n            d_c = np.min(np.sqrt((samples[:, 0:1] - ax)**2 + (samples[:, 1:2] - ay)**2) - ar, axis=1)\n            r_cand = np.minimum(d_b, d_c)\n            best_sample_idx = np.argmax(r_cand)\n            vars_copy[3*idx] = samples[best_sample_idx, 0]\n            vars_copy[3*idx+1] = samples[best_sample_idx, 1]\n            vars_copy[3*idx+2] = max(0.01, r_cand[best_sample_idx])\n            active_indices.append(idx)\n        return vars_copy\n\n    bounds = [(0, 1), (0, 1), (0, 0.5)] * n\n    constraints_dict = {'type': 'ineq', 'fun': constraints, 'jac': constraints_jac}\n    best_sum, best_vars = -1.0, None\n\n    # Phase 1: Multiple starts\n    for start_idx in range(5):\n        vars0 = np.zeros(3 * n)\n        if start_idx == 0:\n            xs = np.linspace(0.1, 0.9, 5)\n            xi, yi = np.meshgrid(xs, xs)\n            vars0[0::3] = np.append(xi.flatten(), 0.5)\n            vars0[1::3] = np.append(yi.flatten(), 0.5)\n            vars0[2::3] = 0.08\n        else:\n            vars0[0::3], vars0[1::3], vars0[2::3] = np.random.rand(n), np.random.rand(n), 0.05\n        \n        # Penalty-based L-BFGS-B Initialization\n        res_p = minimize(penalty_objective, vars0, args=(100,), method='L-BFGS-B', \n                         jac=penalty_grad, bounds=bounds, options={'maxiter': 100})\n        # SLSQP Refinement\n        res = minimize(objective, res_p.x, method='SLSQP', jac=grad_objective,\n                       constraints=constraints_dict, bounds=bounds, options={'maxiter': 200, 'ftol': 1e-7})\n        # LP Radii Refinement\n        opt_r = lp_refine(res.x.reshape(n, 3)[:, :2], n, i_idx, j_idx, res.x[2::3])\n        current_sum = np.sum(opt_r)\n        if current_sum > best_sum:\n            best_sum, best_vars = current_sum, res.x.copy()\n            best_vars[2::3] = opt_r\n\n    # Phase 2: Shake and Refine\n    while time.time() - start_time < 285:\n        vars_shaken = shake(best_vars)\n        res = minimize(objective, vars_shaken, method='SLSQP', jac=grad_objective,\n                       constraints=constraints_dict, bounds=bounds, options={'maxiter': 200, 'ftol': 1e-7})\n        opt_r = lp_refine(res.x.reshape(n, 3)[:, :2], n, i_idx, j_idx, res.x[2::3])\n        current_sum = np.sum(opt_r)\n        if current_sum > best_sum:\n            best_sum, best_vars = current_sum, res.x.copy()\n            best_vars[2::3] = opt_r\n\n    final_centers = best_vars.reshape(n, 3)[:, :2]\n    final_radii = lp_refine(final_centers, n, i_idx, j_idx, best_vars[2::3])\n    return final_centers, final_radii, float(np.sum(final_radii))\n\n```",
  "reasoning": "",
  "timestamp": "2026-02-10T23:22:14.427815",
  "cost_data": null,
  "model_config": {
    "model": "gemini-3-flash-preview",
    "temperature": 0.7,
    "provider_debug": {
      "finish_reason": "STOP",
      "usage_metadata": {
        "prompt_token_count": 4534,
        "candidates_token_count": 3294,
        "thoughts_token_count": 17624
      },
      "response_text_len": 7912
    }
  },
  "llm_call_duration_s": 121.664255812997,
  "eval_duration_s": 285.9916472170735
}