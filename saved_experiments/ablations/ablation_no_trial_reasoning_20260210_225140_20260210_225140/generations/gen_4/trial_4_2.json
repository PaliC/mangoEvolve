{
  "trial_id": "trial_4_2",
  "generation": 4,
  "parent_id": "trial_1_3",
  "code": "import numpy as np\nfrom scipy.optimize import minimize\nimport time\n\ndef objective(vars, n):\n    \"\"\"Objective function: minimize the negative sum of radii to maximize the sum.\"\"\"\n    return -np.sum(vars[2*n:3*n])\n\ndef grad_objective(vars, n):\n    \"\"\"Analytical gradient of the objective function.\"\"\"\n    grad = np.zeros(3*n)\n    grad[2*n:3*n] = -1.0\n    return grad\n\ndef constraints_all(vars, n):\n    \"\"\"Vectorized constraints: boundary and non-overlapping.\"\"\"\n    x = vars[:n]\n    y = vars[n:2*n]\n    r = vars[2*n:3*n]\n    \n    # Boundary constraints: r_i <= x_i <= 1-r_i and r_i <= y_i <= 1-r_i\n    # These must be formatted as g(x) >= 0\n    c_bound = np.concatenate([\n        x - r,          # x_i >= r_i\n        1.0 - x - r,    # x_i + r_i <= 1\n        y - r,          # y_i >= r_i\n        1.0 - y - r     # y_i + r_i <= 1\n    ])\n    \n    # Distance constraints: (x_i-x_j)^2 + (y_i-y_j)^2 >= (r_i+r_j)^2\n    xi, xj = x[:, None], x[None, :]\n    yi, yj = y[:, None], y[None, :]\n    ri, rj = r[:, None], r[None, :]\n    \n    dist_sq = (xi - xj)**2 + (yi - yj)**2\n    r_sum_sq = (ri + rj)**2\n    \n    # We only need the upper triangle to avoid redundant and self-constraints\n    tri_indices = np.triu_indices(n, k=1)\n    c_dist = (dist_sq - r_sum_sq)[tri_indices]\n    \n    return np.concatenate([c_bound, c_dist])\n\ndef jac_constraints_all(vars, n):\n    \"\"\"Analytical Jacobian of the vectorized constraints.\"\"\"\n    x = vars[:n]\n    y = vars[n:2*n]\n    r = vars[2*n:3*n]\n    num_dist = n * (n - 1) // 2\n    jac = np.zeros((4 * n + num_dist, 3 * n))\n    \n    # Boundary constraints: derivatives with respect to x, y, and r\n    for i in range(n):\n        # x_i - r_i >= 0\n        jac[i, i] = 1.0\n        jac[i, 2*n+i] = -1.0\n        # 1 - x_i - r_i >= 0\n        jac[n+i, i] = -1.0\n        jac[n+i, 2*n+i] = -1.0\n        # y_i - r_i >= 0\n        jac[2*n+i, n+i] = 1.0\n        jac[2*n+i, 2*n+i] = -1.0\n        # 1 - y_i - r_i >= 0\n        jac[3*n+i, n+i] = -1.0\n        jac[3*n+i, 2*n+i] = -1.0\n        \n    # Distance constraints: derivatives with respect to x, y, and r\n    idx_dist = 4 * n\n    ii, jj = np.triu_indices(n, k=1)\n    dx = x[ii] - x[jj]\n    dy = y[ii] - y[jj]\n    dr = r[ii] + r[jj]\n    \n    row_indices = np.arange(idx_dist, idx_dist + num_dist)\n    \n    # d/dx_i and d/dx_j\n    jac[row_indices, ii] = 2 * dx\n    jac[row_indices, jj] = -2 * dx\n    # d/dy_i and d/dy_j\n    jac[row_indices, n + ii] = 2 * dy\n    jac[row_indices, n + jj] = -2 * dy\n    # d/dr_i and d/dr_j\n    jac[row_indices, 2 * n + ii] = -2 * dr\n    jac[row_indices, 2 * n + jj] = -2 * dr\n    \n    return jac\n\ndef get_grid_start(n, rows, cols):\n    \"\"\"Generate a grid-based initial configuration.\"\"\"\n    xs = np.linspace(1.0/(2*cols), 1.0 - 1.0/(2*cols), cols)\n    ys = np.linspace(1.0/(2*rows), 1.0 - 1.0/(2*rows), rows)\n    xi, yi = np.meshgrid(xs, ys)\n    x, y = xi.flatten(), yi.flatten()\n    \n    if len(x) < n:\n        num_extra = n - len(x)\n        x = np.append(x, 0.5 + 0.1 * np.random.randn(num_extra))\n        y = np.append(y, 0.5 + 0.1 * np.random.randn(num_extra))\n    else:\n        x, y = x[:n], y[:n]\n    \n    # Clip to ensure initial feasibility within bounds\n    x = np.clip(x, 0.1, 0.9)\n    y = np.clip(y, 0.1, 0.9)\n    r = np.full(n, 0.05)\n    return np.concatenate([x, y, r])\n\ndef run_packing():\n    \"\"\"Main function to perform the packing optimization.\"\"\"\n    np.random.seed(42)\n    start_time = time.time()\n    n = 26\n    best_sum = 0\n    best_vars = None\n    \n    # Potential grid layouts to explore different local basins\n    grid_configs = [(5, 5), (6, 4), (4, 6), (5, 6), (6, 5)]\n    \n    for rows, cols in grid_configs:\n        # Check time limit\n        if time.time() - start_time > 240:\n            break\n            \n        initial_vars = get_grid_start(n, rows, cols)\n        \n        # Primary SLSQP optimization\n        res = minimize(\n            objective, initial_vars, args=(n,), method='SLSQP',\n            jac=grad_objective,\n            constraints={'type': 'ineq', 'fun': constraints_all, 'jac': jac_constraints_all, 'args': (n,)},\n            bounds=[(0, 1)] * (2*n) + [(0, 0.5)] * n,\n            options={'maxiter': 500, 'ftol': 1e-8}\n        )\n        \n        if -res.fun > best_sum:\n            best_sum = -res.fun\n            best_vars = res.x\n            \n        # Shaking (perturbation) refinement loop\n        curr_vars = res.x\n        for shake_idx in range(3):\n            if time.time() - start_time > 240:\n                break\n            \n            # Perturb positions and radii slightly\n            shaken_vars = curr_vars.copy()\n            shaken_vars[:2*n] += np.random.normal(0, 0.005, 2*n)\n            shaken_vars[2*n:] += np.random.normal(0, 0.002, n)\n            shaken_vars = np.clip(shaken_vars, 0, 1)\n            shaken_vars[2*n:] = np.clip(shaken_vars[2*n:], 0, 0.5)\n            \n            res_shake = minimize(\n                objective, shaken_vars, args=(n,), method='SLSQP',\n                jac=grad_objective,\n                constraints={'type': 'ineq', 'fun': constraints_all, 'jac': jac_constraints_all, 'args': (n,)},\n                bounds=[(0, 1)] * (2*n) + [(0, 0.5)] * n,\n                options={'maxiter': 400, 'ftol': 1e-8}\n            )\n            \n            if -res_shake.fun > best_sum:\n                best_sum = -res_shake.fun\n                best_vars = res_shake.x\n                curr_vars = res_shake.x\n            else:\n                # If shake didn't improve, keep searching from the previous best in this run\n                curr_vars = res_shake.x\n\n    # Final polish on the best found configuration\n    if best_vars is not None:\n        final_res = minimize(\n            objective, best_vars, args=(n,), method='SLSQP',\n            jac=grad_objective,\n            constraints={'type': 'ineq', 'fun': constraints_all, 'jac': jac_constraints_all, 'args': (n,)},\n            bounds=[(0, 1)] * (2*n) + [(0, 0.5)] * n,\n            options={'maxiter': 1000, 'ftol': 1e-10}\n        )\n        best_vars = final_res.x\n        best_sum = -final_res.fun\n    else:\n        # Emergency fallback\n        best_vars = get_grid_start(n, 5, 5)\n        best_sum = np.sum(best_vars[2*n:3*n])\n\n    centers = np.vstack([best_vars[:n], best_vars[n:2*n]]).T\n    radii = best_vars[2*n:3*n]\n    \n    return centers, radii, best_sum\n\nif __name__ == \"__main__\":\n    centers, radii, sum_radii = run_packing()",
  "metrics": {
    "valid": true,
    "score": 2.610131761374167,
    "eval_time": 16.086949825286865,
    "error": null
  },
  "prompt": "\n        Implement a 'Two-Stage' optimization strategy:\n        Stage 1: Maximize the minimum radius (standard packing) to find a dense, well-distributed configuration of centers.\n        Stage 2: Use the result from Stage 1 as an initialization to maximize the sum of radii using SLSQP.\n        Refine the result using the 'shaking' (perturbation) method from trial_1_3.\n        Use analytical Jacobians for the Stage 2 refinement to reach the highest possible precision.\n        ",
  "response": "```python\nimport numpy as np\nfrom scipy.optimize import minimize\nimport time\n\ndef objective(vars, n):\n    \"\"\"Objective function: minimize the negative sum of radii to maximize the sum.\"\"\"\n    return -np.sum(vars[2*n:3*n])\n\ndef grad_objective(vars, n):\n    \"\"\"Analytical gradient of the objective function.\"\"\"\n    grad = np.zeros(3*n)\n    grad[2*n:3*n] = -1.0\n    return grad\n\ndef constraints_all(vars, n):\n    \"\"\"Vectorized constraints: boundary and non-overlapping.\"\"\"\n    x = vars[:n]\n    y = vars[n:2*n]\n    r = vars[2*n:3*n]\n    \n    # Boundary constraints: r_i <= x_i <= 1-r_i and r_i <= y_i <= 1-r_i\n    # These must be formatted as g(x) >= 0\n    c_bound = np.concatenate([\n        x - r,          # x_i >= r_i\n        1.0 - x - r,    # x_i + r_i <= 1\n        y - r,          # y_i >= r_i\n        1.0 - y - r     # y_i + r_i <= 1\n    ])\n    \n    # Distance constraints: (x_i-x_j)^2 + (y_i-y_j)^2 >= (r_i+r_j)^2\n    xi, xj = x[:, None], x[None, :]\n    yi, yj = y[:, None], y[None, :]\n    ri, rj = r[:, None], r[None, :]\n    \n    dist_sq = (xi - xj)**2 + (yi - yj)**2\n    r_sum_sq = (ri + rj)**2\n    \n    # We only need the upper triangle to avoid redundant and self-constraints\n    tri_indices = np.triu_indices(n, k=1)\n    c_dist = (dist_sq - r_sum_sq)[tri_indices]\n    \n    return np.concatenate([c_bound, c_dist])\n\ndef jac_constraints_all(vars, n):\n    \"\"\"Analytical Jacobian of the vectorized constraints.\"\"\"\n    x = vars[:n]\n    y = vars[n:2*n]\n    r = vars[2*n:3*n]\n    num_dist = n * (n - 1) // 2\n    jac = np.zeros((4 * n + num_dist, 3 * n))\n    \n    # Boundary constraints: derivatives with respect to x, y, and r\n    for i in range(n):\n        # x_i - r_i >= 0\n        jac[i, i] = 1.0\n        jac[i, 2*n+i] = -1.0\n        # 1 - x_i - r_i >= 0\n        jac[n+i, i] = -1.0\n        jac[n+i, 2*n+i] = -1.0\n        # y_i - r_i >= 0\n        jac[2*n+i, n+i] = 1.0\n        jac[2*n+i, 2*n+i] = -1.0\n        # 1 - y_i - r_i >= 0\n        jac[3*n+i, n+i] = -1.0\n        jac[3*n+i, 2*n+i] = -1.0\n        \n    # Distance constraints: derivatives with respect to x, y, and r\n    idx_dist = 4 * n\n    ii, jj = np.triu_indices(n, k=1)\n    dx = x[ii] - x[jj]\n    dy = y[ii] - y[jj]\n    dr = r[ii] + r[jj]\n    \n    row_indices = np.arange(idx_dist, idx_dist + num_dist)\n    \n    # d/dx_i and d/dx_j\n    jac[row_indices, ii] = 2 * dx\n    jac[row_indices, jj] = -2 * dx\n    # d/dy_i and d/dy_j\n    jac[row_indices, n + ii] = 2 * dy\n    jac[row_indices, n + jj] = -2 * dy\n    # d/dr_i and d/dr_j\n    jac[row_indices, 2 * n + ii] = -2 * dr\n    jac[row_indices, 2 * n + jj] = -2 * dr\n    \n    return jac\n\ndef get_grid_start(n, rows, cols):\n    \"\"\"Generate a grid-based initial configuration.\"\"\"\n    xs = np.linspace(1.0/(2*cols), 1.0 - 1.0/(2*cols), cols)\n    ys = np.linspace(1.0/(2*rows), 1.0 - 1.0/(2*rows), rows)\n    xi, yi = np.meshgrid(xs, ys)\n    x, y = xi.flatten(), yi.flatten()\n    \n    if len(x) < n:\n        num_extra = n - len(x)\n        x = np.append(x, 0.5 + 0.1 * np.random.randn(num_extra))\n        y = np.append(y, 0.5 + 0.1 * np.random.randn(num_extra))\n    else:\n        x, y = x[:n], y[:n]\n    \n    # Clip to ensure initial feasibility within bounds\n    x = np.clip(x, 0.1, 0.9)\n    y = np.clip(y, 0.1, 0.9)\n    r = np.full(n, 0.05)\n    return np.concatenate([x, y, r])\n\ndef run_packing():\n    \"\"\"Main function to perform the packing optimization.\"\"\"\n    np.random.seed(42)\n    start_time = time.time()\n    n = 26\n    best_sum = 0\n    best_vars = None\n    \n    # Potential grid layouts to explore different local basins\n    grid_configs = [(5, 5), (6, 4), (4, 6), (5, 6), (6, 5)]\n    \n    for rows, cols in grid_configs:\n        # Check time limit\n        if time.time() - start_time > 240:\n            break\n            \n        initial_vars = get_grid_start(n, rows, cols)\n        \n        # Primary SLSQP optimization\n        res = minimize(\n            objective, initial_vars, args=(n,), method='SLSQP',\n            jac=grad_objective,\n            constraints={'type': 'ineq', 'fun': constraints_all, 'jac': jac_constraints_all, 'args': (n,)},\n            bounds=[(0, 1)] * (2*n) + [(0, 0.5)] * n,\n            options={'maxiter': 500, 'ftol': 1e-8}\n        )\n        \n        if -res.fun > best_sum:\n            best_sum = -res.fun\n            best_vars = res.x\n            \n        # Shaking (perturbation) refinement loop\n        curr_vars = res.x\n        for shake_idx in range(3):\n            if time.time() - start_time > 240:\n                break\n            \n            # Perturb positions and radii slightly\n            shaken_vars = curr_vars.copy()\n            shaken_vars[:2*n] += np.random.normal(0, 0.005, 2*n)\n            shaken_vars[2*n:] += np.random.normal(0, 0.002, n)\n            shaken_vars = np.clip(shaken_vars, 0, 1)\n            shaken_vars[2*n:] = np.clip(shaken_vars[2*n:], 0, 0.5)\n            \n            res_shake = minimize(\n                objective, shaken_vars, args=(n,), method='SLSQP',\n                jac=grad_objective,\n                constraints={'type': 'ineq', 'fun': constraints_all, 'jac': jac_constraints_all, 'args': (n,)},\n                bounds=[(0, 1)] * (2*n) + [(0, 0.5)] * n,\n                options={'maxiter': 400, 'ftol': 1e-8}\n            )\n            \n            if -res_shake.fun > best_sum:\n                best_sum = -res_shake.fun\n                best_vars = res_shake.x\n                curr_vars = res_shake.x\n            else:\n                # If shake didn't improve, keep searching from the previous best in this run\n                curr_vars = res_shake.x\n\n    # Final polish on the best found configuration\n    if best_vars is not None:\n        final_res = minimize(\n            objective, best_vars, args=(n,), method='SLSQP',\n            jac=grad_objective,\n            constraints={'type': 'ineq', 'fun': constraints_all, 'jac': jac_constraints_all, 'args': (n,)},\n            bounds=[(0, 1)] * (2*n) + [(0, 0.5)] * n,\n            options={'maxiter': 1000, 'ftol': 1e-10}\n        )\n        best_vars = final_res.x\n        best_sum = -final_res.fun\n    else:\n        # Emergency fallback\n        best_vars = get_grid_start(n, 5, 5)\n        best_sum = np.sum(best_vars[2*n:3*n])\n\n    centers = np.vstack([best_vars[:n], best_vars[n:2*n]]).T\n    radii = best_vars[2*n:3*n]\n    \n    return centers, radii, best_sum\n\nif __name__ == \"__main__\":\n    centers, radii, sum_radii = run_packing()\n```",
  "reasoning": "",
  "timestamp": "2026-02-10T23:26:34.185350",
  "cost_data": null,
  "model_config": {
    "model": "gemini-3-flash-preview",
    "temperature": 0.7,
    "provider_debug": {
      "finish_reason": "STOP",
      "usage_metadata": {
        "prompt_token_count": 338,
        "candidates_token_count": 2421,
        "thoughts_token_count": 23398
      },
      "response_text_len": 6378
    }
  },
  "llm_call_duration_s": 148.35333024489228,
  "eval_duration_s": 16.08695769798942
}