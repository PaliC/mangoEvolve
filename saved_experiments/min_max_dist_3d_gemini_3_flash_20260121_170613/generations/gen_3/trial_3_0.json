{
  "trial_id": "trial_3_0",
  "generation": 3,
  "parent_id": "trial_1_1",
  "code": "",
  "metrics": {},
  "prompt": "The following code (trial_1_1) is the current best performer (score=1.000015). \n        It uses Basin-hopping with SLSQP. \n        Improve it by:\n        1. Increasing the number of basin-hopping iterations.\n        2. Using a more robust local optimizer like 'L-BFGS-B' or 'trust-constr' with high precision (ftol, gtol).\n        3. Implementing a 'polishing' step: after basin-hopping, run a very high-precision SLSQP or trust-region optimization.\n        4. Ensure the normalization (max_dist = 1) is handled carefully within the objective function or as a constraint.\n        \n        import numpy as np\nfrom scipy.optimize import minimize, basinhopping\nimport time\n\ndef min_max_dist_dim3_14():\n    \"\"\"\n    Finds the coordinates of 14 points in 3D space that maximize the ratio \n    (min_dist / max_dist)^2 using Basin-hopping with SLSQP.\n    \"\"\"\n    N = 14\n    dim = 3\n    num_pairs = N * (N - 1) // 2\n    \n    # Precompute indices for all pairs to vectorize distance calculations\n    triu_indices = np.triu_indices(N, k=1)\n    idx_i = triu_indices[0]\n    idx_j = triu_indices[1]\n    \n    # Initialize points on a Fibonacci sphere for a near-optimal starting configuration\n    # This reduces the time needed for convergence compared to random initialization\n    indices = np.arange(0, N, dtype=float) + 0.5\n    phi = np.arccos(1 - 2*indices/N)\n    theta = np.pi * (1 + 5**0.5) * indices\n    x_init = np.cos(theta) * np.sin(phi)\n    y_init = np.sin(theta) * np.sin(phi)\n    z_init = np.cos(phi)\n    points_init = np.column_stack((x_init, y_init, z_init))\n    \n    # Scale to approximate diameter 1.0 (Unit sphere has diameter 2.0)\n    points_init *= 0.5\n    \n    # Calculate initial squared min distance\n    diffs_init = points_init[idx_i] - points_init[idx_j]\n    d2_init = np.sum(diffs_init**2, axis=1)\n    s_init = np.min(d2_init)\n    \n    # State vector: [x1, y1, z1, ..., x14, y14, z14, s]\n    x0 = np.concatenate([points_init.flatten(), [s_init]])\n    \n    # Objective: Maximize s => Minimize -s\n    def objective(x):\n        return -x[-1]\n\n    # Gradient of objective\n    def obj_jac(x):\n        grad = np.zeros_like(x)\n        grad[-1] = -1.0\n        return grad\n    \n    # Constraint functions for SLSQP\n    # 1. dist^2_ij >= s  => dist^2_ij - s >= 0\n    # 2. dist^2_ij <= 1  => 1 - dist^2_ij >= 0\n    def constraints(x):\n        points = x[:-1].reshape((N, dim))\n        s = x[-1]\n        diffs = points[idx_i] - points[idx_j]\n        d2 = np.sum(diffs**2, axis=1)\n        return np.concatenate([d2 - s, 1.0 - d2])\n\n    # Jacobian of constraints\n    def constraints_jac(x):\n        points = x[:-1].reshape((N, dim))\n        diffs = points[idx_i] - points[idx_j] # Shape (num_pairs, 3)\n        two_diffs = 2 * diffs\n        \n        # Jacobian matrix shape: (2 * num_pairs, N * dim + 1)\n        jac = np.zeros((2 * num_pairs, N * dim + 1))\n        \n        # We fill the matrix efficiently\n        # Constraint 1: d2 - s >= 0\n        # Derivatives w.r.t p_i are 2*(p_i - p_j), w.r.t p_j are -2*(p_i - p_j)\n        # Derivative w.r.t s is -1\n        \n        # Constraint 2: 1 - d2 >= 0\n        # Derivatives are negative of above (s term is 0)\n        \n        for k in range(num_pairs):\n            i = idx_i[k]\n            j = idx_j[k]\n            \n            # Indices in the flattened state vector\n            sl_i = slice(i*3, i*3+3)\n            sl_j = slice(j*3, j*3+3)\n            \n            # Row k (d2 - s)\n            jac[k, sl_i] = two_diffs[k]\n            jac[k, sl_j] = -two_diffs[k]\n            jac[k, -1] = -1.0\n            \n            # Row num_pairs + k (1 - d2)\n            jac[num_pairs + k, sl_i] = -two_diffs[k]\n            jac[num_pairs + k, sl_j] = two_diffs[k]\n            \n        return jac\n\n    # Define constraints dictionary\n    cons = {'type': 'ineq', 'fun': constraints, 'jac': constraints_jac}\n    \n    # Custom step taker for Basin-hopping\n    # Perturbs points and normalizes them to help the optimizer\n    class NormalizedStep:\n        def __init__(self, stepsize=0.1):\n            self.stepsize = stepsize\n            self.rng = np.random.default_rng()\n            \n        def __call__(self, x):\n            coords = x[:-1].reshape((N, dim))\n            perturbation = self.rng.uniform(-self.stepsize, self.stepsize, size=coords.shape)\n            new_coords = coords + perturbation\n            \n            # Normalize configuration to have max dist = 1.0\n            # This satisfies the prompt's instruction and keeps the search feasible\n            diffs = new_coords[idx_i] - new_coords[idx_j]\n            dists = np.sqrt(np.sum(diffs**2, axis=1))\n            max_d = np.max(dists)\n            if max_d > 1e-9:\n                new_coords /= max_d\n            \n            # Reset s to a feasible value (slightly less than min_dist^2)\n            new_diffs = new_coords[idx_i] - new_coords[idx_j]\n            new_s = np.min(np.sum(new_diffs**2, axis=1)) * 0.95\n            \n            return np.concatenate([new_coords.flatten(), [new_s]])\n\n    take_step = NormalizedStep(stepsize=0.15)\n    \n    # Minimizer options\n    minimizer_kwargs = {\n        'method': 'SLSQP',\n        'jac': obj_jac,\n        'constraints': cons,\n        'options': {'ftol': 1e-6, 'maxiter': 200}\n    }\n    \n    # Time management\n    start_time = time.time()\n    time_limit = 55 # seconds\n    \n    # Initial local minimization\n    res = minimize(objective, x0, jac=obj_jac, constraints=cons, method='SLSQP')\n    best_x = res.x\n    best_fun = res.fun\n    \n    # Basin-hopping loop\n    # We run short bursts of basin-hopping and check time\n    while time.time() - start_time < time_limit:\n        # Restart from current best to perform iterated local search\n        bh_res = basinhopping(objective, best_x, niter=5, \n                              minimizer_kwargs=minimizer_kwargs, \n                              take_step=take_step)\n        \n        # Check if we found a better solution\n        if bh_res.lowest_optimization_result.fun < best_fun:\n            best_fun = bh_res.lowest_optimization_result.fun\n            best_x = bh_res.lowest_optimization_result.x\n\n    # Extract final points\n    final_points = best_x[:-1].reshape((N, dim))\n    \n    # Final normalization to ensure max_dist is exactly 1.0\n    diffs = final_points[idx_i] - final_points[idx_j]\n    max_d = np.sqrt(np.max(np.sum(diffs**2, axis=1)))\n    if max_d > 0:\n        final_points /= max_d\n        \n    return final_points",
  "response": "Here is the improved solution using Iterated Local Search with SLSQP, vectorized Jacobian calculations, and a robust scoring metric to guide the search.\n\n```python\nimport numpy as np\nfrom scipy.optimize import minimize\nimport time\n\ndef min_max_dist_dim3_14():\n    \"\"\"\n    Finds the coordinates of 14 points in 3D space that maximize the ratio \n    (min_dist / max_dist)^2 using an Iterated Local Search with SLSQP and \n    vectorized derivatives.\n    \"\"\"\n    N = 14\n    dim = 3\n    num_pairs = N * (N - 1) // 2\n    \n    # Precompute indices for vectorization of distance calculations and Jacobian\n    triu_indices = np.triu_indices(N, k=1)\n    idx_i = triu_indices[0]\n    idx_j = triu_indices[1]\n    \n    # Precompute indices for the Jacobian matrix construction\n    # The Jacobian has shape (2 * num_pairs, 3 * N + 1)\n    rows_lower = np.arange(num_pairs)\n    rows_upper = np.arange(num_pairs) + num_pairs\n    \n    # Column indices for point i coordinates (x, y, z)\n    col_ix = idx_i * 3\n    col_iy = idx_i * 3 + 1\n    col_iz = idx_i * 3 + 2\n    \n    # Column indices for point j coordinates (x, y, z)\n    col_jx = idx_j * 3\n    col_jy = idx_j * 3 + 1\n    col_jz = idx_j * 3 + 2\n\n    # --- Optimization Functions ---\n\n    def objective(x):\n        # State vector x contains flattened coordinates and the scalar s\n        # x = [p1_x, p1_y, p1_z, ..., pN_z, s]\n        # Objective: Maximize s (squared min distance) -> Minimize -s\n        return -x[-1]\n\n    def obj_jac(x):\n        # Gradient of the objective function\n        grad = np.zeros_like(x)\n        grad[-1] = -1.0\n        return grad\n    \n    def constraints(x):\n        pts = x[:-1].reshape((N, 3))\n        s = x[-1]\n        \n        # Vectorized pairwise squared distances\n        p_i = pts[idx_i]\n        p_j = pts[idx_j]\n        d2 = np.sum((p_i - p_j)**2, axis=1)\n        \n        # Constraints:\n        # 1. d^2_ij >= s  => d^2_ij - s >= 0\n        # 2. d^2_ij <= 1  => 1 - d^2_ij >= 0\n        return np.concatenate([d2 - s, 1.0 - d2])\n\n    def constraints_jac(x):\n        pts = x[:-1].reshape((N, 3))\n        p_i = pts[idx_i]\n        p_j = pts[idx_j]\n        diffs = p_i - p_j # Shape: (num_pairs, 3)\n        two_diffs = 2 * diffs\n        \n        # Initialize sparse-like Jacobian matrix\n        jac = np.zeros((2 * num_pairs, 3 * N + 1))\n        \n        # --- Constraint 1: d^2 - s >= 0 ---\n        # Derivatives w.r.t point i: 2(pi - pj)\n        jac[rows_lower, col_ix] = two_diffs[:, 0]\n        jac[rows_lower, col_iy] = two_diffs[:, 1]\n        jac[rows_lower, col_iz] = two_diffs[:, 2]\n        \n        # Derivatives w.r.t point j: -2(pi - pj)\n        jac[rows_lower, col_jx] = -two_diffs[:, 0]\n        jac[rows_lower, col_jy] = -two_diffs[:, 1]\n        jac[rows_lower, col_jz] = -two_diffs[:, 2]\n        \n        # Derivative w.r.t s is -1\n        jac[rows_lower, -1] = -1.0\n        \n        # --- Constraint 2: 1 - d^2 >= 0 ---\n        # Derivatives are simply the negative of the spatial derivatives above\n        # The derivative w.r.t s is 0 (implicit in initialization)\n        jac[rows_upper, :-1] = -jac[rows_lower, :-1]\n        \n        return jac\n\n    # Constraint dictionary for SLSQP\n    cons = {'type': 'ineq', 'fun': constraints, 'jac': constraints_jac}\n    \n    # --- Initialization ---\n    \n    # Initialize points on a Fibonacci sphere for a good starting distribution\n    indices = np.arange(0, N, dtype=float) + 0.5\n    phi = np.arccos(1 - 2*indices/N)\n    theta = np.pi * (1 + 5**0.5) * indices\n    x_init = np.cos(theta) * np.sin(phi)\n    y_init = np.sin(theta) * np.sin(phi)\n    z_init = np.cos(phi)\n    points = np.column_stack((x_init, y_init, z_init))\n    \n    # Normalize diameter to 1.0\n    diffs = points[idx_i] - points[idx_j]\n    max_d2 = np.max(np.sum(diffs**2, axis=1))\n    points /= np.sqrt(max_d2)\n    \n    # Initial s (min squared distance)\n    diffs = points[idx_i] - points[idx_j]\n    min_d2 = np.min(np.sum(diffs**2, axis=1))\n    x0 = np.concatenate([points.flatten(), [min_d2]])\n    \n    # --- Optimization Loop ---\n    \n    start_time = time.time()\n    time_limit = 58.0 # Use nearly all available 60s\n    \n    # Helper to calculate the true metric for comparison\n    def get_true_score(x_vec):\n        pts = x_vec[:-1].reshape((N, 3))\n        # Calculate pairwise squared distances\n        p_diffs = pts[idx_i] - pts[idx_j]\n        p_d2 = np.sum(p_diffs**2, axis=1)\n        p_max_d2 = np.max(p_d2)\n        if p_max_d2 < 1e-12: return 0.0\n        # The score is (min_dist / max_dist)^2 = min_d2 / max_d2\n        return np.min(p_d2) / p_max_d2\n\n    # 1. Baseline Optimization\n    res = minimize(objective, x0, jac=obj_jac, constraints=cons, method='SLSQP', \n                   options={'ftol': 1e-6, 'maxiter': 500})\n    \n    best_x = res.x\n    best_score = get_true_score(best_x)\n    \n    # 2. Iterated Local Search\n    rng = np.random.default_rng()\n    step_size = 0.15\n    \n    while time.time() - start_time < time_limit - 5.0: # Reserve 5s for polishing\n        \n        # Perturbation: Shake the current best points\n        current_pts = best_x[:-1].reshape((N, 3))\n        perturbation = rng.uniform(-step_size, step_size, size=(N, 3))\n        candidate_pts = current_pts + perturbation\n        \n        # Re-normalize diameter roughly to keep solver stable\n        c_diffs = candidate_pts[idx_i] - candidate_pts[idx_j]\n        c_max_d = np.sqrt(np.max(np.sum(c_diffs**2, axis=1)))\n        if c_max_d > 1e-9:\n            candidate_pts /= c_max_d\n            \n        # Reset s to a feasible value\n        c_diffs = candidate_pts[idx_i] - candidate_pts[idx_j]\n        c_s = np.min(np.sum(c_diffs**2, axis=1)) * 0.9\n        \n        x_cand = np.concatenate([candidate_pts.flatten(), [c_s]])\n        \n        # Local Minimization (Lower precision for speed)\n        res = minimize(objective, x_cand, jac=obj_jac, constraints=cons, method='SLSQP', \n                       options={'ftol': 1e-4, 'maxiter': 150})\n        \n        # Check if we found a better configuration based on the true ratio\n        current_score = get_true_score(res.x)\n        if current_score > best_score:\n            best_score = current_score\n            best_x = res.x\n            \n    # 3. Polishing Step\n    # Run a high-precision optimization on the best candidate found\n    res_final = minimize(objective, best_x, jac=obj_jac, constraints=cons, method='SLSQP', \n                         options={'ftol': 1e-12, 'maxiter': 1000})\n    \n    final_pts = res_final.x[:-1].reshape((N, 3))\n    \n    # --- Final Normalization ---\n    # Ensure the returned points have exactly max_dist = 1.0\n    diffs = final_pts[idx_i] - final_pts[idx_j]\n    max_d = np.sqrt(np.max(np.sum(diffs**2, axis=1)))\n    if max_d > 1e-12:\n        final_pts /= max_d\n        \n    return final_pts",
  "reasoning": "Here is the improved solution using Iterated Local Search with SLSQP, vectorized Jacobian calculations, and a robust scoring metric to guide the search.\n\n```python\nimport numpy as np\nfrom scipy.optimize import minimize\nimport time\n\ndef min_max_dist_dim3_14():\n    \"\"\"\n    Finds the coordinates of 14 points in 3D space that maximize the ratio \n    (min_dist / max_dist)^2 using an Iterated Local Search with SLSQP and \n    vectorized derivatives.\n    \"\"\"\n    N = 14\n    dim = 3\n    num_pairs = N * (N - 1) // 2\n    \n    # Precompute indices for vectorization of distance calculations and Jacobian\n    triu_indices = np.triu_indices(N, k=1)\n    idx_i = triu_indices[0]\n    idx_j = triu_indices[1]\n    \n    # Precompute indices for the Jacobian matrix construction\n    # The Jacobian has shape (2 * num_pairs, 3 * N + 1)\n    rows_lower = np.arange(num_pairs)\n    rows_upper = np.arange(num_pairs) + num_pairs\n    \n    # Column indices for point i coordinates (x, y, z)\n    col_ix = idx_i * 3\n    col_iy = idx_i * 3 + 1\n    col_iz = idx_i * 3 + 2\n    \n    # Column indices for point j coordinates (x, y, z)\n    col_jx = idx_j * 3\n    col_jy = idx_j * 3 + 1\n    col_jz = idx_j * 3 + 2\n\n    # --- Optimization Functions ---\n\n    def objective(x):\n        # State vector x contains flattened coordinates and the scalar s\n        # x = [p1_x, p1_y, p1_z, ..., pN_z, s]\n        # Objective: Maximize s (squared min distance) -> Minimize -s\n        return -x[-1]\n\n    def obj_jac(x):\n        # Gradient of the objective function\n        grad = np.zeros_like(x)\n        grad[-1] = -1.0\n        return grad\n    \n    def constraints(x):\n        pts = x[:-1].reshape((N, 3))\n        s = x[-1]\n        \n        # Vectorized pairwise squared distances\n        p_i = pts[idx_i]\n        p_j = pts[idx_j]\n        d2 = np.sum((p_i - p_j)**2, axis=1)\n        \n        # Constraints:\n        # 1. d^2_ij >= s  => d^2_ij - s >= 0\n        # 2. d^2_ij <= 1  => 1 - d^2_ij >= 0\n        return np.concatenate([d2 - s, 1.0 - d2])\n\n    def constraints_jac(x):\n        pts = x[:-1].reshape((N, 3))\n        p_i = pts[idx_i]\n        p_j = pts[idx_j]\n        diffs = p_i - p_j # Shape: (num_pairs, 3)\n        two_diffs = 2 * diffs\n        \n        # Initialize sparse-like Jacobian matrix\n        jac = np.zeros((2 * num_pairs, 3 * N + 1))\n        \n        # --- Constraint 1: d^2 - s >= 0 ---\n        # Derivatives w.r.t point i: 2(pi - pj)\n        jac[rows_lower, col_ix] = two_diffs[:, 0]\n        jac[rows_lower, col_iy] = two_diffs[:, 1]\n        jac[rows_lower, col_iz] = two_diffs[:, 2]\n        \n        # Derivatives w.r.t point j: -2(pi - pj)\n        jac[rows_lower, col_jx] = -two_diffs[:, 0]\n        jac[rows_lower, col_jy] = -two_diffs[:, 1]\n        jac[rows_lower, col_jz] = -two_diffs[:, 2]\n        \n        # Derivative w.r.t s is -1\n        jac[rows_lower, -1] = -1.0\n        \n        # --- Constraint 2: 1 - d^2 >= 0 ---\n        # Derivatives are simply the negative of the spatial derivatives above\n        # The derivative w.r.t s is 0 (implicit in initialization)\n        jac[rows_upper, :-1] = -jac[rows_lower, :-1]\n        \n        return jac\n\n    # Constraint dictionary for SLSQP\n    cons = {'type': 'ineq', 'fun': constraints, 'jac': constraints_jac}\n    \n    # --- Initialization ---\n    \n    # Initialize points on a Fibonacci sphere for a good starting distribution\n    indices = np.arange(0, N, dtype=float) + 0.5\n    phi = np.arccos(1 - 2*indices/N)\n    theta = np.pi * (1 + 5**0.5) * indices\n    x_init = np.cos(theta) * np.sin(phi)\n    y_init = np.sin(theta) * np.sin(phi)\n    z_init = np.cos(phi)\n    points = np.column_stack((x_init, y_init, z_init))\n    \n    # Normalize diameter to 1.0\n    diffs = points[idx_i] - points[idx_j]\n    max_d2 = np.max(np.sum(diffs**2, axis=1))\n    points /= np.sqrt(max_d2)\n    \n    # Initial s (min squared distance)\n    diffs = points[idx_i] - points[idx_j]\n    min_d2 = np.min(np.sum(diffs**2, axis=1))\n    x0 = np.concatenate([points.flatten(), [min_d2]])\n    \n    # --- Optimization Loop ---\n    \n    start_time = time.time()\n    time_limit = 58.0 # Use nearly all available 60s\n    \n    # Helper to calculate the true metric for comparison\n    def get_true_score(x_vec):\n        pts = x_vec[:-1].reshape((N, 3))\n        # Calculate pairwise squared distances\n        p_diffs = pts[idx_i] - pts[idx_j]\n        p_d2 = np.sum(p_diffs**2, axis=1)\n        p_max_d2 = np.max(p_d2)\n        if p_max_d2 < 1e-12: return 0.0\n        # The score is (min_dist / max_dist)^2 = min_d2 / max_d2\n        return np.min(p_d2) / p_max_d2\n\n    # 1. Baseline Optimization\n    res = minimize(objective, x0, jac=obj_jac, constraints=cons, method='SLSQP', \n                   options={'ftol': 1e-6, 'maxiter': 500})\n    \n    best_x = res.x\n    best_score = get_true_score(best_x)\n    \n    # 2. Iterated Local Search\n    rng = np.random.default_rng()\n    step_size = 0.15\n    \n    while time.time() - start_time < time_limit - 5.0: # Reserve 5s for polishing\n        \n        # Perturbation: Shake the current best points\n        current_pts = best_x[:-1].reshape((N, 3))\n        perturbation = rng.uniform(-step_size, step_size, size=(N, 3))\n        candidate_pts = current_pts + perturbation\n        \n        # Re-normalize diameter roughly to keep solver stable\n        c_diffs = candidate_pts[idx_i] - candidate_pts[idx_j]\n        c_max_d = np.sqrt(np.max(np.sum(c_diffs**2, axis=1)))\n        if c_max_d > 1e-9:\n            candidate_pts /= c_max_d\n            \n        # Reset s to a feasible value\n        c_diffs = candidate_pts[idx_i] - candidate_pts[idx_j]\n        c_s = np.min(np.sum(c_diffs**2, axis=1)) * 0.9\n        \n        x_cand = np.concatenate([candidate_pts.flatten(), [c_s]])\n        \n        # Local Minimization (Lower precision for speed)\n        res = minimize(objective, x_cand, jac=obj_jac, constraints=cons, method='SLSQP', \n                       options={'ftol': 1e-4, 'maxiter': 150})\n        \n        # Check if we found a better configuration based on the true ratio\n        current_score = get_true_score(res.x)\n        if current_score > best_score:\n            best_score = current_score\n            best_x = res.x\n            \n    # 3. Polishing Step\n    # Run a high-precision optimization on the best candidate found\n    res_final = minimize(objective, best_x, jac=obj_jac, constraints=cons, method='SLSQP', \n                         options={'ftol': 1e-12, 'maxiter': 1000})\n    \n    final_pts = res_final.x[:-1].reshape((N, 3))\n    \n    # --- Final Normalization ---\n    # Ensure the returned points have exactly max_dist = 1.0\n    diffs = final_pts[idx_i] - final_pts[idx_j]\n    max_d = np.sqrt(np.max(np.sum(diffs**2, axis=1)))\n    if max_d > 1e-12:\n        final_pts /= max_d\n        \n    return final_pts",
  "timestamp": "2026-01-21T17:25:46.578303",
  "cost_data": null,
  "model_config": {
    "model": "gemini-3-pro-preview",
    "temperature": 0.7,
    "provider_debug": {
      "finish_reason": "STOP",
      "usage_metadata": {
        "prompt_token_count": 2288,
        "candidates_token_count": 2383,
        "thoughts_token_count": 10380
      },
      "response_text_len": 6761
    }
  }
}