{
  "trial_id": "trial_3_5",
  "generation": 3,
  "parent_id": null,
  "code": "import numpy as np\nfrom scipy.optimize import minimize\nimport time\n\ndef min_max_dist_dim3_14():\n    \"\"\"\n    Optimizes the configuration of 14 points in 3D space to maximize the ratio \n    of the minimum pairwise distance to the maximum pairwise distance.\n    \n    Method:\n    - Uses L-BFGS-B optimization.\n    - Smooths the min and max functions using the LogSumExp trick.\n    - Maximizes S_min / S_max (equivalent to minimizing log(S_max) - log(S_min)).\n    - Performs random restarts to avoid local optima.\n    \"\"\"\n    \n    N = 14\n    dim = 3\n    \n    # Hyperparameter for LogSumExp smoothing.\n    # A value of 80 is chosen to be high enough for good approximation\n    # but low enough to avoid floating point overflow within the bounds.\n    k = 80.0\n    \n    # Precompute indices for the upper triangle of the distance matrix\n    # to efficiently vectorize pairwise computations.\n    triu_idx = np.triu_indices(N, k=1)\n    \n    def objective_func(p_flat):\n        \"\"\"\n        Calculates the smoothed objective value and its analytical gradient.\n        Objective: log(S_max) - log(S_min)\n        \"\"\"\n        P = p_flat.reshape((N, dim))\n        \n        # Vectorized calculation of difference vectors: P[i] - P[j]\n        # Shape: (N, N, 3)\n        diff_tensor = P[:, np.newaxis, :] - P[np.newaxis, :, :]\n        \n        # Extract upper triangle differences corresponding to unique pairs\n        # Shape: (M, 3) where M = N*(N-1)/2\n        diff_upper = diff_tensor[triu_idx]\n        \n        # Euclidean distances\n        dists_sq = np.sum(diff_upper**2, axis=1)\n        dists = np.sqrt(dists_sq)\n        \n        # Avoid division by zero in gradient\n        dists = np.maximum(dists, 1e-9)\n        \n        # --- Smooth Max (LogSumExp) ---\n        # S_max approx max(dists)\n        # S_max = (1/k) * log(sum(exp(k*d)))\n        # Implemented with shift for numerical stability\n        max_d = np.max(dists)\n        exp_max = np.exp(k * (dists - max_d))\n        sum_exp_max = np.sum(exp_max)\n        S_max = max_d + np.log(sum_exp_max) / k\n        w_max = exp_max / sum_exp_max  # Softmax weights\n        \n        # --- Smooth Min (LogSumExp) ---\n        # S_min approx min(dists)\n        # S_min = (-1/k) * log(sum(exp(-k*d)))\n        # Implemented with shift for numerical stability\n        min_d = np.min(dists)\n        exp_min = np.exp(-k * (dists - min_d))\n        sum_exp_min = np.sum(exp_min)\n        S_min = min_d - np.log(sum_exp_min) / k\n        w_min = exp_min / sum_exp_min  # Softmax weights\n        \n        # Objective function to minimize\n        obj_val = np.log(S_max) - np.log(S_min)\n        \n        # --- Analytical Gradient ---\n        # Derivative of objective w.r.t each distance d_ij\n        # dL/d(d_ij) = (w_max_ij / S_max) - (w_min_ij / S_min)\n        grad_d = (w_max / S_max) - (w_min / S_min)\n        \n        # Chain rule to get derivative w.r.t coordinates P\n        # d(d_ij)/d(P_i) = (P_i - P_j) / d_ij\n        # d(d_ij)/d(P_j) = -(P_i - P_j) / d_ij\n        \n        factor = (grad_d / dists)[:, np.newaxis]\n        grad_contrib = diff_upper * factor  # Shape (M, 3)\n        \n        grad_P = np.zeros_like(P)\n        # Accumulate gradients for P_i (positive contribution)\n        np.add.at(grad_P, triu_idx[0], grad_contrib)\n        # Accumulate gradients for P_j (negative contribution)\n        np.subtract.at(grad_P, triu_idx[1], grad_contrib)\n        \n        return obj_val, grad_P.flatten()\n\n    # --- Optimization Loop ---\n    \n    best_ratio = -1.0\n    # Initialize with a random configuration\n    best_P = np.random.uniform(-1, 1, (N, dim))\n    \n    start_time = time.time()\n    # Leave a small buffer before 60s timeout\n    time_limit = 58.0 \n    \n    # Bounds prevent points from drifting too far, keeping exponentials in check\n    bounds = [(-1.7, 1.7)] * (N * dim)\n    \n    num_restarts = 0\n    max_restarts = 100\n    \n    while num_restarts < max_restarts:\n        if time.time() - start_time > time_limit:\n            break\n            \n        num_restarts += 1\n        \n        # Random initialization: mix of Uniform box and Gaussian blob\n        if num_restarts % 2 == 0:\n            p0 = np.random.uniform(-1.0, 1.0, (N, dim))\n        else:\n            p0 = np.random.randn(N, dim)\n            \n        try:\n            # L-BFGS-B optimization\n            res = minimize(\n                objective_func, \n                p0.flatten(), \n                method='L-BFGS-B', \n                jac=True, \n                bounds=bounds,\n                options={'maxiter': 300, 'ftol': 1e-5}\n            )\n            \n            P_cand = res.x.reshape((N, dim))\n            \n            # Evaluate the true (non-smoothed) ratio\n            diff = P_cand[:, np.newaxis, :] - P_cand[np.newaxis, :, :]\n            diff_upper = diff[triu_idx]\n            dists = np.sqrt(np.sum(diff_upper**2, axis=1))\n            \n            d_min = np.min(dists)\n            d_max = np.max(dists)\n            \n            if d_max > 1e-9:\n                ratio = d_min / d_max\n                if ratio > best_ratio:\n                    best_ratio = ratio\n                    best_P = P_cand.copy()\n                    \n        except Exception:\n            continue\n            \n    return best_P",
  "metrics": {
    "valid": true,
    "score": 0.9916482273132637,
    "min_max_ratio": 0.23804224414635827,
    "min_distance": 1.7199241904254943,
    "max_distance": 3.5251880305584993,
    "eval_time": 1.05436110496521,
    "error": null
  },
  "prompt": "Optimize the 14-point configuration using L-BFGS-B.\n        - The objective is max(min(d_ij) / max(d_ij)).\n        - Since min/max are not differentiable, use the LogSumExp trick for BOTH min and max:\n          min(x) approx -1/k * log(sum(exp(-k*x)))\n          max(x) approx 1/k * log(sum(exp(k*x)))\n        - Provide the analytical gradient of this LogSumExp-based objective function to the optimizer for higher precision.\n        - Perform 50-100 random restarts.",
  "response": "```python\nimport numpy as np\nfrom scipy.optimize import minimize\nimport time\n\ndef min_max_dist_dim3_14():\n    \"\"\"\n    Optimizes the configuration of 14 points in 3D space to maximize the ratio \n    of the minimum pairwise distance to the maximum pairwise distance.\n    \n    Method:\n    - Uses L-BFGS-B optimization.\n    - Smooths the min and max functions using the LogSumExp trick.\n    - Maximizes S_min / S_max (equivalent to minimizing log(S_max) - log(S_min)).\n    - Performs random restarts to avoid local optima.\n    \"\"\"\n    \n    N = 14\n    dim = 3\n    \n    # Hyperparameter for LogSumExp smoothing.\n    # A value of 80 is chosen to be high enough for good approximation\n    # but low enough to avoid floating point overflow within the bounds.\n    k = 80.0\n    \n    # Precompute indices for the upper triangle of the distance matrix\n    # to efficiently vectorize pairwise computations.\n    triu_idx = np.triu_indices(N, k=1)\n    \n    def objective_func(p_flat):\n        \"\"\"\n        Calculates the smoothed objective value and its analytical gradient.\n        Objective: log(S_max) - log(S_min)\n        \"\"\"\n        P = p_flat.reshape((N, dim))\n        \n        # Vectorized calculation of difference vectors: P[i] - P[j]\n        # Shape: (N, N, 3)\n        diff_tensor = P[:, np.newaxis, :] - P[np.newaxis, :, :]\n        \n        # Extract upper triangle differences corresponding to unique pairs\n        # Shape: (M, 3) where M = N*(N-1)/2\n        diff_upper = diff_tensor[triu_idx]\n        \n        # Euclidean distances\n        dists_sq = np.sum(diff_upper**2, axis=1)\n        dists = np.sqrt(dists_sq)\n        \n        # Avoid division by zero in gradient\n        dists = np.maximum(dists, 1e-9)\n        \n        # --- Smooth Max (LogSumExp) ---\n        # S_max approx max(dists)\n        # S_max = (1/k) * log(sum(exp(k*d)))\n        # Implemented with shift for numerical stability\n        max_d = np.max(dists)\n        exp_max = np.exp(k * (dists - max_d))\n        sum_exp_max = np.sum(exp_max)\n        S_max = max_d + np.log(sum_exp_max) / k\n        w_max = exp_max / sum_exp_max  # Softmax weights\n        \n        # --- Smooth Min (LogSumExp) ---\n        # S_min approx min(dists)\n        # S_min = (-1/k) * log(sum(exp(-k*d)))\n        # Implemented with shift for numerical stability\n        min_d = np.min(dists)\n        exp_min = np.exp(-k * (dists - min_d))\n        sum_exp_min = np.sum(exp_min)\n        S_min = min_d - np.log(sum_exp_min) / k\n        w_min = exp_min / sum_exp_min  # Softmax weights\n        \n        # Objective function to minimize\n        obj_val = np.log(S_max) - np.log(S_min)\n        \n        # --- Analytical Gradient ---\n        # Derivative of objective w.r.t each distance d_ij\n        # dL/d(d_ij) = (w_max_ij / S_max) - (w_min_ij / S_min)\n        grad_d = (w_max / S_max) - (w_min / S_min)\n        \n        # Chain rule to get derivative w.r.t coordinates P\n        # d(d_ij)/d(P_i) = (P_i - P_j) / d_ij\n        # d(d_ij)/d(P_j) = -(P_i - P_j) / d_ij\n        \n        factor = (grad_d / dists)[:, np.newaxis]\n        grad_contrib = diff_upper * factor  # Shape (M, 3)\n        \n        grad_P = np.zeros_like(P)\n        # Accumulate gradients for P_i (positive contribution)\n        np.add.at(grad_P, triu_idx[0], grad_contrib)\n        # Accumulate gradients for P_j (negative contribution)\n        np.subtract.at(grad_P, triu_idx[1], grad_contrib)\n        \n        return obj_val, grad_P.flatten()\n\n    # --- Optimization Loop ---\n    \n    best_ratio = -1.0\n    # Initialize with a random configuration\n    best_P = np.random.uniform(-1, 1, (N, dim))\n    \n    start_time = time.time()\n    # Leave a small buffer before 60s timeout\n    time_limit = 58.0 \n    \n    # Bounds prevent points from drifting too far, keeping exponentials in check\n    bounds = [(-1.7, 1.7)] * (N * dim)\n    \n    num_restarts = 0\n    max_restarts = 100\n    \n    while num_restarts < max_restarts:\n        if time.time() - start_time > time_limit:\n            break\n            \n        num_restarts += 1\n        \n        # Random initialization: mix of Uniform box and Gaussian blob\n        if num_restarts % 2 == 0:\n            p0 = np.random.uniform(-1.0, 1.0, (N, dim))\n        else:\n            p0 = np.random.randn(N, dim)\n            \n        try:\n            # L-BFGS-B optimization\n            res = minimize(\n                objective_func, \n                p0.flatten(), \n                method='L-BFGS-B', \n                jac=True, \n                bounds=bounds,\n                options={'maxiter': 300, 'ftol': 1e-5}\n            )\n            \n            P_cand = res.x.reshape((N, dim))\n            \n            # Evaluate the true (non-smoothed) ratio\n            diff = P_cand[:, np.newaxis, :] - P_cand[np.newaxis, :, :]\n            diff_upper = diff[triu_idx]\n            dists = np.sqrt(np.sum(diff_upper**2, axis=1))\n            \n            d_min = np.min(dists)\n            d_max = np.max(dists)\n            \n            if d_max > 1e-9:\n                ratio = d_min / d_max\n                if ratio > best_ratio:\n                    best_ratio = ratio\n                    best_P = P_cand.copy()\n                    \n        except Exception:\n            continue\n            \n    return best_P\n```",
  "reasoning": "",
  "timestamp": "2026-01-21T17:26:08.159176",
  "cost_data": null,
  "model_config": {
    "model": "gemini-3-pro-preview",
    "temperature": 0.7,
    "provider_debug": {
      "finish_reason": "STOP",
      "usage_metadata": {
        "prompt_token_count": 313,
        "candidates_token_count": 1687,
        "thoughts_token_count": 13423
      },
      "response_text_len": 5244
    }
  }
}