{
  "trial_id": "trial_3_4",
  "generation": 3,
  "parent_id": "trial_1_1",
  "code": "import numpy as np\nfrom scipy.optimize import minimize\nimport time\n\ndef min_max_dist_dim3_14():\n    \"\"\"\n    Finds the coordinates of 14 points in 3D space that maximize the ratio \n    (min_dist / max_dist)^2.\n    \n    Method:\n    1. Initialize with Fibonacci sphere points.\n    2. Perform an initial local optimization (SLSQP).\n    3. Run an intensive stochastic search loop:\n       - Perturb the best known solution.\n       - Perturbation magnitude decays over time (Simulated Annealing schedule).\n       - Run short bursts of local optimization on perturbed points.\n       - Update best solution if improved.\n    4. Final high-precision polish of the best solution.\n    \"\"\"\n    N = 14\n    dim = 3\n    num_pairs = N * (N - 1) // 2\n    \n    # Precompute indices for all pairs to vectorize distance calculations\n    triu_indices = np.triu_indices(N, k=1)\n    idx_i = triu_indices[0]\n    idx_j = triu_indices[1]\n    \n    # Precompute slice objects for Jacobian assignment to speed up the loop\n    # This avoids creating slice objects 182 times per iteration\n    idx_slices = []\n    for k in range(num_pairs):\n        u, v = idx_i[k], idx_j[k]\n        idx_slices.append((slice(3*u, 3*u+3), slice(3*v, 3*v+3)))\n    \n    # Initialize points on a Fibonacci sphere\n    # This provides a near-optimal distribution on the surface\n    indices = np.arange(0, N, dtype=float) + 0.5\n    phi = np.arccos(1 - 2*indices/N)\n    theta = np.pi * (1 + 5**0.5) * indices\n    x_init = np.cos(theta) * np.sin(phi)\n    y_init = np.sin(theta) * np.sin(phi)\n    z_init = np.cos(phi)\n    points_init = np.column_stack((x_init, y_init, z_init))\n    \n    # Scale to approximate diameter 1.0 (Unit sphere has diameter 2.0)\n    points_init *= 0.5\n    \n    # Calculate initial squared min distance\n    diffs_init = points_init[idx_i] - points_init[idx_j]\n    d2_init = np.sum(diffs_init**2, axis=1)\n    s_init = np.min(d2_init)\n    \n    # State vector: [x1, y1, z1, ..., x14, y14, z14, s]\n    x0 = np.concatenate([points_init.flatten(), [s_init]])\n    \n    # Objective: Maximize s => Minimize -s\n    def objective(x):\n        return -x[-1]\n\n    # Gradient of objective\n    def obj_jac(x):\n        grad = np.zeros_like(x)\n        grad[-1] = -1.0\n        return grad\n    \n    # Constraint functions for SLSQP\n    # 1. dist^2_ij >= s  => dist^2_ij - s >= 0\n    # 2. dist^2_ij <= 1  => 1 - dist^2_ij >= 0\n    def constraints(x):\n        points = x[:-1].reshape((N, dim))\n        s = x[-1]\n        diffs = points[idx_i] - points[idx_j]\n        d2 = np.sum(diffs**2, axis=1)\n        return np.concatenate([d2 - s, 1.0 - d2])\n\n    # Jacobian of constraints\n    def constraints_jac(x):\n        points = x[:-1].reshape((N, dim))\n        diffs = points[idx_i] - points[idx_j]\n        two_diffs = 2 * diffs\n        \n        # Jacobian matrix shape: (2 * num_pairs, N * dim + 1)\n        jac = np.zeros((2 * num_pairs, N * dim + 1))\n        \n        # Efficiently fill the Jacobian\n        for k in range(num_pairs):\n            sl_u, sl_v = idx_slices[k]\n            td = two_diffs[k]\n            \n            # Constraint 1: d^2 - s >= 0\n            # d(d^2)/dp_u = 2(p_u - p_v)\n            jac[k, sl_u] = td\n            jac[k, sl_v] = -td\n            jac[k, -1] = -1.0\n            \n            # Constraint 2: 1 - d^2 >= 0\n            # d(1-d^2)/dp_u = -2(p_u - p_v)\n            jac[num_pairs + k, sl_u] = -td\n            jac[num_pairs + k, sl_v] = td\n            \n        return jac\n\n    # Define constraints dictionary\n    cons = {'type': 'ineq', 'fun': constraints, 'jac': constraints_jac}\n    \n    # Time management\n    start_time = time.time()\n    time_limit = 58.0 # seconds (leave buffer)\n    \n    # Initial local minimization\n    # Run with higher precision to establish a good baseline\n    res = minimize(objective, x0, jac=obj_jac, constraints=cons, method='SLSQP', \n                   options={'ftol': 1e-6, 'maxiter': 100})\n    best_x = res.x\n    best_fun = res.fun\n    \n    # Stochastic Search Loop\n    rng = np.random.default_rng()\n    \n    while time.time() - start_time < time_limit:\n        # Calculate progress for annealing\n        elapsed = time.time() - start_time\n        progress = elapsed / time_limit\n        \n        # Linear annealing of perturbation magnitude\n        # Start at 0.15, end at 0.005 to refine local optima\n        perturb_scale = 0.15 * (1.0 - progress) + 0.005\n        \n        # Perturb the best found solution so far (Greedy approach with restarts)\n        current_points = best_x[:-1].reshape((N, dim))\n        perturbation = rng.uniform(-perturb_scale, perturb_scale, size=current_points.shape)\n        new_points = current_points + perturbation\n        \n        # Normalize to max_dist=1 to keep optimizer in feasible region\n        diffs = new_points[idx_i] - new_points[idx_j]\n        max_d = np.sqrt(np.max(np.sum(diffs**2, axis=1)))\n        if max_d > 1e-9:\n            new_points /= max_d\n            \n        # Set s to a feasible value (slightly less than min dist)\n        diffs = new_points[idx_i] - new_points[idx_j]\n        s_new = np.min(np.sum(diffs**2, axis=1)) * 0.95\n        \n        x_trial = np.concatenate([new_points.flatten(), [s_new]])\n        \n        # Quick local optimization\n        # Fewer iterations and looser tolerance for speed\n        try:\n            res_trial = minimize(objective, x_trial, jac=obj_jac, constraints=cons, method='SLSQP',\n                                 options={'ftol': 1e-4, 'maxiter': 30})\n            \n            # Update if better\n            if res_trial.fun < best_fun - 1e-7:\n                best_fun = res_trial.fun\n                best_x = res_trial.x\n        except Exception:\n            continue\n\n    # Final Polish\n    # Run a high-precision optimization on the best result found\n    final_res = minimize(objective, best_x, jac=obj_jac, constraints=cons, method='SLSQP',\n                         options={'ftol': 1e-9, 'maxiter': 500})\n    \n    final_points = final_res.x[:-1].reshape((N, dim))\n    \n    # Final normalization to ensure strict max_dist=1.0 for the return value\n    diffs = final_points[idx_i] - final_points[idx_j]\n    max_d = np.sqrt(np.max(np.sum(diffs**2, axis=1)))\n    if max_d > 0:\n        final_points /= max_d\n        \n    return final_points",
  "metrics": {
    "valid": true,
    "score": 1.0000159135538387,
    "min_max_ratio": 0.24005088264956595,
    "min_distance": 0.48994987769114295,
    "max_distance": 1.0,
    "eval_time": 58.275530099868774,
    "error": null
  },
  "prompt": "Take the best known configuration from trial_1_1 and perform intensive local stochastic search.\n        import numpy as np\nfrom scipy.optimize import minimize, basinhopping\nimport time\n\ndef min_max_dist_dim3_14():\n    \"\"\"\n    Finds the coordinates of 14 points in 3D space that maximize the ratio \n    (min_dist / max_dist)^2 using Basin-hopping with SLSQP.\n    \"\"\"\n    N = 14\n    dim = 3\n    num_pairs = N * (N - 1) // 2\n    \n    # Precompute indices for all pairs to vectorize distance calculations\n    triu_indices = np.triu_indices(N, k=1)\n    idx_i = triu_indices[0]\n    idx_j = triu_indices[1]\n    \n    # Initialize points on a Fibonacci sphere for a near-optimal starting configuration\n    # This reduces the time needed for convergence compared to random initialization\n    indices = np.arange(0, N, dtype=float) + 0.5\n    phi = np.arccos(1 - 2*indices/N)\n    theta = np.pi * (1 + 5**0.5) * indices\n    x_init = np.cos(theta) * np.sin(phi)\n    y_init = np.sin(theta) * np.sin(phi)\n    z_init = np.cos(phi)\n    points_init = np.column_stack((x_init, y_init, z_init))\n    \n    # Scale to approximate diameter 1.0 (Unit sphere has diameter 2.0)\n    points_init *= 0.5\n    \n    # Calculate initial squared min distance\n    diffs_init = points_init[idx_i] - points_init[idx_j]\n    d2_init = np.sum(diffs_init**2, axis=1)\n    s_init = np.min(d2_init)\n    \n    # State vector: [x1, y1, z1, ..., x14, y14, z14, s]\n    x0 = np.concatenate([points_init.flatten(), [s_init]])\n    \n    # Objective: Maximize s => Minimize -s\n    def objective(x):\n        return -x[-1]\n\n    # Gradient of objective\n    def obj_jac(x):\n        grad = np.zeros_like(x)\n        grad[-1] = -1.0\n        return grad\n    \n    # Constraint functions for SLSQP\n    # 1. dist^2_ij >= s  => dist^2_ij - s >= 0\n    # 2. dist^2_ij <= 1  => 1 - dist^2_ij >= 0\n    def constraints(x):\n        points = x[:-1].reshape((N, dim))\n        s = x[-1]\n        diffs = points[idx_i] - points[idx_j]\n        d2 = np.sum(diffs**2, axis=1)\n        return np.concatenate([d2 - s, 1.0 - d2])\n\n    # Jacobian of constraints\n    def constraints_jac(x):\n        points = x[:-1].reshape((N, dim))\n        diffs = points[idx_i] - points[idx_j] # Shape (num_pairs, 3)\n        two_diffs = 2 * diffs\n        \n        # Jacobian matrix shape: (2 * num_pairs, N * dim + 1)\n        jac = np.zeros((2 * num_pairs, N * dim + 1))\n        \n        # We fill the matrix efficiently\n        # Constraint 1: d2 - s >= 0\n        # Derivatives w.r.t p_i are 2*(p_i - p_j), w.r.t p_j are -2*(p_i - p_j)\n        # Derivative w.r.t s is -1\n        \n        # Constraint 2: 1 - d2 >= 0\n        # Derivatives are negative of above (s term is 0)\n        \n        for k in range(num_pairs):\n            i = idx_i[k]\n            j = idx_j[k]\n            \n            # Indices in the flattened state vector\n            sl_i = slice(i*3, i*3+3)\n            sl_j = slice(j*3, j*3+3)\n            \n            # Row k (d2 - s)\n            jac[k, sl_i] = two_diffs[k]\n            jac[k, sl_j] = -two_diffs[k]\n            jac[k, -1] = -1.0\n            \n            # Row num_pairs + k (1 - d2)\n            jac[num_pairs + k, sl_i] = -two_diffs[k]\n            jac[num_pairs + k, sl_j] = two_diffs[k]\n            \n        return jac\n\n    # Define constraints dictionary\n    cons = {'type': 'ineq', 'fun': constraints, 'jac': constraints_jac}\n    \n    # Custom step taker for Basin-hopping\n    # Perturbs points and normalizes them to help the optimizer\n    class NormalizedStep:\n        def __init__(self, stepsize=0.1):\n            self.stepsize = stepsize\n            self.rng = np.random.default_rng()\n            \n        def __call__(self, x):\n            coords = x[:-1].reshape((N, dim))\n            perturbation = self.rng.uniform(-self.stepsize, self.stepsize, size=coords.shape)\n            new_coords = coords + perturbation\n            \n            # Normalize configuration to have max dist = 1.0\n            # This satisfies the prompt's instruction and keeps the search feasible\n            diffs = new_coords[idx_i] - new_coords[idx_j]\n            dists = np.sqrt(np.sum(diffs**2, axis=1))\n            max_d = np.max(dists)\n            if max_d > 1e-9:\n                new_coords /= max_d\n            \n            # Reset s to a feasible value (slightly less than min_dist^2)\n            new_diffs = new_coords[idx_i] - new_coords[idx_j]\n            new_s = np.min(np.sum(new_diffs**2, axis=1)) * 0.95\n            \n            return np.concatenate([new_coords.flatten(), [new_s]])\n\n    take_step = NormalizedStep(stepsize=0.15)\n    \n    # Minimizer options\n    minimizer_kwargs = {\n        'method': 'SLSQP',\n        'jac': obj_jac,\n        'constraints': cons,\n        'options': {'ftol': 1e-6, 'maxiter': 200}\n    }\n    \n    # Time management\n    start_time = time.time()\n    time_limit = 55 # seconds\n    \n    # Initial local minimization\n    res = minimize(objective, x0, jac=obj_jac, constraints=cons, method='SLSQP')\n    best_x = res.x\n    best_fun = res.fun\n    \n    # Basin-hopping loop\n    # We run short bursts of basin-hopping and check time\n    while time.time() - start_time < time_limit:\n        # Restart from current best to perform iterated local search\n        bh_res = basinhopping(objective, best_x, niter=5, \n                              minimizer_kwargs=minimizer_kwargs, \n                              take_step=take_step)\n        \n        # Check if we found a better solution\n        if bh_res.lowest_optimization_result.fun < best_fun:\n            best_fun = bh_res.lowest_optimization_result.fun\n            best_x = bh_res.lowest_optimization_result.x\n\n    # Extract final points\n    final_points = best_x[:-1].reshape((N, dim))\n    \n    # Final normalization to ensure max_dist is exactly 1.0\n    diffs = final_points[idx_i] - final_points[idx_j]\n    max_d = np.sqrt(np.max(np.sum(diffs**2, axis=1)))\n    if max_d > 0:\n        final_points /= max_d\n        \n    return final_points\n        \n        Modification:\n        1. Start with the points from trial_1_1.\n        2. Repeatedly apply small random perturbations.\n        3. After each perturbation, run a quick local optimization (e.g., SLSQP for 10-20 iterations).\n        4. If the score improves, keep the new configuration.\n        5. Use a simulated annealing schedule for the perturbation magnitude.",
  "response": "Here is the Python solution implementing the intensive local stochastic search with simulated annealing as requested.\n\n```python\nimport numpy as np\nfrom scipy.optimize import minimize\nimport time\n\ndef min_max_dist_dim3_14():\n    \"\"\"\n    Finds the coordinates of 14 points in 3D space that maximize the ratio \n    (min_dist / max_dist)^2.\n    \n    Method:\n    1. Initialize with Fibonacci sphere points.\n    2. Perform an initial local optimization (SLSQP).\n    3. Run an intensive stochastic search loop:\n       - Perturb the best known solution.\n       - Perturbation magnitude decays over time (Simulated Annealing schedule).\n       - Run short bursts of local optimization on perturbed points.\n       - Update best solution if improved.\n    4. Final high-precision polish of the best solution.\n    \"\"\"\n    N = 14\n    dim = 3\n    num_pairs = N * (N - 1) // 2\n    \n    # Precompute indices for all pairs to vectorize distance calculations\n    triu_indices = np.triu_indices(N, k=1)\n    idx_i = triu_indices[0]\n    idx_j = triu_indices[1]\n    \n    # Precompute slice objects for Jacobian assignment to speed up the loop\n    # This avoids creating slice objects 182 times per iteration\n    idx_slices = []\n    for k in range(num_pairs):\n        u, v = idx_i[k], idx_j[k]\n        idx_slices.append((slice(3*u, 3*u+3), slice(3*v, 3*v+3)))\n    \n    # Initialize points on a Fibonacci sphere\n    # This provides a near-optimal distribution on the surface\n    indices = np.arange(0, N, dtype=float) + 0.5\n    phi = np.arccos(1 - 2*indices/N)\n    theta = np.pi * (1 + 5**0.5) * indices\n    x_init = np.cos(theta) * np.sin(phi)\n    y_init = np.sin(theta) * np.sin(phi)\n    z_init = np.cos(phi)\n    points_init = np.column_stack((x_init, y_init, z_init))\n    \n    # Scale to approximate diameter 1.0 (Unit sphere has diameter 2.0)\n    points_init *= 0.5\n    \n    # Calculate initial squared min distance\n    diffs_init = points_init[idx_i] - points_init[idx_j]\n    d2_init = np.sum(diffs_init**2, axis=1)\n    s_init = np.min(d2_init)\n    \n    # State vector: [x1, y1, z1, ..., x14, y14, z14, s]\n    x0 = np.concatenate([points_init.flatten(), [s_init]])\n    \n    # Objective: Maximize s => Minimize -s\n    def objective(x):\n        return -x[-1]\n\n    # Gradient of objective\n    def obj_jac(x):\n        grad = np.zeros_like(x)\n        grad[-1] = -1.0\n        return grad\n    \n    # Constraint functions for SLSQP\n    # 1. dist^2_ij >= s  => dist^2_ij - s >= 0\n    # 2. dist^2_ij <= 1  => 1 - dist^2_ij >= 0\n    def constraints(x):\n        points = x[:-1].reshape((N, dim))\n        s = x[-1]\n        diffs = points[idx_i] - points[idx_j]\n        d2 = np.sum(diffs**2, axis=1)\n        return np.concatenate([d2 - s, 1.0 - d2])\n\n    # Jacobian of constraints\n    def constraints_jac(x):\n        points = x[:-1].reshape((N, dim))\n        diffs = points[idx_i] - points[idx_j]\n        two_diffs = 2 * diffs\n        \n        # Jacobian matrix shape: (2 * num_pairs, N * dim + 1)\n        jac = np.zeros((2 * num_pairs, N * dim + 1))\n        \n        # Efficiently fill the Jacobian\n        for k in range(num_pairs):\n            sl_u, sl_v = idx_slices[k]\n            td = two_diffs[k]\n            \n            # Constraint 1: d^2 - s >= 0\n            # d(d^2)/dp_u = 2(p_u - p_v)\n            jac[k, sl_u] = td\n            jac[k, sl_v] = -td\n            jac[k, -1] = -1.0\n            \n            # Constraint 2: 1 - d^2 >= 0\n            # d(1-d^2)/dp_u = -2(p_u - p_v)\n            jac[num_pairs + k, sl_u] = -td\n            jac[num_pairs + k, sl_v] = td\n            \n        return jac\n\n    # Define constraints dictionary\n    cons = {'type': 'ineq', 'fun': constraints, 'jac': constraints_jac}\n    \n    # Time management\n    start_time = time.time()\n    time_limit = 58.0 # seconds (leave buffer)\n    \n    # Initial local minimization\n    # Run with higher precision to establish a good baseline\n    res = minimize(objective, x0, jac=obj_jac, constraints=cons, method='SLSQP', \n                   options={'ftol': 1e-6, 'maxiter': 100})\n    best_x = res.x\n    best_fun = res.fun\n    \n    # Stochastic Search Loop\n    rng = np.random.default_rng()\n    \n    while time.time() - start_time < time_limit:\n        # Calculate progress for annealing\n        elapsed = time.time() - start_time\n        progress = elapsed / time_limit\n        \n        # Linear annealing of perturbation magnitude\n        # Start at 0.15, end at 0.005 to refine local optima\n        perturb_scale = 0.15 * (1.0 - progress) + 0.005\n        \n        # Perturb the best found solution so far (Greedy approach with restarts)\n        current_points = best_x[:-1].reshape((N, dim))\n        perturbation = rng.uniform(-perturb_scale, perturb_scale, size=current_points.shape)\n        new_points = current_points + perturbation\n        \n        # Normalize to max_dist=1 to keep optimizer in feasible region\n        diffs = new_points[idx_i] - new_points[idx_j]\n        max_d = np.sqrt(np.max(np.sum(diffs**2, axis=1)))\n        if max_d > 1e-9:\n            new_points /= max_d\n            \n        # Set s to a feasible value (slightly less than min dist)\n        diffs = new_points[idx_i] - new_points[idx_j]\n        s_new = np.min(np.sum(diffs**2, axis=1)) * 0.95\n        \n        x_trial = np.concatenate([new_points.flatten(), [s_new]])\n        \n        # Quick local optimization\n        # Fewer iterations and looser tolerance for speed\n        try:\n            res_trial = minimize(objective, x_trial, jac=obj_jac, constraints=cons, method='SLSQP',\n                                 options={'ftol': 1e-4, 'maxiter': 30})\n            \n            # Update if better\n            if res_trial.fun < best_fun - 1e-7:\n                best_fun = res_trial.fun\n                best_x = res_trial.x\n        except Exception:\n            continue\n\n    # Final Polish\n    # Run a high-precision optimization on the best result found\n    final_res = minimize(objective, best_x, jac=obj_jac, constraints=cons, method='SLSQP',\n                         options={'ftol': 1e-9, 'maxiter': 500})\n    \n    final_points = final_res.x[:-1].reshape((N, dim))\n    \n    # Final normalization to ensure strict max_dist=1.0 for the return value\n    diffs = final_points[idx_i] - final_points[idx_j]\n    max_d = np.sqrt(np.max(np.sum(diffs**2, axis=1)))\n    if max_d > 0:\n        final_points /= max_d\n        \n    return final_points\n```",
  "reasoning": "Here is the Python solution implementing the intensive local stochastic search with simulated annealing as requested.",
  "timestamp": "2026-01-21T17:26:21.463817",
  "cost_data": null,
  "model_config": {
    "model": "gemini-3-pro-preview",
    "temperature": 0.7,
    "provider_debug": {
      "finish_reason": "STOP",
      "usage_metadata": {
        "prompt_token_count": 2241,
        "candidates_token_count": 2084,
        "thoughts_token_count": 8731
      },
      "response_text_len": 6372
    }
  }
}