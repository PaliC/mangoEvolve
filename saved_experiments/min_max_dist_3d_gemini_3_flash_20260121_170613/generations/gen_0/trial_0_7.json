{
  "trial_id": "trial_0_7",
  "generation": 0,
  "parent_id": null,
  "code": "import numpy as np\nfrom scipy.optimize import minimize\nimport time\n\ndef min_max_dist_dim3_14():\n    \"\"\"\n    Finds 14 points in 3D space that maximize (min_dist / max_dist)^2.\n    \n    Strategy:\n    The optimal configuration for maximizing the minimum distance ratio for N points\n    generally lies on the surface of a sphere (spherical code).\n    We use a 'SoftMax' style approach by minimizing the repulsive potential energy:\n        E = sum_{i<j} exp(-k * distance_ij)\n    Minimizing this sum is equivalent to maximizing the SoftMin of the distances,\n    approximating the maximization of the minimum distance.\n    We use a continuation method (annealing) for the stiffness parameter k.\n    \"\"\"\n    N = 14\n    dim = 3\n    \n    # Precompute indices for upper triangle of distance matrix (i < j)\n    iu = np.triu_indices(N, 1)\n\n    def get_dists_sq(x):\n        # x is (N, 3)\n        # returns (N, N) squared distances matrix\n        # Uses broadcasting: (N, 1, 3) - (1, N, 3) -> (N, N, 3)\n        diff = x[:, np.newaxis, :] - x[np.newaxis, :, :]\n        return np.sum(diff**2, axis=2)\n\n    def objective(x_flat, k):\n        # Reshape to (N, 3)\n        x = x_flat.reshape(N, dim)\n        sq_dists = get_dists_sq(x)\n        # Safe sqrt\n        dists = np.sqrt(np.maximum(sq_dists, 1e-12))\n        # Calculate potential: sum of exp(-k * d) for all pairs\n        return np.sum(np.exp(-k * dists[iu]))\n\n    def objective_jac(x_flat, k):\n        # Analytical gradient of the objective function\n        x = x_flat.reshape(N, dim)\n        sq_dists = get_dists_sq(x)\n        dists = np.sqrt(np.maximum(sq_dists, 1e-12))\n        \n        # Derivative of exp(-k*d) is -k * exp(-k*d)\n        # Chain rule with distance: d(d_ij)/dx_i = (x_i - x_j) / d_ij\n        \n        # Compute weighting factors W_ij = -k * exp(-k * d_ij) / d_ij\n        with np.errstate(divide='ignore', invalid='ignore'):\n            factors = -k * np.exp(-k * dists) / dists\n        np.fill_diagonal(factors, 0)\n        \n        diff = x[:, np.newaxis, :] - x[np.newaxis, :, :]\n        \n        # Gradient for point i is sum_j W_ij * (x_i - x_j)\n        # Einsum: 'ij' is factors (N,N), 'ijk' is diff (N,N,3) -> 'ik' (N,3)\n        grad = np.einsum('ij, ijk -> ik', factors, diff)\n        return grad.flatten()\n\n    def constraint(x_flat):\n        # Constraint: points must lie on unit sphere\n        x = x_flat.reshape(N, dim)\n        return np.sum(x**2, axis=1) - 1.0\n\n    def constraint_jac(x_flat):\n        # Jacobian of the spherical constraint\n        x = x_flat.reshape(N, dim)\n        jac = np.zeros((N, N * dim))\n        rows = np.arange(N)\n        # For constraint i (row i), derivatives are 2*x_i, 2*y_i, 2*z_i\n        for d in range(dim):\n            jac[rows, rows*dim + d] = 2 * x[:, d]\n        return jac\n\n    best_score = -1.0\n    # Initialize with random points\n    best_points = np.random.randn(N, dim)\n    \n    start_time = time.time()\n    # Run optimization with restarts until time is nearly up (60s limit)\n    while time.time() - start_time < 57.0:\n        # Random initialization on unit sphere\n        x0 = np.random.randn(N, dim)\n        x0 /= np.linalg.norm(x0, axis=1, keepdims=True)\n        x_curr = x0.flatten()\n        \n        # Annealing schedule: increase k to stiffen the repulsion\n        # Small k finds global structure, large k refines minimum distance\n        ks = [10, 40, 100, 200]\n        \n        for k in ks:\n            # Check time to ensure safe exit\n            if time.time() - start_time > 59.0: break\n            \n            try:\n                res = minimize(\n                    objective, x_curr, args=(k,), \n                    method='SLSQP', jac=objective_jac,\n                    constraints={'type': 'eq', 'fun': constraint, 'jac': constraint_jac},\n                    options={'maxiter': 200, 'ftol': 1e-5}\n                )\n                x_curr = res.x\n            except Exception:\n                break\n        \n        pts = x_curr.reshape(N, dim)\n        \n        # Evaluate the result\n        diff = pts[:, np.newaxis, :] - pts[np.newaxis, :, :]\n        dists = np.sqrt(np.sum(diff**2, axis=2))\n        p_dists = dists[iu]\n        \n        if len(p_dists) > 0:\n            d_min = np.min(p_dists)\n            d_max = np.max(p_dists)\n            \n            if d_max > 1e-9:\n                score = (d_min / d_max)**2\n            else:\n                score = 0.0\n                \n            if score > best_score:\n                best_score = score\n                best_points = pts.copy()\n\n    return best_points",
  "metrics": {
    "valid": true,
    "score": 0.8539334837905175,
    "min_max_ratio": 0.2049842244804402,
    "min_distance": 0.9053615680982123,
    "max_distance": 1.999686135449751,
    "eval_time": 57.2719292640686,
    "error": null
  },
  "prompt": "Maximize (min_dist / max_dist)^2 for 14 points in 3D. Use a 'SoftMax' style approach: instead of maximizing the minimum distance, maximize the sum of exp(-k * distance_ij) for a large k, which is a smooth approximation of the objective.",
  "response": "```python\nimport numpy as np\nfrom scipy.optimize import minimize\nimport time\n\ndef min_max_dist_dim3_14():\n    \"\"\"\n    Finds 14 points in 3D space that maximize (min_dist / max_dist)^2.\n    \n    Strategy:\n    The optimal configuration for maximizing the minimum distance ratio for N points\n    generally lies on the surface of a sphere (spherical code).\n    We use a 'SoftMax' style approach by minimizing the repulsive potential energy:\n        E = sum_{i<j} exp(-k * distance_ij)\n    Minimizing this sum is equivalent to maximizing the SoftMin of the distances,\n    approximating the maximization of the minimum distance.\n    We use a continuation method (annealing) for the stiffness parameter k.\n    \"\"\"\n    N = 14\n    dim = 3\n    \n    # Precompute indices for upper triangle of distance matrix (i < j)\n    iu = np.triu_indices(N, 1)\n\n    def get_dists_sq(x):\n        # x is (N, 3)\n        # returns (N, N) squared distances matrix\n        # Uses broadcasting: (N, 1, 3) - (1, N, 3) -> (N, N, 3)\n        diff = x[:, np.newaxis, :] - x[np.newaxis, :, :]\n        return np.sum(diff**2, axis=2)\n\n    def objective(x_flat, k):\n        # Reshape to (N, 3)\n        x = x_flat.reshape(N, dim)\n        sq_dists = get_dists_sq(x)\n        # Safe sqrt\n        dists = np.sqrt(np.maximum(sq_dists, 1e-12))\n        # Calculate potential: sum of exp(-k * d) for all pairs\n        return np.sum(np.exp(-k * dists[iu]))\n\n    def objective_jac(x_flat, k):\n        # Analytical gradient of the objective function\n        x = x_flat.reshape(N, dim)\n        sq_dists = get_dists_sq(x)\n        dists = np.sqrt(np.maximum(sq_dists, 1e-12))\n        \n        # Derivative of exp(-k*d) is -k * exp(-k*d)\n        # Chain rule with distance: d(d_ij)/dx_i = (x_i - x_j) / d_ij\n        \n        # Compute weighting factors W_ij = -k * exp(-k * d_ij) / d_ij\n        with np.errstate(divide='ignore', invalid='ignore'):\n            factors = -k * np.exp(-k * dists) / dists\n        np.fill_diagonal(factors, 0)\n        \n        diff = x[:, np.newaxis, :] - x[np.newaxis, :, :]\n        \n        # Gradient for point i is sum_j W_ij * (x_i - x_j)\n        # Einsum: 'ij' is factors (N,N), 'ijk' is diff (N,N,3) -> 'ik' (N,3)\n        grad = np.einsum('ij, ijk -> ik', factors, diff)\n        return grad.flatten()\n\n    def constraint(x_flat):\n        # Constraint: points must lie on unit sphere\n        x = x_flat.reshape(N, dim)\n        return np.sum(x**2, axis=1) - 1.0\n\n    def constraint_jac(x_flat):\n        # Jacobian of the spherical constraint\n        x = x_flat.reshape(N, dim)\n        jac = np.zeros((N, N * dim))\n        rows = np.arange(N)\n        # For constraint i (row i), derivatives are 2*x_i, 2*y_i, 2*z_i\n        for d in range(dim):\n            jac[rows, rows*dim + d] = 2 * x[:, d]\n        return jac\n\n    best_score = -1.0\n    # Initialize with random points\n    best_points = np.random.randn(N, dim)\n    \n    start_time = time.time()\n    # Run optimization with restarts until time is nearly up (60s limit)\n    while time.time() - start_time < 57.0:\n        # Random initialization on unit sphere\n        x0 = np.random.randn(N, dim)\n        x0 /= np.linalg.norm(x0, axis=1, keepdims=True)\n        x_curr = x0.flatten()\n        \n        # Annealing schedule: increase k to stiffen the repulsion\n        # Small k finds global structure, large k refines minimum distance\n        ks = [10, 40, 100, 200]\n        \n        for k in ks:\n            # Check time to ensure safe exit\n            if time.time() - start_time > 59.0: break\n            \n            try:\n                res = minimize(\n                    objective, x_curr, args=(k,), \n                    method='SLSQP', jac=objective_jac,\n                    constraints={'type': 'eq', 'fun': constraint, 'jac': constraint_jac},\n                    options={'maxiter': 200, 'ftol': 1e-5}\n                )\n                x_curr = res.x\n            except Exception:\n                break\n        \n        pts = x_curr.reshape(N, dim)\n        \n        # Evaluate the result\n        diff = pts[:, np.newaxis, :] - pts[np.newaxis, :, :]\n        dists = np.sqrt(np.sum(diff**2, axis=2))\n        p_dists = dists[iu]\n        \n        if len(p_dists) > 0:\n            d_min = np.min(p_dists)\n            d_max = np.max(p_dists)\n            \n            if d_max > 1e-9:\n                score = (d_min / d_max)**2\n            else:\n                score = 0.0\n                \n            if score > best_score:\n                best_score = score\n                best_points = pts.copy()\n\n    return best_points\n```",
  "reasoning": "",
  "timestamp": "2026-01-21T17:10:47.727815",
  "cost_data": null,
  "model_config": {
    "model": "gemini-3-pro-preview",
    "temperature": 0.7,
    "provider_debug": {
      "finish_reason": "STOP",
      "usage_metadata": {
        "prompt_token_count": 235,
        "candidates_token_count": 1481,
        "thoughts_token_count": 13113
      },
      "response_text_len": 4559
    }
  }
}