{
  "trial_id": "trial_0_3",
  "generation": 0,
  "parent_id": null,
  "code": "import numpy as np\nfrom scipy.optimize import minimize\nimport time\n\ndef min_max_dist_dim3_14():\n    \"\"\"\n    Optimizes the placement of 14 points in 3D space to maximize the ratio:\n    (min pairwise distance / max pairwise distance)^2.\n    \n    The strategy uses multi-start L-BFGS-B optimization with a smooth \n    approximation of the min/max ratio objective.\n    \"\"\"\n    N = 14\n    D = 3\n    iu = np.triu_indices(N, k=1)\n    \n    def get_objective_and_grad(coords, k=100.0):\n        \"\"\"\n        Computes the soft-min/soft-max ratio objective and its gradient.\n        The objective is f = -soft_min / soft_max, where soft_min and \n        soft_max are smooth approximations of the actual min and max \n        pairwise squared distances.\n        \"\"\"\n        X = coords.reshape((N, D))\n        # Efficiently compute all pairwise squared distances\n        diffs = X[:, np.newaxis, :] - X[np.newaxis, :, :]\n        dists_sq_mat = np.sum(diffs**2, axis=-1)\n        d_sq = dists_sq_mat[iu]\n        \n        min_d_sq = np.min(d_sq)\n        max_d_sq = np.max(d_sq)\n        \n        # Use the LogSumExp trick for numerical stability\n        exp_neg_k = np.exp(-k * (d_sq - min_d_sq))\n        sum_exp_neg_k = np.sum(exp_neg_k)\n        soft_min = min_d_sq - np.log(sum_exp_neg_k) / k\n        \n        exp_pos_k = np.exp(k * (d_sq - max_d_sq))\n        sum_exp_pos_k = np.sum(exp_pos_k)\n        soft_max = max_d_sq + np.log(sum_exp_pos_k) / k\n        \n        # Ratio objective: maximize soft_min / soft_max => minimize -soft_min / soft_max\n        f = -soft_min / soft_max\n        \n        # Add a small penalty to fix the scale and prevent collapse\n        penalty_weight = 0.1\n        penalty = penalty_weight * (soft_max - 1.0)**2\n        f += penalty\n        \n        # Gradient with respect to squared distances\n        grad_s_min_dsq = exp_neg_k / sum_exp_neg_k\n        grad_s_max_dsq = exp_pos_k / sum_exp_pos_k\n        \n        # d(f)/d(d_sq_ij) = -1/s_max * d(s_min) + s_min/s_max^2 * d(s_max)\n        grad_f_dsq = -grad_s_min_dsq / soft_max + (soft_min / (soft_max**2)) * grad_s_max_dsq\n        # Add gradient contribution from the penalty\n        grad_f_dsq += penalty_weight * 2 * (soft_max - 1.0) * grad_s_max_dsq\n        \n        # Assemble gradient matrix (N, N)\n        grad_matrix = np.zeros((N, N))\n        grad_matrix[iu] = grad_f_dsq\n        grad_matrix = grad_matrix + grad_matrix.T\n        \n        # Map gradient from distances to coordinates X\n        # dV/dx_i = sum_j 2 * grad_matrix[i,j] * (x_i - x_j)\n        grad_f_X = 2 * (np.sum(grad_matrix, axis=1)[:, np.newaxis] * X - grad_matrix @ X)\n        \n        return f, grad_f_X.flatten()\n\n    best_ratio_sq = -1.0\n    best_X = None\n    \n    start_time = time.time()\n    \n    # Predefined start: Fibonacci sphere (distributes points evenly on a sphere)\n    phi = np.pi * (3. - np.sqrt(5.)) # Golden angle\n    y = 1 - (np.arange(N) / float(N - 1)) * 2\n    radius = np.sqrt(1 - y * y)\n    theta = phi * np.arange(N)\n    X_fib = np.zeros((N, 3))\n    X_fib[:, 0] = np.cos(theta) * radius\n    X_fib[:, 1] = y\n    X_fib[:, 2] = np.sin(theta) * radius\n\n    # Run multi-start optimization\n    for i in range(100):\n        # Respect time limit\n        if time.time() - start_time > 56:\n            break\n            \n        # Initialization strategy\n        if i == 0:\n            X0 = X_fib\n        elif i % 2 == 1:\n            X0 = np.random.uniform(-1.0, 1.0, (N, D))\n        else:\n            X0 = np.random.normal(0.0, 1.0, (N, D))\n            \n        # L-BFGS-B optimization\n        res = minimize(get_objective_and_grad, X0.flatten(), jac=True, method='L-BFGS-B', \n                       options={'maxiter': 100, 'ftol': 1e-7})\n        \n        # Evaluate current solution\n        X_final = res.x.reshape((N, D))\n        final_diffs = X_final[:, np.newaxis, :] - X_final[np.newaxis, :, :]\n        final_dists_sq = np.sum(final_diffs**2, axis=-1)[iu]\n        curr_ratio_sq = np.min(final_dists_sq) / np.max(final_dists_sq)\n        \n        # Update best solution found so far\n        if curr_ratio_sq > best_ratio_sq:\n            best_ratio_sq = curr_ratio_sq\n            best_X = X_final\n            \n    return best_X",
  "metrics": {
    "valid": true,
    "score": 0.9796708394773059,
    "min_max_ratio": 0.2351671073781442,
    "min_distance": 0.5145168859199627,
    "max_distance": 1.0609901299235605,
    "eval_time": 0.7261571884155273,
    "error": null
  },
  "prompt": "Optimize 14 points in 3D for the max-min distance ratio. Use a multi-start strategy: generate 100 random initial configurations and for each, run a short L-BFGS-B optimization. Use the best result found.",
  "response": "```python\nimport numpy as np\nfrom scipy.optimize import minimize\nimport time\n\ndef min_max_dist_dim3_14():\n    \"\"\"\n    Optimizes the placement of 14 points in 3D space to maximize the ratio:\n    (min pairwise distance / max pairwise distance)^2.\n    \n    The strategy uses multi-start L-BFGS-B optimization with a smooth \n    approximation of the min/max ratio objective.\n    \"\"\"\n    N = 14\n    D = 3\n    iu = np.triu_indices(N, k=1)\n    \n    def get_objective_and_grad(coords, k=100.0):\n        \"\"\"\n        Computes the soft-min/soft-max ratio objective and its gradient.\n        The objective is f = -soft_min / soft_max, where soft_min and \n        soft_max are smooth approximations of the actual min and max \n        pairwise squared distances.\n        \"\"\"\n        X = coords.reshape((N, D))\n        # Efficiently compute all pairwise squared distances\n        diffs = X[:, np.newaxis, :] - X[np.newaxis, :, :]\n        dists_sq_mat = np.sum(diffs**2, axis=-1)\n        d_sq = dists_sq_mat[iu]\n        \n        min_d_sq = np.min(d_sq)\n        max_d_sq = np.max(d_sq)\n        \n        # Use the LogSumExp trick for numerical stability\n        exp_neg_k = np.exp(-k * (d_sq - min_d_sq))\n        sum_exp_neg_k = np.sum(exp_neg_k)\n        soft_min = min_d_sq - np.log(sum_exp_neg_k) / k\n        \n        exp_pos_k = np.exp(k * (d_sq - max_d_sq))\n        sum_exp_pos_k = np.sum(exp_pos_k)\n        soft_max = max_d_sq + np.log(sum_exp_pos_k) / k\n        \n        # Ratio objective: maximize soft_min / soft_max => minimize -soft_min / soft_max\n        f = -soft_min / soft_max\n        \n        # Add a small penalty to fix the scale and prevent collapse\n        penalty_weight = 0.1\n        penalty = penalty_weight * (soft_max - 1.0)**2\n        f += penalty\n        \n        # Gradient with respect to squared distances\n        grad_s_min_dsq = exp_neg_k / sum_exp_neg_k\n        grad_s_max_dsq = exp_pos_k / sum_exp_pos_k\n        \n        # d(f)/d(d_sq_ij) = -1/s_max * d(s_min) + s_min/s_max^2 * d(s_max)\n        grad_f_dsq = -grad_s_min_dsq / soft_max + (soft_min / (soft_max**2)) * grad_s_max_dsq\n        # Add gradient contribution from the penalty\n        grad_f_dsq += penalty_weight * 2 * (soft_max - 1.0) * grad_s_max_dsq\n        \n        # Assemble gradient matrix (N, N)\n        grad_matrix = np.zeros((N, N))\n        grad_matrix[iu] = grad_f_dsq\n        grad_matrix = grad_matrix + grad_matrix.T\n        \n        # Map gradient from distances to coordinates X\n        # dV/dx_i = sum_j 2 * grad_matrix[i,j] * (x_i - x_j)\n        grad_f_X = 2 * (np.sum(grad_matrix, axis=1)[:, np.newaxis] * X - grad_matrix @ X)\n        \n        return f, grad_f_X.flatten()\n\n    best_ratio_sq = -1.0\n    best_X = None\n    \n    start_time = time.time()\n    \n    # Predefined start: Fibonacci sphere (distributes points evenly on a sphere)\n    phi = np.pi * (3. - np.sqrt(5.)) # Golden angle\n    y = 1 - (np.arange(N) / float(N - 1)) * 2\n    radius = np.sqrt(1 - y * y)\n    theta = phi * np.arange(N)\n    X_fib = np.zeros((N, 3))\n    X_fib[:, 0] = np.cos(theta) * radius\n    X_fib[:, 1] = y\n    X_fib[:, 2] = np.sin(theta) * radius\n\n    # Run multi-start optimization\n    for i in range(100):\n        # Respect time limit\n        if time.time() - start_time > 56:\n            break\n            \n        # Initialization strategy\n        if i == 0:\n            X0 = X_fib\n        elif i % 2 == 1:\n            X0 = np.random.uniform(-1.0, 1.0, (N, D))\n        else:\n            X0 = np.random.normal(0.0, 1.0, (N, D))\n            \n        # L-BFGS-B optimization\n        res = minimize(get_objective_and_grad, X0.flatten(), jac=True, method='L-BFGS-B', \n                       options={'maxiter': 100, 'ftol': 1e-7})\n        \n        # Evaluate current solution\n        X_final = res.x.reshape((N, D))\n        final_diffs = X_final[:, np.newaxis, :] - X_final[np.newaxis, :, :]\n        final_dists_sq = np.sum(final_diffs**2, axis=-1)[iu]\n        curr_ratio_sq = np.min(final_dists_sq) / np.max(final_dists_sq)\n        \n        # Update best solution found so far\n        if curr_ratio_sq > best_ratio_sq:\n            best_ratio_sq = curr_ratio_sq\n            best_X = X_final\n            \n    return best_X\n\n```",
  "reasoning": "",
  "timestamp": "2026-01-21T17:09:28.691004",
  "cost_data": null,
  "model_config": {
    "model": "gemini-3-flash-preview",
    "temperature": 0.7,
    "provider_debug": {
      "finish_reason": "STOP",
      "usage_metadata": {
        "prompt_token_count": 225,
        "candidates_token_count": 1488,
        "thoughts_token_count": 20554
      },
      "response_text_len": 4199
    }
  }
}