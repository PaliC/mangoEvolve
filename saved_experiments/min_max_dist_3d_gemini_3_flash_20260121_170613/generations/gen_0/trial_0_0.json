{
  "trial_id": "trial_0_0",
  "generation": 0,
  "parent_id": null,
  "code": "import numpy as np\nfrom scipy.optimize import differential_evolution, minimize\nfrom scipy.spatial.distance import pdist\nimport time\nimport warnings\n\ndef min_max_dist_dim3_14():\n    \"\"\"\n    Optimizes the placement of 14 points in 3D to maximize the squared ratio \n    of the minimum pairwise distance to the maximum pairwise distance.\n    \n    Strategy:\n    1. Global Search: Differential Evolution (DE) to find a good approximate configuration.\n    2. Local Refinement: SLSQP using a Log-Sum-Exp (SoftMin/SoftMax) formulation \n       to smooth the non-differentiable min/max functions and improve precision.\n    \"\"\"\n    \n    # Problem constants\n    N = 14\n    D = 3\n    TIME_LIMIT = 55.0  # Seconds (leaving buffer for overhead)\n    start_time = time.time()\n    \n    # Suppress warnings for clean execution\n    warnings.filterwarnings(\"ignore\")\n\n    # --- Helper Functions ---\n\n    def get_dists(x):\n        \"\"\"Converts flat parameter vector to pairwise distances.\"\"\"\n        pts = x.reshape((N, D))\n        return pdist(pts)\n\n    # --- Phase 1: Global Search (Differential Evolution) ---\n\n    def objective_de(x):\n        \"\"\"\n        Hard objective for DE.\n        Maximize (min_dist / max_dist)^2 => Minimize -(min_dist / max_dist)^2\n        \"\"\"\n        dists = get_dists(x)\n        if dists.size == 0: return 0.0\n        \n        d_min = np.min(dists)\n        d_max = np.max(dists)\n        \n        # Avoid division by zero\n        if d_max < 1e-9: return 0.0\n        \n        return - (d_min / d_max)**2\n\n    def de_callback(xk, convergence):\n        \"\"\"Stop DE if we are running out of time.\"\"\"\n        if time.time() - start_time > 40.0:\n            return True\n\n    # Bounds for the search space (Unit box is sufficient due to scale invariance)\n    bounds = [(-1.0, 1.0)] * (N * D)\n\n    # Run Differential Evolution\n    # Strategy 'best1bin' is greedy and works well for packing problems\n    result_de = differential_evolution(\n        objective_de,\n        bounds,\n        strategy='best1bin',\n        maxiter=10000,   # High limit, controlled by callback\n        popsize=15,      # Sufficient for 42 dimensions\n        tol=0.01,\n        mutation=(0.5, 0.9),\n        recombination=0.7,\n        workers=1,       # Single thread to avoid multiprocessing overhead\n        callback=de_callback,\n        disp=False\n    )\n    \n    best_x = result_de.x\n\n    # --- Phase 2: Local Refinement (SLSQP with SoftMin) ---\n    \n    # We use Log-Sum-Exp to approximate Min and Max smoothly.\n    # Objective: Maximize SoftMin(d) subject to SoftMax(d) = 1.\n    # This maximizes the ratio while fixing the scale.\n    \n    alpha = 60.0  # Sharpness parameter for the soft approximation\n\n    def soft_min(d, a):\n        \"\"\"\n        SoftMin(d) = -1/a * log(sum(exp(-a * d)))\n        Implemented with Log-Sum-Exp trick for stability.\n        \"\"\"\n        # x = -alpha * d\n        # We want -1/a * log(sum(exp(x)))\n        x = -a * d\n        x_max = np.max(x)\n        # lse = max + log(sum(exp(x - max)))\n        lse = x_max + np.log(np.sum(np.exp(x - x_max)))\n        return -lse / a\n\n    def soft_max(d, a):\n        \"\"\"\n        SoftMax(d) = 1/a * log(sum(exp(a * d)))\n        \"\"\"\n        x = a * d\n        x_max = np.max(x)\n        lse = x_max + np.log(np.sum(np.exp(x - x_max)))\n        return lse / a\n\n    def objective_slsqp(x):\n        dists = get_dists(x)\n        # Minimize negative SoftMin (Maximize SoftMin)\n        return -soft_min(dists, alpha)\n        \n    def constraint_scale(x):\n        dists = get_dists(x)\n        # Constrain SoftMax to be 1.0 to fix scale\n        return 1.0 - soft_max(dists, alpha)\n\n    # Normalize the DE result to satisfy the constraint initially\n    dists0 = get_dists(best_x)\n    current_softmax = soft_max(dists0, alpha)\n    if current_softmax > 1e-9:\n        best_x = best_x / current_softmax\n\n    # Define constraints for SLSQP\n    cons = {'type': 'eq', 'fun': constraint_scale}\n    \n    # Run SLSQP\n    # Using numerical gradients (jac=None) is fast enough for 42 variables\n    res_slsqp = minimize(\n        objective_slsqp,\n        best_x,\n        method='SLSQP',\n        constraints=cons,\n        options={'maxiter': 250, 'ftol': 1e-7, 'disp': False}\n    )\n    \n    final_x = res_slsqp.x.reshape((N, D))\n    \n    # Center the points at the origin (optional cleanup)\n    final_x -= np.mean(final_x, axis=0)\n    \n    return final_x",
  "metrics": {
    "valid": true,
    "score": 0.9645335634039062,
    "min_max_ratio": 0.2315334487202371,
    "min_distance": 0.4661462438728728,
    "max_distance": 0.9687580375273852,
    "eval_time": 40.57374429702759,
    "error": null
  },
  "prompt": "Optimize 14 points in 3D to maximize the ratio (min_dist / max_dist)^2. Use a hybrid approach: first, use Differential Evolution for global search, then refine the best candidate using SLSQP with a SoftMin (log-sum-exp) objective to handle the non-differentiability of the minimum distance.",
  "response": "Here is the Python solution using Differential Evolution for global search and SLSQP with a SoftMin/SoftMax objective for local refinement.\n\n```python\nimport numpy as np\nfrom scipy.optimize import differential_evolution, minimize\nfrom scipy.spatial.distance import pdist\nimport time\nimport warnings\n\ndef min_max_dist_dim3_14():\n    \"\"\"\n    Optimizes the placement of 14 points in 3D to maximize the squared ratio \n    of the minimum pairwise distance to the maximum pairwise distance.\n    \n    Strategy:\n    1. Global Search: Differential Evolution (DE) to find a good approximate configuration.\n    2. Local Refinement: SLSQP using a Log-Sum-Exp (SoftMin/SoftMax) formulation \n       to smooth the non-differentiable min/max functions and improve precision.\n    \"\"\"\n    \n    # Problem constants\n    N = 14\n    D = 3\n    TIME_LIMIT = 55.0  # Seconds (leaving buffer for overhead)\n    start_time = time.time()\n    \n    # Suppress warnings for clean execution\n    warnings.filterwarnings(\"ignore\")\n\n    # --- Helper Functions ---\n\n    def get_dists(x):\n        \"\"\"Converts flat parameter vector to pairwise distances.\"\"\"\n        pts = x.reshape((N, D))\n        return pdist(pts)\n\n    # --- Phase 1: Global Search (Differential Evolution) ---\n\n    def objective_de(x):\n        \"\"\"\n        Hard objective for DE.\n        Maximize (min_dist / max_dist)^2 => Minimize -(min_dist / max_dist)^2\n        \"\"\"\n        dists = get_dists(x)\n        if dists.size == 0: return 0.0\n        \n        d_min = np.min(dists)\n        d_max = np.max(dists)\n        \n        # Avoid division by zero\n        if d_max < 1e-9: return 0.0\n        \n        return - (d_min / d_max)**2\n\n    def de_callback(xk, convergence):\n        \"\"\"Stop DE if we are running out of time.\"\"\"\n        if time.time() - start_time > 40.0:\n            return True\n\n    # Bounds for the search space (Unit box is sufficient due to scale invariance)\n    bounds = [(-1.0, 1.0)] * (N * D)\n\n    # Run Differential Evolution\n    # Strategy 'best1bin' is greedy and works well for packing problems\n    result_de = differential_evolution(\n        objective_de,\n        bounds,\n        strategy='best1bin',\n        maxiter=10000,   # High limit, controlled by callback\n        popsize=15,      # Sufficient for 42 dimensions\n        tol=0.01,\n        mutation=(0.5, 0.9),\n        recombination=0.7,\n        workers=1,       # Single thread to avoid multiprocessing overhead\n        callback=de_callback,\n        disp=False\n    )\n    \n    best_x = result_de.x\n\n    # --- Phase 2: Local Refinement (SLSQP with SoftMin) ---\n    \n    # We use Log-Sum-Exp to approximate Min and Max smoothly.\n    # Objective: Maximize SoftMin(d) subject to SoftMax(d) = 1.\n    # This maximizes the ratio while fixing the scale.\n    \n    alpha = 60.0  # Sharpness parameter for the soft approximation\n\n    def soft_min(d, a):\n        \"\"\"\n        SoftMin(d) = -1/a * log(sum(exp(-a * d)))\n        Implemented with Log-Sum-Exp trick for stability.\n        \"\"\"\n        # x = -alpha * d\n        # We want -1/a * log(sum(exp(x)))\n        x = -a * d\n        x_max = np.max(x)\n        # lse = max + log(sum(exp(x - max)))\n        lse = x_max + np.log(np.sum(np.exp(x - x_max)))\n        return -lse / a\n\n    def soft_max(d, a):\n        \"\"\"\n        SoftMax(d) = 1/a * log(sum(exp(a * d)))\n        \"\"\"\n        x = a * d\n        x_max = np.max(x)\n        lse = x_max + np.log(np.sum(np.exp(x - x_max)))\n        return lse / a\n\n    def objective_slsqp(x):\n        dists = get_dists(x)\n        # Minimize negative SoftMin (Maximize SoftMin)\n        return -soft_min(dists, alpha)\n        \n    def constraint_scale(x):\n        dists = get_dists(x)\n        # Constrain SoftMax to be 1.0 to fix scale\n        return 1.0 - soft_max(dists, alpha)\n\n    # Normalize the DE result to satisfy the constraint initially\n    dists0 = get_dists(best_x)\n    current_softmax = soft_max(dists0, alpha)\n    if current_softmax > 1e-9:\n        best_x = best_x / current_softmax\n\n    # Define constraints for SLSQP\n    cons = {'type': 'eq', 'fun': constraint_scale}\n    \n    # Run SLSQP\n    # Using numerical gradients (jac=None) is fast enough for 42 variables\n    res_slsqp = minimize(\n        objective_slsqp,\n        best_x,\n        method='SLSQP',\n        constraints=cons,\n        options={'maxiter': 250, 'ftol': 1e-7, 'disp': False}\n    )\n    \n    final_x = res_slsqp.x.reshape((N, D))\n    \n    # Center the points at the origin (optional cleanup)\n    final_x -= np.mean(final_x, axis=0)\n    \n    return final_x\n```",
  "reasoning": "Here is the Python solution using Differential Evolution for global search and SLSQP with a SoftMin/SoftMax objective for local refinement.",
  "timestamp": "2026-01-21T17:09:51.546911",
  "cost_data": null,
  "model_config": {
    "model": "gemini-3-pro-preview",
    "temperature": 0.7,
    "provider_debug": {
      "finish_reason": "STOP",
      "usage_metadata": {
        "prompt_token_count": 243,
        "candidates_token_count": 1407,
        "thoughts_token_count": 8945
      },
      "response_text_len": 4514
    }
  }
}