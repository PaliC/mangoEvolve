{
  "trial_id": "trial_2_9",
  "generation": 2,
  "parent_id": "trial_1_1",
  "code": "import numpy as np\nfrom scipy.optimize import basinhopping, minimize\nimport time\n\ndef min_max_dist_dim3_14():\n    \"\"\"\n    Returns a numpy array of shape (14, 3) maximizing (min_dist / max_dist)^2.\n    \"\"\"\n    N = 14\n    \n    # 1. Initialization: Phyllotactic Spiral on Sphere\n    # Using offset 0.5 for better distribution avoiding poles\n    indices = np.arange(N) + 0.5\n    phi = np.pi * (1 + 5**0.5) # Golden angle\n    \n    z = 1 - 2 * indices / N\n    r = np.sqrt(1 - z**2)\n    theta = phi * indices\n    \n    x = r * np.cos(theta)\n    y = r * np.sin(theta)\n    \n    points = np.column_stack((x, y, z))\n    \n    # 2. Optimization Configuration\n    # We optimize the ratio using a smooth approximation (SoftMin / SoftMax).\n    # Increasing alpha makes the approximation closer to the true min/max.\n    # We use a mutable config to allow annealing alpha during the process if needed,\n    # though 2-stage (exploration/refinement) is usually sufficient.\n    config = {'alpha': 20.0}\n    \n    # Precompute indices for pairwise distances (upper triangle)\n    iu = np.triu_indices(N, 1)\n    \n    def get_d2_diff(p):\n        \"\"\"Compute squared pairwise distances and difference vectors.\"\"\"\n        # p: (N, 3)\n        diff = p[iu[0]] - p[iu[1]]\n        d2 = np.sum(diff**2, axis=1)\n        return d2, diff\n\n    def objective(p_flat):\n        \"\"\"\n        Objective function to minimize.\n        We want to MAXIMIZE (min_d / max_d).\n        This is equivalent to MINIMIZING -(SoftMin_d / SoftMax_d).\n        \"\"\"\n        p = p_flat.reshape((N, 3))\n        d2, _ = get_d2_diff(p)\n        \n        alpha = config['alpha']\n        \n        max_d2 = np.max(d2)\n        min_d2 = np.min(d2)\n        \n        # Smooth Max (LogSumExp)\n        # s_max approx max(d2)\n        exp_max = np.exp(alpha * (d2 - max_d2))\n        s_max = max_d2 + np.log(np.sum(exp_max)) / alpha\n        \n        # Smooth Min (SoftMin)\n        # s_min approx min(d2)\n        exp_min = np.exp(-alpha * (d2 - min_d2))\n        s_min = min_d2 - np.log(np.sum(exp_min)) / alpha\n        \n        # Prevent division by zero or negative results in degenerate cases\n        if s_max < 1e-12: \n            return 0.0\n            \n        # We minimize negative ratio (operating on squared distances directly)\n        # Ratio of squared distances corresponds to squared ratio of distances.\n        return - (s_min / s_max)\n\n    def gradient(p_flat):\n        \"\"\"Analytical gradient of the objective function.\"\"\"\n        p = p_flat.reshape((N, 3))\n        d2, diff = get_d2_diff(p)\n        alpha = config['alpha']\n        \n        # Recompute smooth values\n        max_d2 = np.max(d2)\n        min_d2 = np.min(d2)\n        \n        exp_max = np.exp(alpha * (d2 - max_d2))\n        sum_exp_max = np.sum(exp_max)\n        s_max = max_d2 + np.log(sum_exp_max) / alpha\n        \n        exp_min = np.exp(-alpha * (d2 - min_d2))\n        sum_exp_min = np.sum(exp_min)\n        s_min = min_d2 - np.log(sum_exp_min) / alpha\n        \n        # Weights for the weighted sum of gradients\n        w_max = exp_max / sum_exp_max\n        w_min = exp_min / sum_exp_min\n        \n        # Chain rule: dL/dx = (dL/ds_min * ds_min/dd2 + dL/ds_max * ds_max/dd2) * dd2/dx\n        # L = - s_min / s_max\n        # dL/ds_min = -1 / s_max\n        # dL/ds_max = s_min / s_max^2\n        \n        factor_min = -1.0 / s_max\n        factor_max = s_min / (s_max**2)\n        \n        # Total weight per pair\n        w_total = factor_min * w_min + factor_max * w_max\n        \n        # Gradient w.r.t points\n        # d(d2_uv)/dx_u = 2(x_u - x_v)\n        # d(d2_uv)/dx_v = -2(x_u - x_v)\n        \n        weighted_diff = (w_total[:, None] * 2) * diff\n        \n        grad = np.zeros((N, 3))\n        np.add.at(grad, iu[0], weighted_diff)\n        np.add.at(grad, iu[1], -weighted_diff)\n        \n        return grad.flatten()\n\n    class StepTaker:\n        \"\"\"Custom step taker for basin hopping.\"\"\"\n        def __init__(self, stepsize=0.1):\n            self.stepsize = stepsize\n            \n        def __call__(self, x):\n            # Perturb coordinates\n            x += np.random.uniform(-self.stepsize, self.stepsize, x.shape)\n            \n            # Re-center and normalize to keep points in a reasonable numerical range\n            # (Problem is scale invariant, but numerical stability is better at unit scale)\n            p = x.reshape((N, 3))\n            p -= np.mean(p, axis=0)\n            rms = np.sqrt(np.mean(p**2))\n            if rms > 1e-9: \n                p /= rms\n            return p.flatten()\n\n    # 3. Phase 1: Basin Hopping\n    # Use a moderate alpha for the energy landscape to be smooth enough for hopping\n    config['alpha'] = 20.0\n    \n    # L-BFGS-B is efficient for high-dimensional smooth optimization\n    minimizer_kwargs = {\n        \"method\": \"L-BFGS-B\", \n        \"jac\": gradient, \n        \"options\": {\"maxiter\": 100}\n    }\n    \n    # Run Basin Hopping\n    # niter=200 fits comfortably within 60s (typically takes ~10-15s)\n    res = basinhopping(\n        objective, \n        points.flatten(), \n        niter=200, \n        T=0.002, \n        take_step=StepTaker(0.1), \n        minimizer_kwargs=minimizer_kwargs,\n        seed=42\n    )\n    \n    best_p = res.x\n    \n    # 4. Phase 2: Refinement\n    # Sharpen the alpha to approximate the true Min/Max more closely and polish the result\n    config['alpha'] = 100.0\n    final_res = minimize(\n        objective, \n        best_p, \n        jac=gradient, \n        method='L-BFGS-B', \n        options={\"ftol\": 1e-12, \"maxiter\": 2000}\n    )\n    \n    return final_res.x.reshape((N, 3))",
  "metrics": {
    "valid": true,
    "score": 0.9110783011043523,
    "min_max_ratio": 0.21870167002216628,
    "min_distance": 936.0462089892003,
    "max_distance": 2001.572101338434,
    "eval_time": 1.798494815826416,
    "error": null
  },
  "prompt": "Combine the basin-hopping approach from trial_1_1 with a different initialization. Instead of random points, use points distributed on a phyllotactic spiral on a sphere, which often provides good starting points for Tammes-like problems.",
  "response": "An algorithm to place 14 points in 3D space to maximize the squared ratio of minimum to maximum pairwise distance. This solution combines a phyllotactic spiral initialization on a sphere with a basin-hopping global optimization strategy, using a smooth approximation of the min-max objective function for efficient gradient-based local search.\n\n```python\nimport numpy as np\nfrom scipy.optimize import basinhopping, minimize\nimport time\n\ndef min_max_dist_dim3_14():\n    \"\"\"\n    Returns a numpy array of shape (14, 3) maximizing (min_dist / max_dist)^2.\n    \"\"\"\n    N = 14\n    \n    # 1. Initialization: Phyllotactic Spiral on Sphere\n    # Using offset 0.5 for better distribution avoiding poles\n    indices = np.arange(N) + 0.5\n    phi = np.pi * (1 + 5**0.5) # Golden angle\n    \n    z = 1 - 2 * indices / N\n    r = np.sqrt(1 - z**2)\n    theta = phi * indices\n    \n    x = r * np.cos(theta)\n    y = r * np.sin(theta)\n    \n    points = np.column_stack((x, y, z))\n    \n    # 2. Optimization Configuration\n    # We optimize the ratio using a smooth approximation (SoftMin / SoftMax).\n    # Increasing alpha makes the approximation closer to the true min/max.\n    # We use a mutable config to allow annealing alpha during the process if needed,\n    # though 2-stage (exploration/refinement) is usually sufficient.\n    config = {'alpha': 20.0}\n    \n    # Precompute indices for pairwise distances (upper triangle)\n    iu = np.triu_indices(N, 1)\n    \n    def get_d2_diff(p):\n        \"\"\"Compute squared pairwise distances and difference vectors.\"\"\"\n        # p: (N, 3)\n        diff = p[iu[0]] - p[iu[1]]\n        d2 = np.sum(diff**2, axis=1)\n        return d2, diff\n\n    def objective(p_flat):\n        \"\"\"\n        Objective function to minimize.\n        We want to MAXIMIZE (min_d / max_d).\n        This is equivalent to MINIMIZING -(SoftMin_d / SoftMax_d).\n        \"\"\"\n        p = p_flat.reshape((N, 3))\n        d2, _ = get_d2_diff(p)\n        \n        alpha = config['alpha']\n        \n        max_d2 = np.max(d2)\n        min_d2 = np.min(d2)\n        \n        # Smooth Max (LogSumExp)\n        # s_max approx max(d2)\n        exp_max = np.exp(alpha * (d2 - max_d2))\n        s_max = max_d2 + np.log(np.sum(exp_max)) / alpha\n        \n        # Smooth Min (SoftMin)\n        # s_min approx min(d2)\n        exp_min = np.exp(-alpha * (d2 - min_d2))\n        s_min = min_d2 - np.log(np.sum(exp_min)) / alpha\n        \n        # Prevent division by zero or negative results in degenerate cases\n        if s_max < 1e-12: \n            return 0.0\n            \n        # We minimize negative ratio (operating on squared distances directly)\n        # Ratio of squared distances corresponds to squared ratio of distances.\n        return - (s_min / s_max)\n\n    def gradient(p_flat):\n        \"\"\"Analytical gradient of the objective function.\"\"\"\n        p = p_flat.reshape((N, 3))\n        d2, diff = get_d2_diff(p)\n        alpha = config['alpha']\n        \n        # Recompute smooth values\n        max_d2 = np.max(d2)\n        min_d2 = np.min(d2)\n        \n        exp_max = np.exp(alpha * (d2 - max_d2))\n        sum_exp_max = np.sum(exp_max)\n        s_max = max_d2 + np.log(sum_exp_max) / alpha\n        \n        exp_min = np.exp(-alpha * (d2 - min_d2))\n        sum_exp_min = np.sum(exp_min)\n        s_min = min_d2 - np.log(sum_exp_min) / alpha\n        \n        # Weights for the weighted sum of gradients\n        w_max = exp_max / sum_exp_max\n        w_min = exp_min / sum_exp_min\n        \n        # Chain rule: dL/dx = (dL/ds_min * ds_min/dd2 + dL/ds_max * ds_max/dd2) * dd2/dx\n        # L = - s_min / s_max\n        # dL/ds_min = -1 / s_max\n        # dL/ds_max = s_min / s_max^2\n        \n        factor_min = -1.0 / s_max\n        factor_max = s_min / (s_max**2)\n        \n        # Total weight per pair\n        w_total = factor_min * w_min + factor_max * w_max\n        \n        # Gradient w.r.t points\n        # d(d2_uv)/dx_u = 2(x_u - x_v)\n        # d(d2_uv)/dx_v = -2(x_u - x_v)\n        \n        weighted_diff = (w_total[:, None] * 2) * diff\n        \n        grad = np.zeros((N, 3))\n        np.add.at(grad, iu[0], weighted_diff)\n        np.add.at(grad, iu[1], -weighted_diff)\n        \n        return grad.flatten()\n\n    class StepTaker:\n        \"\"\"Custom step taker for basin hopping.\"\"\"\n        def __init__(self, stepsize=0.1):\n            self.stepsize = stepsize\n            \n        def __call__(self, x):\n            # Perturb coordinates\n            x += np.random.uniform(-self.stepsize, self.stepsize, x.shape)\n            \n            # Re-center and normalize to keep points in a reasonable numerical range\n            # (Problem is scale invariant, but numerical stability is better at unit scale)\n            p = x.reshape((N, 3))\n            p -= np.mean(p, axis=0)\n            rms = np.sqrt(np.mean(p**2))\n            if rms > 1e-9: \n                p /= rms\n            return p.flatten()\n\n    # 3. Phase 1: Basin Hopping\n    # Use a moderate alpha for the energy landscape to be smooth enough for hopping\n    config['alpha'] = 20.0\n    \n    # L-BFGS-B is efficient for high-dimensional smooth optimization\n    minimizer_kwargs = {\n        \"method\": \"L-BFGS-B\", \n        \"jac\": gradient, \n        \"options\": {\"maxiter\": 100}\n    }\n    \n    # Run Basin Hopping\n    # niter=200 fits comfortably within 60s (typically takes ~10-15s)\n    res = basinhopping(\n        objective, \n        points.flatten(), \n        niter=200, \n        T=0.002, \n        take_step=StepTaker(0.1), \n        minimizer_kwargs=minimizer_kwargs,\n        seed=42\n    )\n    \n    best_p = res.x\n    \n    # 4. Phase 2: Refinement\n    # Sharpen the alpha to approximate the true Min/Max more closely and polish the result\n    config['alpha'] = 100.0\n    final_res = minimize(\n        objective, \n        best_p, \n        jac=gradient, \n        method='L-BFGS-B', \n        options={\"ftol\": 1e-12, \"maxiter\": 2000}\n    )\n    \n    return final_res.x.reshape((N, 3))\n```",
  "reasoning": "An algorithm to place 14 points in 3D space to maximize the squared ratio of minimum to maximum pairwise distance. This solution combines a phyllotactic spiral initialization on a sphere with a basin-hopping global optimization strategy, using a smooth approximation of the min-max objective function for efficient gradient-based local search.",
  "timestamp": "2026-01-21T17:21:50.934623",
  "cost_data": null,
  "model_config": {
    "model": "gemini-3-pro-preview",
    "temperature": 0.7,
    "provider_debug": {
      "finish_reason": "STOP",
      "usage_metadata": {
        "prompt_token_count": 220,
        "candidates_token_count": 1950,
        "thoughts_token_count": 12378
      },
      "response_text_len": 5917
    }
  }
}