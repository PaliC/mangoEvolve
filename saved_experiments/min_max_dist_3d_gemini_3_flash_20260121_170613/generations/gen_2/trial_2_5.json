{
  "trial_id": "trial_2_5",
  "generation": 2,
  "parent_id": "trial_1_1",
  "code": "import numpy as np\nfrom scipy.optimize import minimize\nimport time\n\ndef min_max_dist_dim3_14():\n    \"\"\"\n    Finds the coordinates of 14 points in 3D space that maximize the ratio \n    (min_dist / max_dist)^2.\n    \n    Algorithm:\n    1. Initialize points on a Fibonacci sphere for a near-optimal distribution.\n    2. Perform Iterated Local Search (similar to Basin-hopping) using SLSQP with loose tolerance.\n       - This explores the energy landscape to find a deep basin of attraction.\n    3. Perform a final 'polishing' step using SLSQP with very high precision tolerances.\n       - This maximizes the minimum distance within the identified basin.\n    \"\"\"\n    N = 14\n    dim = 3\n    num_pairs = N * (N - 1) // 2\n    \n    # Precompute indices for all pairs to vectorize distance calculations\n    triu_indices = np.triu_indices(N, k=1)\n    idx_i = triu_indices[0]\n    idx_j = triu_indices[1]\n    \n    # --- Optimization Definitions ---\n    \n    # State vector x: [p1_x, p1_y, p1_z, ..., p14_z, s]\n    # s is the squared minimum distance (variable to maximize)\n    \n    def objective(x):\n        # Minimize negative s -> Maximize s\n        return -x[-1]\n\n    def obj_jac(x):\n        # Gradient of objective: [0, ..., 0, -1]\n        grad = np.zeros_like(x)\n        grad[-1] = -1.0\n        return grad\n    \n    def constraints(x):\n        points = x[:-1].reshape((N, dim))\n        s = x[-1]\n        diffs = points[idx_i] - points[idx_j]\n        d2 = np.sum(diffs**2, axis=1)\n        \n        # Constraints:\n        # 1. d_ij^2 >= s  =>  d_ij^2 - s >= 0\n        # 2. d_ij^2 <= 1  =>  1 - d_ij^2 >= 0\n        return np.concatenate([d2 - s, 1.0 - d2])\n\n    def constraints_jac(x):\n        points = x[:-1].reshape((N, dim))\n        diffs = points[idx_i] - points[idx_j]\n        two_diffs = 2 * diffs\n        \n        # Jacobian matrix shape: (2 * num_pairs, N * dim + 1)\n        jac = np.zeros((2 * num_pairs, N * dim + 1))\n        \n        # Fill gradients efficiently using a loop (fast for N=14)\n        for k in range(num_pairs):\n            i = idx_i[k]\n            j = idx_j[k]\n            \n            # Indices in the flattened state vector\n            sl_i = slice(i*3, i*3+3)\n            sl_j = slice(j*3, j*3+3)\n            \n            # Row k: d2 - s >= 0\n            # d(d2)/dpi = 2(pi-pj), d(d2)/dpj = -2(pi-pj), d(-s)/ds = -1\n            jac[k, sl_i] = two_diffs[k]\n            jac[k, sl_j] = -two_diffs[k]\n            jac[k, -1] = -1.0\n            \n            # Row num_pairs + k: 1 - d2 >= 0\n            # Gradients are negative of above (s term is 0)\n            jac[num_pairs + k, sl_i] = -two_diffs[k]\n            jac[num_pairs + k, sl_j] = two_diffs[k]\n            \n        return jac\n\n    cons = {'type': 'ineq', 'fun': constraints, 'jac': constraints_jac}\n    \n    # --- Initialization ---\n    \n    # Fibonacci sphere for good initial distribution\n    indices = np.arange(0, N, dtype=float) + 0.5\n    phi = np.arccos(1 - 2*indices/N)\n    theta = np.pi * (1 + 5**0.5) * indices\n    x_init = np.cos(theta) * np.sin(phi)\n    y_init = np.sin(theta) * np.sin(phi)\n    z_init = np.cos(phi)\n    points_init = np.column_stack((x_init, y_init, z_init))\n    \n    # Scale to diameter ~1.0 (Unit sphere diameter is 2.0)\n    points_init *= 0.5\n    \n    # Calculate initial s\n    diffs = points_init[idx_i] - points_init[idx_j]\n    d2_init = np.sum(diffs**2, axis=1)\n    s_init = np.min(d2_init)\n    \n    x0 = np.concatenate([points_init.flatten(), [s_init]])\n    \n    # --- Global Search Loop ---\n    \n    start_time = time.time()\n    time_limit = 55.0 # Total budget\n    polish_duration = 10.0 # Reserve for polishing\n    \n    best_x = x0\n    best_fun = 0.0 # Objective is negative, so 0 is a safe upper bound for init\n    \n    # Initial optimization\n    res = minimize(objective, x0, jac=obj_jac, constraints=cons, method='SLSQP', \n                   options={'ftol': 1e-4, 'maxiter': 500})\n    if res.fun < best_fun:\n        best_fun = res.fun\n        best_x = res.x\n    \n    cnt = 0\n    # Run iterated local search until time reserve\n    while time.time() - start_time < (time_limit - polish_duration):\n        cnt += 1\n        \n        # Perturbation strategy: Shake the current best points slightly\n        current_pts = best_x[:-1].reshape((N, dim))\n        \n        # Decay perturbation magnitude over time to settle into the basin\n        perturb_scale = 0.15 * np.exp(-cnt / 30.0)\n        perturb_scale = max(perturb_scale, 0.02)\n        \n        perturbation = np.random.uniform(-perturb_scale, perturb_scale, (N, dim))\n        new_pts = current_pts + perturbation\n        \n        # Feasibility projection (Normalize max dist to <= 1) to help optimizer\n        d_diffs = new_pts[idx_i] - new_pts[idx_j]\n        max_d = np.sqrt(np.max(np.sum(d_diffs**2, axis=1)))\n        if max_d > 1e-9:\n            new_pts /= max_d\n            \n        # Reset s\n        d_diffs = new_pts[idx_i] - new_pts[idx_j]\n        new_s = np.min(np.sum(d_diffs**2, axis=1))\n        \n        x_trial = np.concatenate([new_pts.flatten(), [new_s]])\n        \n        # Fast local optimization with loose tolerance\n        res = minimize(objective, x_trial, jac=obj_jac, constraints=cons, method='SLSQP', \n                       options={'ftol': 1e-4, 'maxiter': 200})\n        \n        if res.fun < best_fun:\n            best_fun = res.fun\n            best_x = res.x\n\n    # --- Polishing Step ---\n    \n    # Run high-precision optimization on the best result found.\n    # We restart the optimizer to reset the Hessian approximation and use tight tolerance (1e-12).\n    res_polish = minimize(objective, best_x, jac=obj_jac, constraints=cons, method='SLSQP', \n                          options={'ftol': 1e-12, 'maxiter': 2000})\n    \n    final_x = res_polish.x\n    final_points = final_x[:-1].reshape((N, dim))\n    \n    # --- Final Normalization ---\n    \n    # Ensure max distance is exactly 1.0 to maximize the score.\n    # The optimizer constraints ensure d <= 1, but scaling up to exactly 1 is valid and optimal.\n    diffs = final_points[idx_i] - final_points[idx_j]\n    dists = np.sqrt(np.sum(diffs**2, axis=1))\n    max_d = np.max(dists)\n    \n    if max_d > 1e-9:\n        final_points /= max_d\n        \n    return final_points",
  "metrics": {
    "valid": true,
    "score": 0.9994412152467665,
    "min_max_ratio": 0.23991292800904468,
    "min_distance": 0.4898090730162567,
    "max_distance": 1.0,
    "eval_time": 45.22976064682007,
    "error": null
  },
  "prompt": "Take the best configuration from trial_1_1 and perform a 'polishing' step. Use a very high-precision optimizer or a coordinate descent method where you move one point at a time to maximize its distance from all others, keeping the max distance constant. Trial code:\nimport numpy as np\nfrom scipy.optimize import minimize, basinhopping\nimport time\n\ndef min_max_dist_dim3_14():\n    \"\"\"\n    Finds the coordinates of 14 points in 3D space that maximize the ratio \n    (min_dist / max_dist)^2 using Basin-hopping with SLSQP.\n    \"\"\"\n    N = 14\n    dim = 3\n    num_pairs = N * (N - 1) // 2\n    \n    # Precompute indices for all pairs to vectorize distance calculations\n    triu_indices = np.triu_indices(N, k=1)\n    idx_i = triu_indices[0]\n    idx_j = triu_indices[1]\n    \n    # Initialize points on a Fibonacci sphere for a near-optimal starting configuration\n    # This reduces the time needed for convergence compared to random initialization\n    indices = np.arange(0, N, dtype=float) + 0.5\n    phi = np.arccos(1 - 2*indices/N)\n    theta = np.pi * (1 + 5**0.5) * indices\n    x_init = np.cos(theta) * np.sin(phi)\n    y_init = np.sin(theta) * np.sin(phi)\n    z_init = np.cos(phi)\n    points_init = np.column_stack((x_init, y_init, z_init))\n    \n    # Scale to approximate diameter 1.0 (Unit sphere has diameter 2.0)\n    points_init *= 0.5\n    \n    # Calculate initial squared min distance\n    diffs_init = points_init[idx_i] - points_init[idx_j]\n    d2_init = np.sum(diffs_init**2, axis=1)\n    s_init = np.min(d2_init)\n    \n    # State vector: [x1, y1, z1, ..., x14, y14, z14, s]\n    x0 = np.concatenate([points_init.flatten(), [s_init]])\n    \n    # Objective: Maximize s => Minimize -s\n    def objective(x):\n        return -x[-1]\n\n    # Gradient of objective\n    def obj_jac(x):\n        grad = np.zeros_like(x)\n        grad[-1] = -1.0\n        return grad\n    \n    # Constraint functions for SLSQP\n    # 1. dist^2_ij >= s  => dist^2_ij - s >= 0\n    # 2. dist^2_ij <= 1  => 1 - dist^2_ij >= 0\n    def constraints(x):\n        points = x[:-1].reshape((N, dim))\n        s = x[-1]\n        diffs = points[idx_i] - points[idx_j]\n        d2 = np.sum(diffs**2, axis=1)\n        return np.concatenate([d2 - s, 1.0 - d2])\n\n    # Jacobian of constraints\n    def constraints_jac(x):\n        points = x[:-1].reshape((N, dim))\n        diffs = points[idx_i] - points[idx_j] # Shape (num_pairs, 3)\n        two_diffs = 2 * diffs\n        \n        # Jacobian matrix shape: (2 * num_pairs, N * dim + 1)\n        jac = np.zeros((2 * num_pairs, N * dim + 1))\n        \n        # We fill the matrix efficiently\n        # Constraint 1: d2 - s >= 0\n        # Derivatives w.r.t p_i are 2*(p_i - p_j), w.r.t p_j are -2*(p_i - p_j)\n        # Derivative w.r.t s is -1\n        \n        # Constraint 2: 1 - d2 >= 0\n        # Derivatives are negative of above (s term is 0)\n        \n        for k in range(num_pairs):\n            i = idx_i[k]\n            j = idx_j[k]\n            \n            # Indices in the flattened state vector\n            sl_i = slice(i*3, i*3+3)\n            sl_j = slice(j*3, j*3+3)\n            \n            # Row k (d2 - s)\n            jac[k, sl_i] = two_diffs[k]\n            jac[k, sl_j] = -two_diffs[k]\n            jac[k, -1] = -1.0\n            \n            # Row num_pairs + k (1 - d2)\n            jac[num_pairs + k, sl_i] = -two_diffs[k]\n            jac[num_pairs + k, sl_j] = two_diffs[k]\n            \n        return jac\n\n    # Define constraints dictionary\n    cons = {'type': 'ineq', 'fun': constraints, 'jac': constraints_jac}\n    \n    # Custom step taker for Basin-hopping\n    # Perturbs points and normalizes them to help the optimizer\n    class NormalizedStep:\n        def __init__(self, stepsize=0.1):\n            self.stepsize = stepsize\n            self.rng = np.random.default_rng()\n            \n        def __call__(self, x):\n            coords = x[:-1].reshape((N, dim))\n            perturbation = self.rng.uniform(-self.stepsize, self.stepsize, size=coords.shape)\n            new_coords = coords + perturbation\n            \n            # Normalize configuration to have max dist = 1.0\n            # This satisfies the prompt's instruction and keeps the search feasible\n            diffs = new_coords[idx_i] - new_coords[idx_j]\n            dists = np.sqrt(np.sum(diffs**2, axis=1))\n            max_d = np.max(dists)\n            if max_d > 1e-9:\n                new_coords /= max_d\n            \n            # Reset s to a feasible value (slightly less than min_dist^2)\n            new_diffs = new_coords[idx_i] - new_coords[idx_j]\n            new_s = np.min(np.sum(new_diffs**2, axis=1)) * 0.95\n            \n            return np.concatenate([new_coords.flatten(), [new_s]])\n\n    take_step = NormalizedStep(stepsize=0.15)\n    \n    # Minimizer options\n    minimizer_kwargs = {\n        'method': 'SLSQP',\n        'jac': obj_jac,\n        'constraints': cons,\n        'options': {'ftol': 1e-6, 'maxiter': 200}\n    }\n    \n    # Time management\n    start_time = time.time()\n    time_limit = 55 # seconds\n    \n    # Initial local minimization\n    res = minimize(objective, x0, jac=obj_jac, constraints=cons, method='SLSQP')\n    best_x = res.x\n    best_fun = res.fun\n    \n    # Basin-hopping loop\n    # We run short bursts of basin-hopping and check time\n    while time.time() - start_time < time_limit:\n        # Restart from current best to perform iterated local search\n        bh_res = basinhopping(objective, best_x, niter=5, \n                              minimizer_kwargs=minimizer_kwargs, \n                              take_step=take_step)\n        \n        # Check if we found a better solution\n        if bh_res.lowest_optimization_result.fun < best_fun:\n            best_fun = bh_res.lowest_optimization_result.fun\n            best_x = bh_res.lowest_optimization_result.x\n\n    # Extract final points\n    final_points = best_x[:-1].reshape((N, dim))\n    \n    # Final normalization to ensure max_dist is exactly 1.0\n    diffs = final_points[idx_i] - final_points[idx_j]\n    max_d = np.sqrt(np.max(np.sum(diffs**2, axis=1)))\n    if max_d > 0:\n        final_points /= max_d\n        \n    return final_points",
  "response": "Here is the polished solution using an Iterated Local Search (Basin-hopping) strategy followed by a high-precision SLSQP polishing step.\n\n```python\nimport numpy as np\nfrom scipy.optimize import minimize\nimport time\n\ndef min_max_dist_dim3_14():\n    \"\"\"\n    Finds the coordinates of 14 points in 3D space that maximize the ratio \n    (min_dist / max_dist)^2.\n    \n    Algorithm:\n    1. Initialize points on a Fibonacci sphere for a near-optimal distribution.\n    2. Perform Iterated Local Search (similar to Basin-hopping) using SLSQP with loose tolerance.\n       - This explores the energy landscape to find a deep basin of attraction.\n    3. Perform a final 'polishing' step using SLSQP with very high precision tolerances.\n       - This maximizes the minimum distance within the identified basin.\n    \"\"\"\n    N = 14\n    dim = 3\n    num_pairs = N * (N - 1) // 2\n    \n    # Precompute indices for all pairs to vectorize distance calculations\n    triu_indices = np.triu_indices(N, k=1)\n    idx_i = triu_indices[0]\n    idx_j = triu_indices[1]\n    \n    # --- Optimization Definitions ---\n    \n    # State vector x: [p1_x, p1_y, p1_z, ..., p14_z, s]\n    # s is the squared minimum distance (variable to maximize)\n    \n    def objective(x):\n        # Minimize negative s -> Maximize s\n        return -x[-1]\n\n    def obj_jac(x):\n        # Gradient of objective: [0, ..., 0, -1]\n        grad = np.zeros_like(x)\n        grad[-1] = -1.0\n        return grad\n    \n    def constraints(x):\n        points = x[:-1].reshape((N, dim))\n        s = x[-1]\n        diffs = points[idx_i] - points[idx_j]\n        d2 = np.sum(diffs**2, axis=1)\n        \n        # Constraints:\n        # 1. d_ij^2 >= s  =>  d_ij^2 - s >= 0\n        # 2. d_ij^2 <= 1  =>  1 - d_ij^2 >= 0\n        return np.concatenate([d2 - s, 1.0 - d2])\n\n    def constraints_jac(x):\n        points = x[:-1].reshape((N, dim))\n        diffs = points[idx_i] - points[idx_j]\n        two_diffs = 2 * diffs\n        \n        # Jacobian matrix shape: (2 * num_pairs, N * dim + 1)\n        jac = np.zeros((2 * num_pairs, N * dim + 1))\n        \n        # Fill gradients efficiently using a loop (fast for N=14)\n        for k in range(num_pairs):\n            i = idx_i[k]\n            j = idx_j[k]\n            \n            # Indices in the flattened state vector\n            sl_i = slice(i*3, i*3+3)\n            sl_j = slice(j*3, j*3+3)\n            \n            # Row k: d2 - s >= 0\n            # d(d2)/dpi = 2(pi-pj), d(d2)/dpj = -2(pi-pj), d(-s)/ds = -1\n            jac[k, sl_i] = two_diffs[k]\n            jac[k, sl_j] = -two_diffs[k]\n            jac[k, -1] = -1.0\n            \n            # Row num_pairs + k: 1 - d2 >= 0\n            # Gradients are negative of above (s term is 0)\n            jac[num_pairs + k, sl_i] = -two_diffs[k]\n            jac[num_pairs + k, sl_j] = two_diffs[k]\n            \n        return jac\n\n    cons = {'type': 'ineq', 'fun': constraints, 'jac': constraints_jac}\n    \n    # --- Initialization ---\n    \n    # Fibonacci sphere for good initial distribution\n    indices = np.arange(0, N, dtype=float) + 0.5\n    phi = np.arccos(1 - 2*indices/N)\n    theta = np.pi * (1 + 5**0.5) * indices\n    x_init = np.cos(theta) * np.sin(phi)\n    y_init = np.sin(theta) * np.sin(phi)\n    z_init = np.cos(phi)\n    points_init = np.column_stack((x_init, y_init, z_init))\n    \n    # Scale to diameter ~1.0 (Unit sphere diameter is 2.0)\n    points_init *= 0.5\n    \n    # Calculate initial s\n    diffs = points_init[idx_i] - points_init[idx_j]\n    d2_init = np.sum(diffs**2, axis=1)\n    s_init = np.min(d2_init)\n    \n    x0 = np.concatenate([points_init.flatten(), [s_init]])\n    \n    # --- Global Search Loop ---\n    \n    start_time = time.time()\n    time_limit = 55.0 # Total budget\n    polish_duration = 10.0 # Reserve for polishing\n    \n    best_x = x0\n    best_fun = 0.0 # Objective is negative, so 0 is a safe upper bound for init\n    \n    # Initial optimization\n    res = minimize(objective, x0, jac=obj_jac, constraints=cons, method='SLSQP', \n                   options={'ftol': 1e-4, 'maxiter': 500})\n    if res.fun < best_fun:\n        best_fun = res.fun\n        best_x = res.x\n    \n    cnt = 0\n    # Run iterated local search until time reserve\n    while time.time() - start_time < (time_limit - polish_duration):\n        cnt += 1\n        \n        # Perturbation strategy: Shake the current best points slightly\n        current_pts = best_x[:-1].reshape((N, dim))\n        \n        # Decay perturbation magnitude over time to settle into the basin\n        perturb_scale = 0.15 * np.exp(-cnt / 30.0)\n        perturb_scale = max(perturb_scale, 0.02)\n        \n        perturbation = np.random.uniform(-perturb_scale, perturb_scale, (N, dim))\n        new_pts = current_pts + perturbation\n        \n        # Feasibility projection (Normalize max dist to <= 1) to help optimizer\n        d_diffs = new_pts[idx_i] - new_pts[idx_j]\n        max_d = np.sqrt(np.max(np.sum(d_diffs**2, axis=1)))\n        if max_d > 1e-9:\n            new_pts /= max_d\n            \n        # Reset s\n        d_diffs = new_pts[idx_i] - new_pts[idx_j]\n        new_s = np.min(np.sum(d_diffs**2, axis=1))\n        \n        x_trial = np.concatenate([new_pts.flatten(), [new_s]])\n        \n        # Fast local optimization with loose tolerance\n        res = minimize(objective, x_trial, jac=obj_jac, constraints=cons, method='SLSQP', \n                       options={'ftol': 1e-4, 'maxiter': 200})\n        \n        if res.fun < best_fun:\n            best_fun = res.fun\n            best_x = res.x\n\n    # --- Polishing Step ---\n    \n    # Run high-precision optimization on the best result found.\n    # We restart the optimizer to reset the Hessian approximation and use tight tolerance (1e-12).\n    res_polish = minimize(objective, best_x, jac=obj_jac, constraints=cons, method='SLSQP', \n                          options={'ftol': 1e-12, 'maxiter': 2000})\n    \n    final_x = res_polish.x\n    final_points = final_x[:-1].reshape((N, dim))\n    \n    # --- Final Normalization ---\n    \n    # Ensure max distance is exactly 1.0 to maximize the score.\n    # The optimizer constraints ensure d <= 1, but scaling up to exactly 1 is valid and optimal.\n    diffs = final_points[idx_i] - final_points[idx_j]\n    dists = np.sqrt(np.sum(diffs**2, axis=1))\n    max_d = np.max(dists)\n    \n    if max_d > 1e-9:\n        final_points /= max_d\n        \n    return final_points\n```",
  "reasoning": "Here is the polished solution using an Iterated Local Search (Basin-hopping) strategy followed by a high-precision SLSQP polishing step.",
  "timestamp": "2026-01-21T17:22:13.206868",
  "cost_data": null,
  "model_config": {
    "model": "gemini-3-pro-preview",
    "temperature": 0.7,
    "provider_debug": {
      "finish_reason": "STOP",
      "usage_metadata": {
        "prompt_token_count": 2189,
        "candidates_token_count": 2140,
        "thoughts_token_count": 10209
      },
      "response_text_len": 6366
    }
  }
}