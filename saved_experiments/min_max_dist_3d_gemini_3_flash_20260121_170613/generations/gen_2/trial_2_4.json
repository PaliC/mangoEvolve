{
  "trial_id": "trial_2_4",
  "generation": 2,
  "parent_id": null,
  "code": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.spatial.distance import pdist, squareform\nimport time\n\ndef min_max_dist_dim3_14():\n    \"\"\"\n    Places 14 points in 3D space to maximize the squared ratio (min pairwise distance / max pairwise distance)^2.\n    Uses a LogSumExp-based SoftMin/SoftMax objective function and L-BFGS-B optimization.\n    \"\"\"\n    n_points = 14\n    dim = 3\n    # p controls how closely the SoftMin/SoftMax approximates the true Min/Max.\n    p = 12\n    \n    def objective_and_gradient(pts_flat):\n        # Reshape to (14, 3)\n        pts = pts_flat.reshape((n_points, dim))\n        \n        # Center and scale points to maintain numerical stability.\n        # The objective (ratio of distances) is scale-invariant and translation-invariant.\n        pts_centered = pts - np.mean(pts, axis=0)\n        scale = np.linalg.norm(pts_centered)\n        if scale < 1e-9:\n            scale = 1.0\n        pts_scaled = pts_centered / scale\n        \n        # Calculate pairwise distances and their squares\n        dists = pdist(pts_scaled)\n        dists_sq = dists**2\n        # Avoid division by zero or log of zero\n        dists_sq = np.maximum(dists_sq, 1e-15)\n        \n        # LogSumExp for SoftMin: approximates ln(sum(d2^-p))\n        # This pushes the minimum distance to be as large as possible.\n        v_neg = -p * np.log(dists_sq)\n        v_neg_max = np.max(v_neg)\n        exp_v_neg = np.exp(v_neg - v_neg_max)\n        sum_exp_v_neg = np.sum(exp_v_neg)\n        log_sum_exp_neg = v_neg_max + np.log(sum_exp_v_neg)\n        \n        # LogSumExp for SoftMax: approximates ln(sum(d2^p))\n        # This pulls the maximum distance to be as small as possible.\n        v_pos = p * np.log(dists_sq)\n        v_pos_max = np.max(v_pos)\n        exp_v_pos = np.exp(v_pos - v_pos_max)\n        sum_exp_v_pos = np.sum(exp_v_pos)\n        log_sum_exp_pos = v_pos_max + np.log(sum_exp_v_pos)\n        \n        # Objective: minimize the ratio (SoftMax / SoftMin)\n        # In log space, this is log(SoftMax) + log(SoftMin)\n        obj = log_sum_exp_neg + log_sum_exp_pos\n        \n        # Gradient calculation using the chain rule and LogSumExp derivative properties\n        # d(log sum exp(v_i))/dx = sum(exp(v_i) * dv_i/dx) / sum(exp(v_i))\n        weights = (exp_v_neg / sum_exp_v_neg * (-p / dists_sq) + \n                   exp_v_pos / sum_exp_v_pos * (p / dists_sq)) * 2\n        \n        # Vectorized gradient construction\n        weight_matrix = squareform(weights)\n        sum_w = np.sum(weight_matrix, axis=1)\n        grad = pts_scaled * sum_w[:, np.newaxis] - weight_matrix @ pts_scaled\n        \n        # Return objective and flattened gradient, adjusted for the scaling factor\n        return obj, grad.flatten() / scale\n\n    best_pts = None\n    best_ratio_sq = -1.0\n    \n    start_time = time.time()\n    \n    # Pre-defined structured starting configurations to improve convergence\n    phi = (1 + 5**0.5) / 2\n    ico = [[0, i, j * phi] for i in [-1, 1] for j in [-1, 1]] + \\\n          [[i, j * phi, 0] for i in [-1, 1] for j in [-1, 1]] + \\\n          [[j * phi, 0, i] for i in [-1, 1] for j in [-1, 1]]\n    pts_ico_start = np.vstack([ico, [[0, 0, 0], [1, 1, 1]]])\n    \n    cube = [[i, j, k] for i in [-1, 1] for j in [-1, 1] for k in [-1, 1]]\n    face_centers = [[1, 0, 0], [-1, 0, 0], [0, 1, 0], [0, -1, 0], [0, 0, 1], [0, 0, -1]]\n    pts_cube_start = np.vstack([cube, face_centers])\n    \n    count = 0\n    # Multi-start loop to avoid local optima\n    while time.time() - start_time < 55:\n        if count == 0:\n            pts = pts_ico_start\n        elif count == 1:\n            pts = pts_cube_start\n        else:\n            # Random starts for exploration\n            pts = np.random.normal(0, 1, (n_points, dim))\n        \n        # L-BFGS-B optimization\n        res = minimize(objective_and_gradient, pts.flatten(), jac=True, method='L-BFGS-B', \n                       options={'maxiter': 300, 'ftol': 1e-10})\n        \n        # Calculate the true squared ratio for the resulting configuration\n        pts_final = res.x.reshape((n_points, dim))\n        dists = pdist(pts_final)\n        if len(dists) > 0:\n            ratio_sq = np.min(dists)**2 / np.max(dists)**2\n            if ratio_sq > best_ratio_sq:\n                best_ratio_sq = ratio_sq\n                best_pts = pts_final\n        \n        count += 1\n                \n    return best_pts",
  "metrics": {
    "valid": true,
    "score": 0.9515534981527467,
    "min_max_ratio": 0.22841762218372066,
    "min_distance": 1.9169807199237197,
    "max_distance": 4.011002584818386,
    "eval_time": 55.232014894485474,
    "error": null
  },
  "prompt": "Implement a 'LogSumExp' (SoftMin) objective function to maximize the minimum distance between 14 points. This makes the objective differentiable. Use 'L-BFGS-B' for optimization and wrap it in a multi-start loop to avoid local optima.",
  "response": "```python\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.spatial.distance import pdist, squareform\nimport time\n\ndef min_max_dist_dim3_14():\n    \"\"\"\n    Places 14 points in 3D space to maximize the squared ratio (min pairwise distance / max pairwise distance)^2.\n    Uses a LogSumExp-based SoftMin/SoftMax objective function and L-BFGS-B optimization.\n    \"\"\"\n    n_points = 14\n    dim = 3\n    # p controls how closely the SoftMin/SoftMax approximates the true Min/Max.\n    p = 12\n    \n    def objective_and_gradient(pts_flat):\n        # Reshape to (14, 3)\n        pts = pts_flat.reshape((n_points, dim))\n        \n        # Center and scale points to maintain numerical stability.\n        # The objective (ratio of distances) is scale-invariant and translation-invariant.\n        pts_centered = pts - np.mean(pts, axis=0)\n        scale = np.linalg.norm(pts_centered)\n        if scale < 1e-9:\n            scale = 1.0\n        pts_scaled = pts_centered / scale\n        \n        # Calculate pairwise distances and their squares\n        dists = pdist(pts_scaled)\n        dists_sq = dists**2\n        # Avoid division by zero or log of zero\n        dists_sq = np.maximum(dists_sq, 1e-15)\n        \n        # LogSumExp for SoftMin: approximates ln(sum(d2^-p))\n        # This pushes the minimum distance to be as large as possible.\n        v_neg = -p * np.log(dists_sq)\n        v_neg_max = np.max(v_neg)\n        exp_v_neg = np.exp(v_neg - v_neg_max)\n        sum_exp_v_neg = np.sum(exp_v_neg)\n        log_sum_exp_neg = v_neg_max + np.log(sum_exp_v_neg)\n        \n        # LogSumExp for SoftMax: approximates ln(sum(d2^p))\n        # This pulls the maximum distance to be as small as possible.\n        v_pos = p * np.log(dists_sq)\n        v_pos_max = np.max(v_pos)\n        exp_v_pos = np.exp(v_pos - v_pos_max)\n        sum_exp_v_pos = np.sum(exp_v_pos)\n        log_sum_exp_pos = v_pos_max + np.log(sum_exp_v_pos)\n        \n        # Objective: minimize the ratio (SoftMax / SoftMin)\n        # In log space, this is log(SoftMax) + log(SoftMin)\n        obj = log_sum_exp_neg + log_sum_exp_pos\n        \n        # Gradient calculation using the chain rule and LogSumExp derivative properties\n        # d(log sum exp(v_i))/dx = sum(exp(v_i) * dv_i/dx) / sum(exp(v_i))\n        weights = (exp_v_neg / sum_exp_v_neg * (-p / dists_sq) + \n                   exp_v_pos / sum_exp_v_pos * (p / dists_sq)) * 2\n        \n        # Vectorized gradient construction\n        weight_matrix = squareform(weights)\n        sum_w = np.sum(weight_matrix, axis=1)\n        grad = pts_scaled * sum_w[:, np.newaxis] - weight_matrix @ pts_scaled\n        \n        # Return objective and flattened gradient, adjusted for the scaling factor\n        return obj, grad.flatten() / scale\n\n    best_pts = None\n    best_ratio_sq = -1.0\n    \n    start_time = time.time()\n    \n    # Pre-defined structured starting configurations to improve convergence\n    phi = (1 + 5**0.5) / 2\n    ico = [[0, i, j * phi] for i in [-1, 1] for j in [-1, 1]] + \\\n          [[i, j * phi, 0] for i in [-1, 1] for j in [-1, 1]] + \\\n          [[j * phi, 0, i] for i in [-1, 1] for j in [-1, 1]]\n    pts_ico_start = np.vstack([ico, [[0, 0, 0], [1, 1, 1]]])\n    \n    cube = [[i, j, k] for i in [-1, 1] for j in [-1, 1] for k in [-1, 1]]\n    face_centers = [[1, 0, 0], [-1, 0, 0], [0, 1, 0], [0, -1, 0], [0, 0, 1], [0, 0, -1]]\n    pts_cube_start = np.vstack([cube, face_centers])\n    \n    count = 0\n    # Multi-start loop to avoid local optima\n    while time.time() - start_time < 55:\n        if count == 0:\n            pts = pts_ico_start\n        elif count == 1:\n            pts = pts_cube_start\n        else:\n            # Random starts for exploration\n            pts = np.random.normal(0, 1, (n_points, dim))\n        \n        # L-BFGS-B optimization\n        res = minimize(objective_and_gradient, pts.flatten(), jac=True, method='L-BFGS-B', \n                       options={'maxiter': 300, 'ftol': 1e-10})\n        \n        # Calculate the true squared ratio for the resulting configuration\n        pts_final = res.x.reshape((n_points, dim))\n        dists = pdist(pts_final)\n        if len(dists) > 0:\n            ratio_sq = np.min(dists)**2 / np.max(dists)**2\n            if ratio_sq > best_ratio_sq:\n                best_ratio_sq = ratio_sq\n                best_pts = pts_final\n        \n        count += 1\n                \n    return best_pts\n\n```",
  "reasoning": "",
  "timestamp": "2026-01-21T17:22:31.182858",
  "cost_data": null,
  "model_config": {
    "model": "gemini-3-flash-preview",
    "temperature": 0.7,
    "provider_debug": {
      "finish_reason": "STOP",
      "usage_metadata": {
        "prompt_token_count": 227,
        "candidates_token_count": 1496,
        "thoughts_token_count": 18898
      },
      "response_text_len": 4392
    }
  }
}