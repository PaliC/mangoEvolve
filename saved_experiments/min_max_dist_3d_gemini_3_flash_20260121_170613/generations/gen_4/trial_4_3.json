{
  "trial_id": "trial_4_3",
  "generation": 4,
  "parent_id": "trial_3_6",
  "code": "import numpy as np\nimport time\nfrom scipy.spatial.distance import pdist\nfrom scipy.optimize import minimize\n\ndef min_max_dist_dim3_14():\n    \"\"\"\n    Finds a configuration of 14 points in 3D that maximizes the ratio (min_dist / max_dist)^2.\n    Uses a mini-CMA-ES implementation for optimization.\n    \"\"\"\n    \n    # ---------------------------------------------------------\n    # 1. Initialization: Generating \"trial_3_6\" Mean\n    # ---------------------------------------------------------\n    # Since the specific 'trial_3_6' data is not persistent in this environment,\n    # we simulate it by running a quick pre-optimization (L-BFGS-B) on a repulsive \n    # potential to generate a high-quality starting configuration (mean).\n    \n    N_POINTS = 14\n    DIM = 3\n    N_VARS = N_POINTS * DIM\n    \n    np.random.seed(42)  # Ensure reproducibility of the starting point\n    \n    def initial_energy(x):\n        \"\"\"Soft repulsive potential to spread points roughly on a sphere.\"\"\"\n        pts = x.reshape((N_POINTS, DIM))\n        # Add soft constraint to keep points near unit sphere\n        radii = np.linalg.norm(pts, axis=1)\n        sphere_penalty = np.sum((radii - 1.0)**2)\n        \n        # Pairwise repulsion (Thomson-like)\n        dists = pdist(pts)\n        # Avoid div by zero\n        repulsion = np.sum(1.0 / (dists**2 + 1e-6))\n        \n        return repulsion + 500 * sphere_penalty\n\n    # Start from random and optimize briefly\n    x0 = np.random.randn(N_VARS)\n    res = minimize(initial_energy, x0, method='L-BFGS-B', options={'maxiter': 60, 'ftol': 1e-4})\n    mean = res.x\n    \n    # ---------------------------------------------------------\n    # 2. Mini-CMA-ES Implementation\n    # ---------------------------------------------------------\n    \n    # Objective function\n    def get_fitness(ind):\n        pts = ind.reshape((N_POINTS, DIM))\n        d = pdist(pts)\n        if d.size == 0: return 0.0\n        mn, mx = np.min(d), np.max(d)\n        if mx < 1e-9: return 0.0\n        return (mn / mx)**2\n\n    # Strategy Parameters\n    sigma = 0.02          # Initial step size (small, assuming good init)\n    lam = 28              # Population size (sample count)\n    mu = lam // 2         # Selection size\n    \n    # Weights for recombination\n    weights = np.log(mu + 0.5) - np.log(np.arange(1, mu + 1))\n    weights /= np.sum(weights)\n    mueff = 1 / np.sum(weights**2)\n    \n    # Adaptation Constants\n    cc = (4 + mueff/N_VARS) / (N_VARS + 4 + 2 * mueff/N_VARS)\n    cs = (mueff + 2) / (N_VARS + mueff + 5)\n    c1 = 2 / ((N_VARS + 1.3)**2 + mueff)\n    cmu = min(1 - c1, 2 * (mueff - 2 + 1/mueff) / ((N_VARS + 2)**2 + mueff))\n    damps = 1 + 2 * max(0, np.sqrt((mueff - 1)/(N_VARS + 1)) - 1) + cs\n    \n    # Dynamic State Variables\n    pc = np.zeros(N_VARS)\n    ps = np.zeros(N_VARS)\n    B = np.eye(N_VARS)\n    D = np.ones(N_VARS)\n    C = np.eye(N_VARS)\n    \n    chiN = N_VARS**0.5 * (1 - 1/(4*N_VARS) + 1/(21*N_VARS**2))\n    \n    best_ind = mean\n    best_score = get_fitness(mean)\n    \n    start_time = time.time()\n    generation = 0\n    \n    # Main Loop (approx 60s budget)\n    while time.time() - start_time < 55:\n        generation += 1\n        \n        # ----------------------\n        # Sampling\n        # ----------------------\n        arz = np.random.randn(lam, N_VARS)\n        # arx = mean + sigma * (B @ (D * arz))\n        # Optimized: T = arz * D; Y = T @ B.T\n        T = arz * D\n        Y = np.dot(T, B.T)\n        arx = mean + sigma * Y\n        \n        # ----------------------\n        # Evaluation\n        # ----------------------\n        fitnesses = np.array([get_fitness(ind) for ind in arx])\n        \n        # ----------------------\n        # Selection & Update Best\n        # ----------------------\n        # Sort descending (maximize)\n        arindex = np.argsort(fitnesses)[::-1]\n        \n        if fitnesses[arindex[0]] > best_score:\n            best_score = fitnesses[arindex[0]]\n            best_ind = arx[arindex[0]].copy()\n            \n        # ----------------------\n        # Recombination\n        # ----------------------\n        xold = mean\n        # Weighted mean of top mu vectors\n        mean = np.dot(weights, arx[arindex[:mu]])\n        \n        # ----------------------\n        # Evolution Paths\n        # ----------------------\n        y_w = (mean - xold) / sigma\n        \n        # Calculate z_w = B @ D^-1 @ B.T @ y_w\n        # T2 = B.T @ y_w\n        # T3 = T2 / D\n        # z_w = B @ T3\n        z_w = np.dot(B, np.dot(B.T, y_w) / D)\n        \n        # Update evolution path ps\n        ps = (1 - cs) * ps + np.sqrt(cs * (2 - cs) * mueff) * z_w\n        \n        # Hsig check for stalls/explosions\n        hsig_val = np.linalg.norm(ps) / np.sqrt(1 - (1 - cs)**(2*generation)) / chiN\n        hsig = 1.0 if hsig_val < 1.4 + 2/(N_VARS + 1) else 0.0\n        \n        # Update evolution path pc\n        pc = (1 - cc) * pc + hsig * np.sqrt(cc * (2 - cc) * mueff) * y_w\n        \n        # ----------------------\n        # Covariance Matrix Adaptation\n        # ----------------------\n        # Rank-1 update\n        dh = (1 - hsig) * cc * (2 - cc)\n        C = (1 - c1 - cmu) * C + c1 * (np.outer(pc, pc) + dh * C)\n        \n        # Rank-mu update\n        # Reconstruct y vectors for selected population\n        y_k = (arx[arindex[:mu]] - xold) / sigma\n        for i in range(mu):\n            C += cmu * weights[i] * np.outer(y_k[i], y_k[i])\n            \n        # ----------------------\n        # Step Size Control\n        # ----------------------\n        sigma *= np.exp((cs / damps) * (np.linalg.norm(ps) / chiN - 1))\n        \n        # ----------------------\n        # Decomposition (Eigen)\n        # ----------------------\n        # Perform O(N^3) decomposition only periodically\n        if generation % (N_VARS // 10 + 2) == 0:\n            C = np.triu(C) + np.triu(C, 1).T  # Enforce symmetry\n            vals, vecs = np.linalg.eigh(C)\n            D = np.sqrt(np.maximum(vals, 1e-14))\n            B = vecs\n            \n        # ----------------------\n        # Maintenance\n        # ----------------------\n        # If sigma gets too small, we might be stuck. \n        # But for this problem, we just want to refine.\n        if sigma < 1e-5:\n            sigma = 0.02\n        \n        # Periodically re-center the mean to prevent drift in translation\n        # (Problem is translation invariant)\n        if generation % 50 == 0:\n            m_pts = mean.reshape((N_POINTS, DIM))\n            center = np.mean(m_pts, axis=0)\n            mean = (m_pts - center).flatten()\n\n    return best_ind.reshape((N_POINTS, DIM))",
  "metrics": {
    "valid": true,
    "score": 0.9396774503164101,
    "min_max_ratio": 0.22556681178474436,
    "min_distance": 0.9840614175582119,
    "max_distance": 2.0719754437845266,
    "eval_time": 55.37159299850464,
    "error": null
  },
  "prompt": "Implement a mini-CMA-ES (Covariance Matrix Adaptation Evolution Strategy) specifically for this problem. Use the configuration from trial_3_6 as the initial mean. Sample 20-30 candidates per 'generation' within the script, evaluate them, and update the mean. Return the best found.",
  "response": "```python\nimport numpy as np\nimport time\nfrom scipy.spatial.distance import pdist\nfrom scipy.optimize import minimize\n\ndef min_max_dist_dim3_14():\n    \"\"\"\n    Finds a configuration of 14 points in 3D that maximizes the ratio (min_dist / max_dist)^2.\n    Uses a mini-CMA-ES implementation for optimization.\n    \"\"\"\n    \n    # ---------------------------------------------------------\n    # 1. Initialization: Generating \"trial_3_6\" Mean\n    # ---------------------------------------------------------\n    # Since the specific 'trial_3_6' data is not persistent in this environment,\n    # we simulate it by running a quick pre-optimization (L-BFGS-B) on a repulsive \n    # potential to generate a high-quality starting configuration (mean).\n    \n    N_POINTS = 14\n    DIM = 3\n    N_VARS = N_POINTS * DIM\n    \n    np.random.seed(42)  # Ensure reproducibility of the starting point\n    \n    def initial_energy(x):\n        \"\"\"Soft repulsive potential to spread points roughly on a sphere.\"\"\"\n        pts = x.reshape((N_POINTS, DIM))\n        # Add soft constraint to keep points near unit sphere\n        radii = np.linalg.norm(pts, axis=1)\n        sphere_penalty = np.sum((radii - 1.0)**2)\n        \n        # Pairwise repulsion (Thomson-like)\n        dists = pdist(pts)\n        # Avoid div by zero\n        repulsion = np.sum(1.0 / (dists**2 + 1e-6))\n        \n        return repulsion + 500 * sphere_penalty\n\n    # Start from random and optimize briefly\n    x0 = np.random.randn(N_VARS)\n    res = minimize(initial_energy, x0, method='L-BFGS-B', options={'maxiter': 60, 'ftol': 1e-4})\n    mean = res.x\n    \n    # ---------------------------------------------------------\n    # 2. Mini-CMA-ES Implementation\n    # ---------------------------------------------------------\n    \n    # Objective function\n    def get_fitness(ind):\n        pts = ind.reshape((N_POINTS, DIM))\n        d = pdist(pts)\n        if d.size == 0: return 0.0\n        mn, mx = np.min(d), np.max(d)\n        if mx < 1e-9: return 0.0\n        return (mn / mx)**2\n\n    # Strategy Parameters\n    sigma = 0.02          # Initial step size (small, assuming good init)\n    lam = 28              # Population size (sample count)\n    mu = lam // 2         # Selection size\n    \n    # Weights for recombination\n    weights = np.log(mu + 0.5) - np.log(np.arange(1, mu + 1))\n    weights /= np.sum(weights)\n    mueff = 1 / np.sum(weights**2)\n    \n    # Adaptation Constants\n    cc = (4 + mueff/N_VARS) / (N_VARS + 4 + 2 * mueff/N_VARS)\n    cs = (mueff + 2) / (N_VARS + mueff + 5)\n    c1 = 2 / ((N_VARS + 1.3)**2 + mueff)\n    cmu = min(1 - c1, 2 * (mueff - 2 + 1/mueff) / ((N_VARS + 2)**2 + mueff))\n    damps = 1 + 2 * max(0, np.sqrt((mueff - 1)/(N_VARS + 1)) - 1) + cs\n    \n    # Dynamic State Variables\n    pc = np.zeros(N_VARS)\n    ps = np.zeros(N_VARS)\n    B = np.eye(N_VARS)\n    D = np.ones(N_VARS)\n    C = np.eye(N_VARS)\n    \n    chiN = N_VARS**0.5 * (1 - 1/(4*N_VARS) + 1/(21*N_VARS**2))\n    \n    best_ind = mean\n    best_score = get_fitness(mean)\n    \n    start_time = time.time()\n    generation = 0\n    \n    # Main Loop (approx 60s budget)\n    while time.time() - start_time < 55:\n        generation += 1\n        \n        # ----------------------\n        # Sampling\n        # ----------------------\n        arz = np.random.randn(lam, N_VARS)\n        # arx = mean + sigma * (B @ (D * arz))\n        # Optimized: T = arz * D; Y = T @ B.T\n        T = arz * D\n        Y = np.dot(T, B.T)\n        arx = mean + sigma * Y\n        \n        # ----------------------\n        # Evaluation\n        # ----------------------\n        fitnesses = np.array([get_fitness(ind) for ind in arx])\n        \n        # ----------------------\n        # Selection & Update Best\n        # ----------------------\n        # Sort descending (maximize)\n        arindex = np.argsort(fitnesses)[::-1]\n        \n        if fitnesses[arindex[0]] > best_score:\n            best_score = fitnesses[arindex[0]]\n            best_ind = arx[arindex[0]].copy()\n            \n        # ----------------------\n        # Recombination\n        # ----------------------\n        xold = mean\n        # Weighted mean of top mu vectors\n        mean = np.dot(weights, arx[arindex[:mu]])\n        \n        # ----------------------\n        # Evolution Paths\n        # ----------------------\n        y_w = (mean - xold) / sigma\n        \n        # Calculate z_w = B @ D^-1 @ B.T @ y_w\n        # T2 = B.T @ y_w\n        # T3 = T2 / D\n        # z_w = B @ T3\n        z_w = np.dot(B, np.dot(B.T, y_w) / D)\n        \n        # Update evolution path ps\n        ps = (1 - cs) * ps + np.sqrt(cs * (2 - cs) * mueff) * z_w\n        \n        # Hsig check for stalls/explosions\n        hsig_val = np.linalg.norm(ps) / np.sqrt(1 - (1 - cs)**(2*generation)) / chiN\n        hsig = 1.0 if hsig_val < 1.4 + 2/(N_VARS + 1) else 0.0\n        \n        # Update evolution path pc\n        pc = (1 - cc) * pc + hsig * np.sqrt(cc * (2 - cc) * mueff) * y_w\n        \n        # ----------------------\n        # Covariance Matrix Adaptation\n        # ----------------------\n        # Rank-1 update\n        dh = (1 - hsig) * cc * (2 - cc)\n        C = (1 - c1 - cmu) * C + c1 * (np.outer(pc, pc) + dh * C)\n        \n        # Rank-mu update\n        # Reconstruct y vectors for selected population\n        y_k = (arx[arindex[:mu]] - xold) / sigma\n        for i in range(mu):\n            C += cmu * weights[i] * np.outer(y_k[i], y_k[i])\n            \n        # ----------------------\n        # Step Size Control\n        # ----------------------\n        sigma *= np.exp((cs / damps) * (np.linalg.norm(ps) / chiN - 1))\n        \n        # ----------------------\n        # Decomposition (Eigen)\n        # ----------------------\n        # Perform O(N^3) decomposition only periodically\n        if generation % (N_VARS // 10 + 2) == 0:\n            C = np.triu(C) + np.triu(C, 1).T  # Enforce symmetry\n            vals, vecs = np.linalg.eigh(C)\n            D = np.sqrt(np.maximum(vals, 1e-14))\n            B = vecs\n            \n        # ----------------------\n        # Maintenance\n        # ----------------------\n        # If sigma gets too small, we might be stuck. \n        # But for this problem, we just want to refine.\n        if sigma < 1e-5:\n            sigma = 0.02\n        \n        # Periodically re-center the mean to prevent drift in translation\n        # (Problem is translation invariant)\n        if generation % 50 == 0:\n            m_pts = mean.reshape((N_POINTS, DIM))\n            center = np.mean(m_pts, axis=0)\n            mean = (m_pts - center).flatten()\n\n    return best_ind.reshape((N_POINTS, DIM))\n```",
  "reasoning": "",
  "timestamp": "2026-01-21T17:33:46.750433",
  "cost_data": null,
  "model_config": {
    "model": "gemini-3-pro-preview",
    "temperature": 0.7,
    "provider_debug": {
      "finish_reason": "STOP",
      "usage_metadata": {
        "prompt_token_count": 236,
        "candidates_token_count": 2154,
        "thoughts_token_count": 9417
      },
      "response_text_len": 6572
    }
  }
}