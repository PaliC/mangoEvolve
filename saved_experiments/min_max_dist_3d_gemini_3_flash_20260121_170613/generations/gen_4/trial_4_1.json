{
  "trial_id": "trial_4_1",
  "generation": 4,
  "parent_id": "trial_3_6",
  "code": "import numpy as np\nfrom scipy.optimize import minimize\nimport time\n\ndef min_max_dist_dim3_14():\n    \"\"\"\n    Returns 14 points in 3D maximizing (min_dist / max_dist)^2.\n    \n    Strategy:\n    1. Basin Hopping with Adaptive Step Size:\n       - Uses a high-power repulsion potential (Riesz energy, p=30) as a surrogate objective.\n       - Points are softly constrained to the unit sphere using a penalty method.\n       - Step size and temperature decay over iterations to settle into a deep basin.\n    2. Polishing:\n       - Takes the best spherical configuration found.\n       - Uses SLSQP to solve the exact constrained optimization problem:\n         Maximize t subject to dist_ij^2 >= t * D^2 and dist_ij^2 <= D^2.\n    \"\"\"\n    N = 14\n    \n    # --- Helper Functions ---\n    def get_dists_sq_matrix(X):\n        # Efficient pairwise squared distance matrix\n        sq_norms = np.sum(X**2, axis=1)\n        d2 = sq_norms[:, None] + sq_norms[None, :] - 2 * np.dot(X, X.T)\n        d2[d2 < 0] = 0\n        np.fill_diagonal(d2, 0)\n        return d2\n\n    def get_score(X):\n        d2 = get_dists_sq_matrix(X)\n        mask = np.ones(d2.shape, dtype=bool)\n        np.fill_diagonal(mask, 0)\n        valid_d2 = d2[mask]\n        if len(valid_d2) == 0: return 0.0\n        d_min_sq = np.min(valid_d2)\n        d_max_sq = np.max(valid_d2)\n        if d_max_sq < 1e-9: return 0.0\n        return d_min_sq / d_max_sq\n\n    # --- Phase 1 Setup: Surrogate Energy (Riesz Potential) ---\n    p = 30\n    w_penalty = 1000.0  # Weight for sphere constraint penalty\n    iu = np.triu_indices(N, k=1) # Upper triangle indices for pairwise operations\n    \n    def energy_func(flat_x):\n        X = flat_x.reshape((N, 3))\n        sq_norms = np.sum(X**2, axis=1)\n        d2 = sq_norms[:, None] + sq_norms[None, :] - 2 * np.dot(X, X.T)\n        d2 = np.maximum(d2, 1e-6) # Avoid singularity\n        \n        # Riesz energy: sum (d^2)^(-p/2)\n        d2_vals = d2[iu]\n        e_repulsion = np.sum(d2_vals ** (-p / 2.0))\n        \n        # Sphere penalty: sum (|x|^2 - 1)^2\n        e_penalty = w_penalty * np.sum((sq_norms - 1.0)**2)\n        \n        return e_repulsion + e_penalty\n\n    def energy_grad(flat_x):\n        X = flat_x.reshape((N, 3))\n        sq_norms = np.sum(X**2, axis=1)\n        d2 = sq_norms[:, None] + sq_norms[None, :] - 2 * np.dot(X, X.T)\n        d2 = np.maximum(d2, 1e-6)\n        \n        # Gradient of Repulsion\n        alpha = p / 2.0 + 1.0\n        factors = d2 ** (-alpha)\n        np.fill_diagonal(factors, 0.0)\n        \n        # Vectorized gradient calculation\n        sum_factors = np.sum(factors, axis=1)[:, None]\n        grad_rep = -p * (X * sum_factors - np.dot(factors, X))\n        \n        # Gradient of Penalty\n        grad_pen = 4 * w_penalty * (sq_norms - 1.0)[:, None] * X\n        \n        return (grad_rep + grad_pen).flatten()\n\n    # --- Phase 1: Basin Hopping ---\n    start_time = time.time()\n    \n    # Initialization\n    current_x = np.random.randn(N, 3)\n    current_x /= np.linalg.norm(current_x, axis=1, keepdims=True)\n    \n    # Initial local minimization\n    res = minimize(energy_func, current_x.flatten(), jac=energy_grad, method='L-BFGS-B', options={'maxiter': 50})\n    current_x = res.x.reshape(N, 3)\n    current_val = res.fun\n    \n    best_x = current_x.copy()\n    best_val = current_val\n    \n    # Adaptive Schedule\n    T_max = 0.5\n    T_min = 0.001\n    step_max = 0.3\n    step_min = 0.02\n    n_iters = 100\n    \n    for i in range(n_iters):\n        # Time budget check (save 10s for polishing)\n        if time.time() - start_time > 45:\n            break\n            \n        progress = i / n_iters\n        T = T_max * (T_min / T_max)**progress\n        step = step_max * (step_min / step_max)**progress\n        \n        # Perturbation\n        perturbation = np.random.randn(N, 3) * step\n        trial_x = current_x + perturbation\n        # Normalize to sphere to keep it in the valid domain for the penalty method\n        trial_x /= np.linalg.norm(trial_x, axis=1, keepdims=True)\n        \n        # Local Minimization\n        res = minimize(energy_func, trial_x.flatten(), jac=energy_grad, method='L-BFGS-B', \n                       options={'maxiter': 40, 'ftol': 1e-4})\n        trial_x = res.x.reshape(N, 3)\n        trial_val = res.fun\n        \n        # Metropolis Acceptance\n        delta = trial_val - current_val\n        if delta < 0 or np.random.rand() < np.exp(-delta / T):\n            current_x = trial_x\n            current_val = trial_val\n            if current_val < best_val:\n                best_val = current_val\n                best_x = current_x.copy()\n\n    # Normalize best result to sphere exactly\n    best_x /= np.linalg.norm(best_x, axis=1, keepdims=True)\n    \n    # --- Phase 2: Polish with Exact Constraints ---\n    # Maximize t subject to:\n    # 1. dist_ij^2 >= t\n    # 2. dist_ij^2 <= 1\n    # Objective: Minimize -t\n    \n    # Scale best_x so max dist is 1 to start feasible\n    d2_mat = get_dists_sq_matrix(best_x)\n    max_d2 = np.max(d2_mat)\n    polish_x = best_x / np.sqrt(max_d2)\n    \n    # Initial t estimate\n    d2_vals = d2_mat[iu] / max_d2\n    t0 = np.min(d2_vals) * 0.99 \n    \n    x0 = np.concatenate(([t0], polish_x.flatten()))\n    \n    def obj_polish(v): return -v[0]\n    def grad_obj_polish(v):\n        g = np.zeros_like(v)\n        g[0] = -1.0\n        return g\n    \n    # Constraint functions and Jacobian\n    def cons_polish(v):\n        t = v[0]\n        X = v[1:].reshape(N, 3)\n        d2 = np.sum(X**2, axis=1)[:, None] + np.sum(X**2, axis=1)[None, :] - 2 * np.dot(X, X.T)\n        dists = d2[iu]\n        return np.concatenate([dists - t, 1.0 - dists])\n        \n    def jac_polish(v):\n        X = v[1:].reshape(N, 3)\n        n_pairs = len(iu[0])\n        jac = np.zeros((2 * n_pairs, 1 + 3*N))\n        \n        # Derivatives w.r.t t\n        jac[:n_pairs, 0] = -1.0\n        \n        # Derivatives w.r.t X\n        # For each pair k corresponding to (i, j)\n        for k in range(n_pairs):\n            i, j = iu[0][k], iu[1][k]\n            diff = 2 * (X[i] - X[j])\n            \n            # Constraint 1: dists - t >= 0\n            jac[k, 1 + 3*i : 1 + 3*i + 3] = diff\n            jac[k, 1 + 3*j : 1 + 3*j + 3] = -diff\n            \n            # Constraint 2: 1 - dists >= 0\n            # d(1-d^2)/dx = -d(d^2)/dx\n            jac[n_pairs + k, 1 + 3*i : 1 + 3*i + 3] = -diff\n            jac[n_pairs + k, 1 + 3*j : 1 + 3*j + 3] = diff\n            \n        return jac\n        \n    cons = {'type': 'ineq', 'fun': cons_polish, 'jac': jac_polish}\n    bounds = [(0.0, 1.0)] + [(None, None)] * (3*N)\n    \n    # Run exact optimization\n    res_polish = minimize(obj_polish, x0, method='SLSQP', jac=grad_obj_polish,\n                          constraints=cons, bounds=bounds,\n                          options={'maxiter': 300, 'ftol': 1e-7})\n                          \n    final_polish_x = res_polish.x[1:].reshape(N, 3)\n    \n    # Return the better of the two results (Basin vs Polished)\n    score_polish = get_score(final_polish_x)\n    score_basin = get_score(best_x)\n    \n    if score_polish > score_basin:\n        return final_polish_x\n    else:\n        return best_x",
  "metrics": {
    "valid": true,
    "score": 0.936692421950167,
    "min_max_ratio": 0.2248502644935076,
    "min_distance": 0.4741837877016254,
    "max_distance": 1.0000000000568212,
    "eval_time": 0.3577730655670166,
    "error": null
  },
  "prompt": "Improve upon the basin-hopping approach in trial_3_6. Implement an adaptive step size that decreases as the temperature cools. Use a high-power repulsion potential (e.g., 1/r^p with p=30) as a surrogate objective function during the hopping phase, then polish the best result with the exact min-max ratio objective.",
  "response": "```python\nimport numpy as np\nfrom scipy.optimize import minimize\nimport time\n\ndef min_max_dist_dim3_14():\n    \"\"\"\n    Returns 14 points in 3D maximizing (min_dist / max_dist)^2.\n    \n    Strategy:\n    1. Basin Hopping with Adaptive Step Size:\n       - Uses a high-power repulsion potential (Riesz energy, p=30) as a surrogate objective.\n       - Points are softly constrained to the unit sphere using a penalty method.\n       - Step size and temperature decay over iterations to settle into a deep basin.\n    2. Polishing:\n       - Takes the best spherical configuration found.\n       - Uses SLSQP to solve the exact constrained optimization problem:\n         Maximize t subject to dist_ij^2 >= t * D^2 and dist_ij^2 <= D^2.\n    \"\"\"\n    N = 14\n    \n    # --- Helper Functions ---\n    def get_dists_sq_matrix(X):\n        # Efficient pairwise squared distance matrix\n        sq_norms = np.sum(X**2, axis=1)\n        d2 = sq_norms[:, None] + sq_norms[None, :] - 2 * np.dot(X, X.T)\n        d2[d2 < 0] = 0\n        np.fill_diagonal(d2, 0)\n        return d2\n\n    def get_score(X):\n        d2 = get_dists_sq_matrix(X)\n        mask = np.ones(d2.shape, dtype=bool)\n        np.fill_diagonal(mask, 0)\n        valid_d2 = d2[mask]\n        if len(valid_d2) == 0: return 0.0\n        d_min_sq = np.min(valid_d2)\n        d_max_sq = np.max(valid_d2)\n        if d_max_sq < 1e-9: return 0.0\n        return d_min_sq / d_max_sq\n\n    # --- Phase 1 Setup: Surrogate Energy (Riesz Potential) ---\n    p = 30\n    w_penalty = 1000.0  # Weight for sphere constraint penalty\n    iu = np.triu_indices(N, k=1) # Upper triangle indices for pairwise operations\n    \n    def energy_func(flat_x):\n        X = flat_x.reshape((N, 3))\n        sq_norms = np.sum(X**2, axis=1)\n        d2 = sq_norms[:, None] + sq_norms[None, :] - 2 * np.dot(X, X.T)\n        d2 = np.maximum(d2, 1e-6) # Avoid singularity\n        \n        # Riesz energy: sum (d^2)^(-p/2)\n        d2_vals = d2[iu]\n        e_repulsion = np.sum(d2_vals ** (-p / 2.0))\n        \n        # Sphere penalty: sum (|x|^2 - 1)^2\n        e_penalty = w_penalty * np.sum((sq_norms - 1.0)**2)\n        \n        return e_repulsion + e_penalty\n\n    def energy_grad(flat_x):\n        X = flat_x.reshape((N, 3))\n        sq_norms = np.sum(X**2, axis=1)\n        d2 = sq_norms[:, None] + sq_norms[None, :] - 2 * np.dot(X, X.T)\n        d2 = np.maximum(d2, 1e-6)\n        \n        # Gradient of Repulsion\n        alpha = p / 2.0 + 1.0\n        factors = d2 ** (-alpha)\n        np.fill_diagonal(factors, 0.0)\n        \n        # Vectorized gradient calculation\n        sum_factors = np.sum(factors, axis=1)[:, None]\n        grad_rep = -p * (X * sum_factors - np.dot(factors, X))\n        \n        # Gradient of Penalty\n        grad_pen = 4 * w_penalty * (sq_norms - 1.0)[:, None] * X\n        \n        return (grad_rep + grad_pen).flatten()\n\n    # --- Phase 1: Basin Hopping ---\n    start_time = time.time()\n    \n    # Initialization\n    current_x = np.random.randn(N, 3)\n    current_x /= np.linalg.norm(current_x, axis=1, keepdims=True)\n    \n    # Initial local minimization\n    res = minimize(energy_func, current_x.flatten(), jac=energy_grad, method='L-BFGS-B', options={'maxiter': 50})\n    current_x = res.x.reshape(N, 3)\n    current_val = res.fun\n    \n    best_x = current_x.copy()\n    best_val = current_val\n    \n    # Adaptive Schedule\n    T_max = 0.5\n    T_min = 0.001\n    step_max = 0.3\n    step_min = 0.02\n    n_iters = 100\n    \n    for i in range(n_iters):\n        # Time budget check (save 10s for polishing)\n        if time.time() - start_time > 45:\n            break\n            \n        progress = i / n_iters\n        T = T_max * (T_min / T_max)**progress\n        step = step_max * (step_min / step_max)**progress\n        \n        # Perturbation\n        perturbation = np.random.randn(N, 3) * step\n        trial_x = current_x + perturbation\n        # Normalize to sphere to keep it in the valid domain for the penalty method\n        trial_x /= np.linalg.norm(trial_x, axis=1, keepdims=True)\n        \n        # Local Minimization\n        res = minimize(energy_func, trial_x.flatten(), jac=energy_grad, method='L-BFGS-B', \n                       options={'maxiter': 40, 'ftol': 1e-4})\n        trial_x = res.x.reshape(N, 3)\n        trial_val = res.fun\n        \n        # Metropolis Acceptance\n        delta = trial_val - current_val\n        if delta < 0 or np.random.rand() < np.exp(-delta / T):\n            current_x = trial_x\n            current_val = trial_val\n            if current_val < best_val:\n                best_val = current_val\n                best_x = current_x.copy()\n\n    # Normalize best result to sphere exactly\n    best_x /= np.linalg.norm(best_x, axis=1, keepdims=True)\n    \n    # --- Phase 2: Polish with Exact Constraints ---\n    # Maximize t subject to:\n    # 1. dist_ij^2 >= t\n    # 2. dist_ij^2 <= 1\n    # Objective: Minimize -t\n    \n    # Scale best_x so max dist is 1 to start feasible\n    d2_mat = get_dists_sq_matrix(best_x)\n    max_d2 = np.max(d2_mat)\n    polish_x = best_x / np.sqrt(max_d2)\n    \n    # Initial t estimate\n    d2_vals = d2_mat[iu] / max_d2\n    t0 = np.min(d2_vals) * 0.99 \n    \n    x0 = np.concatenate(([t0], polish_x.flatten()))\n    \n    def obj_polish(v): return -v[0]\n    def grad_obj_polish(v):\n        g = np.zeros_like(v)\n        g[0] = -1.0\n        return g\n    \n    # Constraint functions and Jacobian\n    def cons_polish(v):\n        t = v[0]\n        X = v[1:].reshape(N, 3)\n        d2 = np.sum(X**2, axis=1)[:, None] + np.sum(X**2, axis=1)[None, :] - 2 * np.dot(X, X.T)\n        dists = d2[iu]\n        return np.concatenate([dists - t, 1.0 - dists])\n        \n    def jac_polish(v):\n        X = v[1:].reshape(N, 3)\n        n_pairs = len(iu[0])\n        jac = np.zeros((2 * n_pairs, 1 + 3*N))\n        \n        # Derivatives w.r.t t\n        jac[:n_pairs, 0] = -1.0\n        \n        # Derivatives w.r.t X\n        # For each pair k corresponding to (i, j)\n        for k in range(n_pairs):\n            i, j = iu[0][k], iu[1][k]\n            diff = 2 * (X[i] - X[j])\n            \n            # Constraint 1: dists - t >= 0\n            jac[k, 1 + 3*i : 1 + 3*i + 3] = diff\n            jac[k, 1 + 3*j : 1 + 3*j + 3] = -diff\n            \n            # Constraint 2: 1 - dists >= 0\n            # d(1-d^2)/dx = -d(d^2)/dx\n            jac[n_pairs + k, 1 + 3*i : 1 + 3*i + 3] = -diff\n            jac[n_pairs + k, 1 + 3*j : 1 + 3*j + 3] = diff\n            \n        return jac\n        \n    cons = {'type': 'ineq', 'fun': cons_polish, 'jac': jac_polish}\n    bounds = [(0.0, 1.0)] + [(None, None)] * (3*N)\n    \n    # Run exact optimization\n    res_polish = minimize(obj_polish, x0, method='SLSQP', jac=grad_obj_polish,\n                          constraints=cons, bounds=bounds,\n                          options={'maxiter': 300, 'ftol': 1e-7})\n                          \n    final_polish_x = res_polish.x[1:].reshape(N, 3)\n    \n    # Return the better of the two results (Basin vs Polished)\n    score_polish = get_score(final_polish_x)\n    score_basin = get_score(best_x)\n    \n    if score_polish > score_basin:\n        return final_polish_x\n    else:\n        return best_x\n```",
  "reasoning": "",
  "timestamp": "2026-01-21T17:31:46.730610",
  "cost_data": null,
  "model_config": {
    "model": "gemini-3-pro-preview",
    "temperature": 0.7,
    "provider_debug": {
      "finish_reason": "STOP",
      "usage_metadata": {
        "prompt_token_count": 246,
        "candidates_token_count": 2550,
        "thoughts_token_count": 12147
      },
      "response_text_len": 7114
    }
  }
}