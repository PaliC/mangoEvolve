{
  "trial_id": "trial_4_5",
  "generation": 4,
  "parent_id": null,
  "code": "import numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.spatial.distance import pdist\nimport time\n\ndef objective_and_gradient(x_flat, p):\n    \"\"\"\n    Objective function and gradient for maximizing the ratio min_dist / max_dist.\n    We minimize soft_max / soft_min using the LogSumExp approximation.\n    \"\"\"\n    x = x_flat.reshape((14, 3))\n    \n    # Calculate pairwise differences and distances\n    diffs = x[:, np.newaxis, :] - x[np.newaxis, :, :]  # Shape (14, 14, 3)\n    dists_sq = np.sum(diffs**2, axis=2)\n    dists = np.sqrt(dists_sq + 1e-12)  # Shape (14, 14)\n    \n    # Extract upper triangle distances\n    iu = np.triu_indices(14, k=1)\n    d = dists[iu]\n    \n    # Numerical stability: use max_d and min_d in LogSumExp\n    max_d = np.max(d)\n    min_d = np.min(d)\n    \n    # SoftMax for maximum distance\n    exp_p = np.exp(p * (d - max_d))\n    sum_exp_p = np.sum(exp_p)\n    soft_max = max_d + (1.0 / p) * np.log(sum_exp_p)\n    \n    # SoftMin for minimum distance\n    exp_neg_p = np.exp(-p * (d - min_d))\n    sum_exp_neg_p = np.sum(exp_neg_p)\n    soft_min = min_d - (1.0 / p) * np.log(sum_exp_neg_p)\n    \n    # Objective: Minimize the ratio soft_max / soft_min\n    f = soft_max / soft_min\n    \n    # Gradient of SoftMax and SoftMin\n    w_max = exp_p / sum_exp_p\n    w_min = exp_neg_p / sum_exp_neg_p\n    \n    W_max = np.zeros((14, 14))\n    W_max[iu] = w_max\n    W_max = W_max + W_max.T\n    \n    W_min = np.zeros((14, 14))\n    W_min[iu] = w_min\n    W_min = W_min + W_min.T\n    \n    # Inverse distances for gradient (ignoring diagonal)\n    dist_inv = 1.0 / dists\n    np.fill_diagonal(dist_inv, 0)\n    \n    # d_soft_max / dx_i = sum_j W_max_ij * (x_i - x_j) / d_ij\n    grad_soft_max = np.sum(W_max[:, :, np.newaxis] * diffs * dist_inv[:, :, np.newaxis], axis=1)\n    grad_soft_min = np.sum(W_min[:, :, np.newaxis] * diffs * dist_inv[:, :, np.newaxis], axis=1)\n    \n    # Quotient rule for gradient of f = soft_max / soft_min\n    grad_f = (soft_min * grad_soft_max - soft_max * grad_soft_min) / (soft_min**2)\n    \n    # Penalty to keep the scale near 1.0 and prevent drift\n    # The objective is scale-invariant, but optimizers prefer bounded landscapes.\n    scale_penalty = 1e-3 * (np.sum(x**2) - 14)**2\n    grad_scale_penalty = 1e-3 * 2 * (np.sum(x**2) - 14) * 2 * x\n    \n    return f + scale_penalty, (grad_f + grad_scale_penalty).flatten()\n\ndef get_actual_ratio(x_flat):\n    x = x_flat.reshape((14, 3))\n    dists = pdist(x)\n    return np.min(dists) / np.max(dists)\n\ndef min_max_dist_dim3_14():\n    start_time = time.time()\n    \n    # Stage progression for p as requested\n    p_stages = [10, 30, 100, 250, 500]\n    \n    best_x = None\n    best_ratio = -1.0\n    \n    # Try different initializations to find global optimum\n    for i in range(10):\n        # Time check to respect the 60s limit\n        if time.time() - start_time > 50:\n            break\n            \n        # Initialization strategies\n        if i == 0:\n            # Tetrakis hexahedron (cube + face centers)\n            v1 = np.array([[1,1,1], [1,1,-1], [1,-1,1], [1,-1,-1],\n                           [-1,1,1], [-1,1,-1], [-1,-1,1], [-1,-1,-1]], dtype=float)\n            v2 = np.array([[1.5,0,0], [-1.5,0,0], [0,1.5,0], [0,-1.5,0], [0,0,1.5], [0,0,-1.5]], dtype=float)\n            x = np.vstack([v1, v2]).flatten()\n        elif i == 1:\n            # Points on a sphere\n            x = np.random.randn(14, 3)\n            x /= np.linalg.norm(x, axis=1)[:, np.newaxis]\n            x = x.flatten()\n        else:\n            # Random uniform in a cube\n            x = np.random.uniform(-1, 1, (14, 3)).flatten()\n            \n        # Optimization stages\n        current_x = x\n        for p_val in p_stages:\n            res = minimize(objective_and_gradient, current_x, args=(p_val,), \n                           jac=True, method='L-BFGS-B', \n                           options={'maxiter': 100, 'ftol': 1e-7})\n            current_x = res.x\n            \n        # Evaluate actual ratio for the best found in this trajectory\n        ratio = get_actual_ratio(current_x)\n        if ratio > best_ratio:\n            best_ratio = ratio\n            best_x = current_x\n            \n    # Return the best configuration found\n    return best_x.reshape((14, 3))",
  "metrics": {
    "valid": true,
    "score": 0.9977426558199168,
    "min_max_ratio": 0.23950519380789684,
    "min_distance": 1.0062364418362588,
    "max_distance": 2.0560921443278177,
    "eval_time": 0.39050912857055664,
    "error": null
  },
  "prompt": "Use the LogSumExp approximation for the minimum distance: -1/p * log(sum(exp(-p * d_ij))). Start with p=10 and gradually increase p to 500 across multiple optimization stages. Use the output of one stage as the input for the next. This helps smooth the landscape before tackling the non-differentiable min function.",
  "response": "```python\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.spatial.distance import pdist\nimport time\n\ndef objective_and_gradient(x_flat, p):\n    \"\"\"\n    Objective function and gradient for maximizing the ratio min_dist / max_dist.\n    We minimize soft_max / soft_min using the LogSumExp approximation.\n    \"\"\"\n    x = x_flat.reshape((14, 3))\n    \n    # Calculate pairwise differences and distances\n    diffs = x[:, np.newaxis, :] - x[np.newaxis, :, :]  # Shape (14, 14, 3)\n    dists_sq = np.sum(diffs**2, axis=2)\n    dists = np.sqrt(dists_sq + 1e-12)  # Shape (14, 14)\n    \n    # Extract upper triangle distances\n    iu = np.triu_indices(14, k=1)\n    d = dists[iu]\n    \n    # Numerical stability: use max_d and min_d in LogSumExp\n    max_d = np.max(d)\n    min_d = np.min(d)\n    \n    # SoftMax for maximum distance\n    exp_p = np.exp(p * (d - max_d))\n    sum_exp_p = np.sum(exp_p)\n    soft_max = max_d + (1.0 / p) * np.log(sum_exp_p)\n    \n    # SoftMin for minimum distance\n    exp_neg_p = np.exp(-p * (d - min_d))\n    sum_exp_neg_p = np.sum(exp_neg_p)\n    soft_min = min_d - (1.0 / p) * np.log(sum_exp_neg_p)\n    \n    # Objective: Minimize the ratio soft_max / soft_min\n    f = soft_max / soft_min\n    \n    # Gradient of SoftMax and SoftMin\n    w_max = exp_p / sum_exp_p\n    w_min = exp_neg_p / sum_exp_neg_p\n    \n    W_max = np.zeros((14, 14))\n    W_max[iu] = w_max\n    W_max = W_max + W_max.T\n    \n    W_min = np.zeros((14, 14))\n    W_min[iu] = w_min\n    W_min = W_min + W_min.T\n    \n    # Inverse distances for gradient (ignoring diagonal)\n    dist_inv = 1.0 / dists\n    np.fill_diagonal(dist_inv, 0)\n    \n    # d_soft_max / dx_i = sum_j W_max_ij * (x_i - x_j) / d_ij\n    grad_soft_max = np.sum(W_max[:, :, np.newaxis] * diffs * dist_inv[:, :, np.newaxis], axis=1)\n    grad_soft_min = np.sum(W_min[:, :, np.newaxis] * diffs * dist_inv[:, :, np.newaxis], axis=1)\n    \n    # Quotient rule for gradient of f = soft_max / soft_min\n    grad_f = (soft_min * grad_soft_max - soft_max * grad_soft_min) / (soft_min**2)\n    \n    # Penalty to keep the scale near 1.0 and prevent drift\n    # The objective is scale-invariant, but optimizers prefer bounded landscapes.\n    scale_penalty = 1e-3 * (np.sum(x**2) - 14)**2\n    grad_scale_penalty = 1e-3 * 2 * (np.sum(x**2) - 14) * 2 * x\n    \n    return f + scale_penalty, (grad_f + grad_scale_penalty).flatten()\n\ndef get_actual_ratio(x_flat):\n    x = x_flat.reshape((14, 3))\n    dists = pdist(x)\n    return np.min(dists) / np.max(dists)\n\ndef min_max_dist_dim3_14():\n    start_time = time.time()\n    \n    # Stage progression for p as requested\n    p_stages = [10, 30, 100, 250, 500]\n    \n    best_x = None\n    best_ratio = -1.0\n    \n    # Try different initializations to find global optimum\n    for i in range(10):\n        # Time check to respect the 60s limit\n        if time.time() - start_time > 50:\n            break\n            \n        # Initialization strategies\n        if i == 0:\n            # Tetrakis hexahedron (cube + face centers)\n            v1 = np.array([[1,1,1], [1,1,-1], [1,-1,1], [1,-1,-1],\n                           [-1,1,1], [-1,1,-1], [-1,-1,1], [-1,-1,-1]], dtype=float)\n            v2 = np.array([[1.5,0,0], [-1.5,0,0], [0,1.5,0], [0,-1.5,0], [0,0,1.5], [0,0,-1.5]], dtype=float)\n            x = np.vstack([v1, v2]).flatten()\n        elif i == 1:\n            # Points on a sphere\n            x = np.random.randn(14, 3)\n            x /= np.linalg.norm(x, axis=1)[:, np.newaxis]\n            x = x.flatten()\n        else:\n            # Random uniform in a cube\n            x = np.random.uniform(-1, 1, (14, 3)).flatten()\n            \n        # Optimization stages\n        current_x = x\n        for p_val in p_stages:\n            res = minimize(objective_and_gradient, current_x, args=(p_val,), \n                           jac=True, method='L-BFGS-B', \n                           options={'maxiter': 100, 'ftol': 1e-7})\n            current_x = res.x\n            \n        # Evaluate actual ratio for the best found in this trajectory\n        ratio = get_actual_ratio(current_x)\n        if ratio > best_ratio:\n            best_ratio = ratio\n            best_x = current_x\n            \n    # Return the best configuration found\n    return best_x.reshape((14, 3))\n\n```",
  "reasoning": "",
  "timestamp": "2026-01-21T17:31:52.410859",
  "cost_data": null,
  "model_config": {
    "model": "gemini-3-flash-preview",
    "temperature": 0.7,
    "provider_debug": {
      "finish_reason": "STOP",
      "usage_metadata": {
        "prompt_token_count": 249,
        "candidates_token_count": 1580,
        "thoughts_token_count": 20533
      },
      "response_text_len": 4240
    }
  }
}