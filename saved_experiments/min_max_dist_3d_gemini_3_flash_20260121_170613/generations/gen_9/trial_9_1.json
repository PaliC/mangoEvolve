{
  "trial_id": "trial_9_1",
  "generation": 9,
  "parent_id": "trial_8_0",
  "code": "import numpy as np\nfrom scipy.optimize import minimize\nimport time\n\ndef min_max_dist_dim3_14():\n    \"\"\"\n    Finds coordinates for 14 points in 3D that maximize the squared ratio \n    (min_dist / max_dist)^2.\n    \n    Strategy:\n    1. Heuristic Initialization: Try random clusters and Center+Shell configurations.\n       Optimize with a repulsive potential to find a good basin.\n    2. Formulation: Maximize s s.t. d_ij^2 >= s and d_ij^2 <= 1.\n    3. Refinement: Micro-Basin-Hopping with high-precision SLSQP.\n    \"\"\"\n    \n    N = 14\n    dim = 3\n    idx = np.triu_indices(N, k=1)\n    n_pairs = len(idx[0])\n    n_vars = N * dim + 1\n    \n    # -------------------------------------------------------------------------\n    # 1. Jacobian and Constraint Definitions\n    # -------------------------------------------------------------------------\n    \n    def unpack(params):\n        # x: (N, 3), s: scalar\n        return params[:-1].reshape((N, dim)), params[-1]\n\n    # Objective: Maximize s => Minimize -s\n    def obj(params):\n        return -params[-1]\n\n    def obj_jac(params):\n        grad = np.zeros(len(params))\n        grad[-1] = -1.0\n        return grad\n\n    # Constraints:\n    # 1. d^2 >= s  =>  d^2 - s >= 0\n    # 2. d^2 <= 1  =>  1 - d^2 >= 0\n    def constraints(params):\n        x, s = unpack(params)\n        diff = x[idx[0]] - x[idx[1]]\n        d2 = np.sum(diff**2, axis=1)\n        return np.concatenate([d2 - s, 1.0 - d2])\n\n    def constraints_jac(params):\n        x, s = unpack(params)\n        diff = x[idx[0]] - x[idx[1]] # Shape (91, 3)\n        \n        # Sparse-like Jacobian construction\n        J = np.zeros((2 * n_pairs, n_vars))\n        \n        # Indices for constructing the matrix\n        rows = np.arange(n_pairs)\n        \n        # Gradients for d^2 - s >= 0\n        # d(d^2)/dx_i = 2*(xi - xj)\n        # d(d^2)/dx_j = -2*(xi - xj)\n        \n        two_diff = 2 * diff\n        \n        # Fill x_i columns\n        for d in range(dim):\n            # Map point indices to param indices\n            col_i = idx[0] * dim + d\n            col_j = idx[1] * dim + d\n            \n            J[rows, col_i] = two_diff[:, d]\n            J[rows, col_j] = -two_diff[:, d]\n            \n        J[rows, -1] = -1.0\n        \n        # Gradients for 1 - d^2 >= 0\n        # Just negative of the distance gradients, 0 for s\n        J[n_pairs + rows, :-1] = -J[rows, :-1]\n        \n        return J\n\n    # -------------------------------------------------------------------------\n    # 2. Initialization Helpers (Repulsion)\n    # -------------------------------------------------------------------------\n    \n    def repulsion_obj(flat_x):\n        x = flat_x.reshape((N, dim))\n        diff = x[idx[0]] - x[idx[1]]\n        d2 = np.sum(diff**2, axis=1) + 1e-9\n        # Soft repulsion to spread points\n        return np.sum(1.0 / d2**4)\n\n    def repulsion_jac(flat_x):\n        x = flat_x.reshape((N, dim))\n        diff = x[idx[0]] - x[idx[1]]\n        d2 = np.sum(diff**2, axis=1) + 1e-9\n        \n        # Derivative of d^-8 is -4 * d^-10 * 2 * diff\n        grad = np.zeros((N, dim))\n        factors = -8.0 * (d2**(-5))[:, None] * diff\n        \n        np.add.at(grad, idx[0], factors)\n        np.add.at(grad, idx[1], -factors)\n        \n        return grad.flatten()\n\n    # -------------------------------------------------------------------------\n    # 3. Main Optimization Routine\n    # -------------------------------------------------------------------------\n    \n    t0 = time.time()\n    best_params = None\n    best_s = -np.inf\n    \n    # A. Generate Candidates\n    candidates = []\n    \n    # Random clouds\n    for _ in range(3):\n        candidates.append(np.random.randn(N * dim))\n        \n    # Center + Shell (1 point at origin, 13 on sphere)\n    # This is a known good structural motif for N=14\n    x_cs = np.random.randn(N, dim)\n    x_cs[0] = [0, 0, 0]\n    x_cs[1:] /= np.linalg.norm(x_cs[1:], axis=1, keepdims=True) + 1e-9\n    candidates.append(x_cs.flatten())\n\n    # Pre-optimize candidates with repulsion\n    refined_candidates = []\n    for cand in candidates:\n        if time.time() - t0 > 10: break\n        res = minimize(repulsion_obj, cand, jac=repulsion_jac, method='L-BFGS-B', tol=1e-3)\n        refined_candidates.append(res.x)\n        \n    # Select best candidate based on approximate s\n    for cand in refined_candidates:\n        x = cand.reshape((N, dim))\n        diff = x[idx[0]] - x[idx[1]]\n        d2 = np.sum(diff**2, axis=1)\n        \n        # Normalize to max_dist = 1\n        max_d2 = np.max(d2)\n        if max_d2 > 1e-9:\n            x_norm = x / np.sqrt(max_d2)\n            d2_norm = d2 / max_d2\n            s_val = np.min(d2_norm)\n            \n            # Create params vector\n            p = np.concatenate([x_norm.flatten(), [s_val]])\n            \n            if s_val > best_s:\n                best_s = s_val\n                best_params = p\n\n    # B. Micro-Basin-Hopping with SLSQP\n    # Definition of constraints for scipy\n    cons = {'type': 'ineq', 'fun': constraints, 'jac': constraints_jac}\n    # Bounds: coords unbounded, s in [0, 1]\n    bnds = [(None, None)] * (N * dim) + [(0.0, 1.0)]\n    \n    current_params = best_params\n    \n    # Initial Polish\n    res = minimize(obj, current_params, jac=obj_jac, constraints=cons, method='SLSQP', \n                   bounds=bnds, options={'ftol': 1e-9, 'maxiter': 200, 'disp': False})\n    \n    if -res.fun > best_s:\n        best_s = -res.fun\n        best_params = res.x\n        \n    # Loop\n    # We use small steps to refine the local basin, as requested\n    loop_iter = 0\n    while time.time() - t0 < 58:\n        loop_iter += 1\n        \n        # Perturbation scale: 1e-6 to 1e-9 as requested\n        # We vary it slightly to prevent getting stuck\n        scale = 10 ** np.random.uniform(-9, -6)\n        \n        perturbation = np.random.normal(0, scale, size=len(best_params))\n        # Ensure s stays valid in perturbation (not strictly necessary as optimizer fixes it, but helpful)\n        perturbation[-1] = 0 \n        \n        p_try = best_params + perturbation\n        \n        try:\n            # High precision polish\n            res = minimize(obj, p_try, jac=obj_jac, constraints=cons, method='SLSQP', \n                           bounds=bnds, options={'ftol': 1e-10, 'maxiter': 100, 'disp': False})\n            \n            # Check if feasible and better\n            if res.success or res.message == 'Iteration limit reached':\n                # Manually verify constraints (SLSQP can be loose)\n                c_vals = constraints(res.x)\n                if np.min(c_vals) > -1e-7:\n                    score = -res.fun\n                    if score > best_s + 1e-10:\n                        best_s = score\n                        best_params = res.x\n        except Exception:\n            continue\n\n    # -------------------------------------------------------------------------\n    # 4. Final Formatting\n    # -------------------------------------------------------------------------\n    \n    final_x, final_s = unpack(best_params)\n    \n    # Final normalization to ensure max_dist is exactly 1.0 for the output\n    # (The optimizer ensures <= 1, but we want the ratio, so scaling up to equality is optimal)\n    diff = final_x[idx[0]] - final_x[idx[1]]\n    d2 = np.sum(diff**2, axis=1)\n    max_d = np.sqrt(np.max(d2))\n    \n    if max_d > 1e-9:\n        final_x /= max_d\n        \n    return final_x",
  "metrics": {
    "valid": true,
    "score": 0.9712670344400297,
    "min_max_ratio": 0.233149798663882,
    "min_distance": 0.4828558777356676,
    "max_distance": 1.0,
    "eval_time": 58.28548192977905,
    "error": null
  },
  "prompt": "Refine the coordinates from trial_8_0 using a 'Micro-Basin-Hopping' approach. Use very small step sizes (1e-6 to 1e-9) and a high-precision SLSQP polish in each step. Focus on the slack variable formulation: maximize s s.t. d_ij^2 >= s and d_ij^2 <= 1.",
  "response": "An algorithm to place 14 points in 3D to maximize the min-max distance ratio, using a Micro-Basin-Hopping refinement strategy with SLSQP polishing.\n\n```python\nimport numpy as np\nfrom scipy.optimize import minimize\nimport time\n\ndef min_max_dist_dim3_14():\n    \"\"\"\n    Finds coordinates for 14 points in 3D that maximize the squared ratio \n    (min_dist / max_dist)^2.\n    \n    Strategy:\n    1. Heuristic Initialization: Try random clusters and Center+Shell configurations.\n       Optimize with a repulsive potential to find a good basin.\n    2. Formulation: Maximize s s.t. d_ij^2 >= s and d_ij^2 <= 1.\n    3. Refinement: Micro-Basin-Hopping with high-precision SLSQP.\n    \"\"\"\n    \n    N = 14\n    dim = 3\n    idx = np.triu_indices(N, k=1)\n    n_pairs = len(idx[0])\n    n_vars = N * dim + 1\n    \n    # -------------------------------------------------------------------------\n    # 1. Jacobian and Constraint Definitions\n    # -------------------------------------------------------------------------\n    \n    def unpack(params):\n        # x: (N, 3), s: scalar\n        return params[:-1].reshape((N, dim)), params[-1]\n\n    # Objective: Maximize s => Minimize -s\n    def obj(params):\n        return -params[-1]\n\n    def obj_jac(params):\n        grad = np.zeros(len(params))\n        grad[-1] = -1.0\n        return grad\n\n    # Constraints:\n    # 1. d^2 >= s  =>  d^2 - s >= 0\n    # 2. d^2 <= 1  =>  1 - d^2 >= 0\n    def constraints(params):\n        x, s = unpack(params)\n        diff = x[idx[0]] - x[idx[1]]\n        d2 = np.sum(diff**2, axis=1)\n        return np.concatenate([d2 - s, 1.0 - d2])\n\n    def constraints_jac(params):\n        x, s = unpack(params)\n        diff = x[idx[0]] - x[idx[1]] # Shape (91, 3)\n        \n        # Sparse-like Jacobian construction\n        J = np.zeros((2 * n_pairs, n_vars))\n        \n        # Indices for constructing the matrix\n        rows = np.arange(n_pairs)\n        \n        # Gradients for d^2 - s >= 0\n        # d(d^2)/dx_i = 2*(xi - xj)\n        # d(d^2)/dx_j = -2*(xi - xj)\n        \n        two_diff = 2 * diff\n        \n        # Fill x_i columns\n        for d in range(dim):\n            # Map point indices to param indices\n            col_i = idx[0] * dim + d\n            col_j = idx[1] * dim + d\n            \n            J[rows, col_i] = two_diff[:, d]\n            J[rows, col_j] = -two_diff[:, d]\n            \n        J[rows, -1] = -1.0\n        \n        # Gradients for 1 - d^2 >= 0\n        # Just negative of the distance gradients, 0 for s\n        J[n_pairs + rows, :-1] = -J[rows, :-1]\n        \n        return J\n\n    # -------------------------------------------------------------------------\n    # 2. Initialization Helpers (Repulsion)\n    # -------------------------------------------------------------------------\n    \n    def repulsion_obj(flat_x):\n        x = flat_x.reshape((N, dim))\n        diff = x[idx[0]] - x[idx[1]]\n        d2 = np.sum(diff**2, axis=1) + 1e-9\n        # Soft repulsion to spread points\n        return np.sum(1.0 / d2**4)\n\n    def repulsion_jac(flat_x):\n        x = flat_x.reshape((N, dim))\n        diff = x[idx[0]] - x[idx[1]]\n        d2 = np.sum(diff**2, axis=1) + 1e-9\n        \n        # Derivative of d^-8 is -4 * d^-10 * 2 * diff\n        grad = np.zeros((N, dim))\n        factors = -8.0 * (d2**(-5))[:, None] * diff\n        \n        np.add.at(grad, idx[0], factors)\n        np.add.at(grad, idx[1], -factors)\n        \n        return grad.flatten()\n\n    # -------------------------------------------------------------------------\n    # 3. Main Optimization Routine\n    # -------------------------------------------------------------------------\n    \n    t0 = time.time()\n    best_params = None\n    best_s = -np.inf\n    \n    # A. Generate Candidates\n    candidates = []\n    \n    # Random clouds\n    for _ in range(3):\n        candidates.append(np.random.randn(N * dim))\n        \n    # Center + Shell (1 point at origin, 13 on sphere)\n    # This is a known good structural motif for N=14\n    x_cs = np.random.randn(N, dim)\n    x_cs[0] = [0, 0, 0]\n    x_cs[1:] /= np.linalg.norm(x_cs[1:], axis=1, keepdims=True) + 1e-9\n    candidates.append(x_cs.flatten())\n\n    # Pre-optimize candidates with repulsion\n    refined_candidates = []\n    for cand in candidates:\n        if time.time() - t0 > 10: break\n        res = minimize(repulsion_obj, cand, jac=repulsion_jac, method='L-BFGS-B', tol=1e-3)\n        refined_candidates.append(res.x)\n        \n    # Select best candidate based on approximate s\n    for cand in refined_candidates:\n        x = cand.reshape((N, dim))\n        diff = x[idx[0]] - x[idx[1]]\n        d2 = np.sum(diff**2, axis=1)\n        \n        # Normalize to max_dist = 1\n        max_d2 = np.max(d2)\n        if max_d2 > 1e-9:\n            x_norm = x / np.sqrt(max_d2)\n            d2_norm = d2 / max_d2\n            s_val = np.min(d2_norm)\n            \n            # Create params vector\n            p = np.concatenate([x_norm.flatten(), [s_val]])\n            \n            if s_val > best_s:\n                best_s = s_val\n                best_params = p\n\n    # B. Micro-Basin-Hopping with SLSQP\n    # Definition of constraints for scipy\n    cons = {'type': 'ineq', 'fun': constraints, 'jac': constraints_jac}\n    # Bounds: coords unbounded, s in [0, 1]\n    bnds = [(None, None)] * (N * dim) + [(0.0, 1.0)]\n    \n    current_params = best_params\n    \n    # Initial Polish\n    res = minimize(obj, current_params, jac=obj_jac, constraints=cons, method='SLSQP', \n                   bounds=bnds, options={'ftol': 1e-9, 'maxiter': 200, 'disp': False})\n    \n    if -res.fun > best_s:\n        best_s = -res.fun\n        best_params = res.x\n        \n    # Loop\n    # We use small steps to refine the local basin, as requested\n    loop_iter = 0\n    while time.time() - t0 < 58:\n        loop_iter += 1\n        \n        # Perturbation scale: 1e-6 to 1e-9 as requested\n        # We vary it slightly to prevent getting stuck\n        scale = 10 ** np.random.uniform(-9, -6)\n        \n        perturbation = np.random.normal(0, scale, size=len(best_params))\n        # Ensure s stays valid in perturbation (not strictly necessary as optimizer fixes it, but helpful)\n        perturbation[-1] = 0 \n        \n        p_try = best_params + perturbation\n        \n        try:\n            # High precision polish\n            res = minimize(obj, p_try, jac=obj_jac, constraints=cons, method='SLSQP', \n                           bounds=bnds, options={'ftol': 1e-10, 'maxiter': 100, 'disp': False})\n            \n            # Check if feasible and better\n            if res.success or res.message == 'Iteration limit reached':\n                # Manually verify constraints (SLSQP can be loose)\n                c_vals = constraints(res.x)\n                if np.min(c_vals) > -1e-7:\n                    score = -res.fun\n                    if score > best_s + 1e-10:\n                        best_s = score\n                        best_params = res.x\n        except Exception:\n            continue\n\n    # -------------------------------------------------------------------------\n    # 4. Final Formatting\n    # -------------------------------------------------------------------------\n    \n    final_x, final_s = unpack(best_params)\n    \n    # Final normalization to ensure max_dist is exactly 1.0 for the output\n    # (The optimizer ensures <= 1, but we want the ratio, so scaling up to equality is optimal)\n    diff = final_x[idx[0]] - final_x[idx[1]]\n    d2 = np.sum(diff**2, axis=1)\n    max_d = np.sqrt(np.max(d2))\n    \n    if max_d > 1e-9:\n        final_x /= max_d\n        \n    return final_x\n```",
  "reasoning": "An algorithm to place 14 points in 3D to maximize the min-max distance ratio, using a Micro-Basin-Hopping refinement strategy with SLSQP polishing.",
  "timestamp": "2026-01-21T17:57:34.430327",
  "cost_data": null,
  "model_config": {
    "model": "gemini-3-pro-preview",
    "temperature": 0.7,
    "provider_debug": {
      "finish_reason": "STOP",
      "usage_metadata": {
        "prompt_token_count": 254,
        "candidates_token_count": 2384,
        "thoughts_token_count": 7347
      },
      "response_text_len": 7547
    }
  }
}