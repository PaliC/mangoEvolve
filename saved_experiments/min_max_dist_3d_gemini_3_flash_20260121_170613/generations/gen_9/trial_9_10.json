{
  "trial_id": "trial_9_10",
  "generation": 9,
  "parent_id": "trial_8_0",
  "code": "import numpy as np\nfrom scipy.optimize import minimize, basinhopping\n\ndef min_max_dist_dim3_14():\n    \"\"\"\n    Places 14 points in 3D space to maximize the squared ratio (min pairwise distance / max pairwise distance)^2.\n    Uses Basin-hopping with low temperature and an analytical Jacobian for refinement.\n    \"\"\"\n    N = 14\n    k = 100.0  # Constant for smooth approximation of min and max\n\n    def get_obj_and_grad(x_flat):\n        \"\"\"\n        Calculates the objective (negative ratio of squared min/max distances) and its Jacobian.\n        Uses a smooth approximation (log-sum-exp) for differentiability.\n        \"\"\"\n        x = x_flat.reshape((N, 3))\n        \n        # All-pairs squared distances\n        diff = x[:, np.newaxis, :] - x[np.newaxis, :, :] # (N, N, 3)\n        dist_sq = np.sum(diff**2, axis=-1) # (N, N)\n        \n        # Indices for the upper triangle (excluding diagonal)\n        i, j = np.triu_indices(N, k=1)\n        d_sq = dist_sq[i, j]\n        \n        # Smooth min using log-sum-exp trick for numerical stability\n        d_sq_min_val = np.min(d_sq)\n        exp_k_min = np.exp(-k * (d_sq - d_sq_min_val))\n        sum_exp_k_min = np.sum(exp_k_min)\n        s_min = d_sq_min_val - (1.0/k) * np.log(sum_exp_k_min)\n        \n        # Smooth max using log-sum-exp trick\n        d_sq_max_val = np.max(d_sq)\n        exp_k_max = np.exp(k * (d_sq - d_sq_max_val))\n        sum_exp_k_max = np.sum(exp_k_max)\n        s_max = d_sq_max_val + (1.0/k) * np.log(sum_exp_k_max)\n        \n        # Objective: - (min_dist^2 / max_dist^2)\n        obj = -s_min / s_max\n        \n        # Gradient of s_min and s_max with respect to point coordinates\n        w_min = exp_k_min / sum_exp_k_min\n        w_max = exp_k_max / sum_exp_k_max\n        \n        # Construct weight matrices for vectorized gradient computation\n        W_min = np.zeros((N, N))\n        W_min[i, j] = w_min\n        W_min[j, i] = w_min\n        \n        W_max = np.zeros((N, N))\n        W_max[i, j] = w_max\n        W_max[j, i] = w_max\n        \n        # grad_s[i] = 2 * sum_j W[i, j] * (x_i - x_j)\n        grad_s_min = 2 * (np.diag(np.sum(W_min, axis=1)) @ x - W_min @ x)\n        grad_s_max = 2 * (np.diag(np.sum(W_max, axis=1)) @ x - W_max @ x)\n        \n        # Derivative of the quotient: d(-s_min/s_max) = -(grad_s_min * s_max - s_min * grad_s_max) / s_max^2\n        grad = -(grad_s_min * s_max - s_min * grad_s_max) / (s_max**2)\n        \n        return obj, grad.flatten()\n\n    def get_structured_initial_guess():\n        \"\"\"\n        Provides a bicapped hexagonal antiprism as a structured starting configuration.\n        \"\"\"\n        pts = []\n        h, r = 0.6, 1.0\n        # Hexagonal antiprism (12 points)\n        for j in range(6):\n            theta1 = j * 2 * np.pi / 6\n            pts.append([r * np.cos(theta1), r * np.sin(theta1), h])\n            theta2 = (j + 0.5) * 2 * np.pi / 6\n            pts.append([r * np.cos(theta2), r * np.sin(theta2), -h])\n        # Two caps (2 points)\n        pts.append([0, 0, 1.5])\n        pts.append([0, 0, -1.5])\n        return np.array(pts).flatten()\n\n    # Optimization pipeline\n    best_x = None\n    best_obj = float('inf')\n    \n    # 1. Try different initializations to avoid poor local minima\n    initializations = [get_structured_initial_guess()]\n    for _ in range(3):\n        initializations.append(np.random.randn(N * 3))\n        \n    for x0 in initializations:\n        res = minimize(get_obj_and_grad, x0, jac=True, method='L-BFGS-B', options={'maxiter': 500})\n        if res.fun < best_obj:\n            best_obj = res.fun\n            best_x = res.x\n            \n    # 2. Refine the best solution found with Basin-hopping\n    # Using low temperature and small step size as specified\n    minimizer_kwargs = {\"method\": \"L-BFGS-B\", \"jac\": True}\n    res_bh = basinhopping(\n        get_obj_and_grad, \n        best_x, \n        minimizer_kwargs=minimizer_kwargs, \n        niter=30, \n        T=0.0001, \n        stepsize=0.01\n    )\n    \n    # Final points reshaped to (14, 3)\n    return res_bh.x.reshape((N, 3))\n\nif __name__ == \"__main__\":\n    points = min_max_dist_dim3_14()\n    print(points.shape)",
  "metrics": {
    "valid": true,
    "score": 0.9983029803260244,
    "min_max_ratio": 0.23963969805971747,
    "min_distance": 14.289444905491488,
    "max_distance": 29.190126458067617,
    "eval_time": 0.5326828956604004,
    "error": null
  },
  "prompt": "Refine the best known solution trial_8_0 using Basin-hopping with a very low temperature and tiny step size. Focus on the analytical Jacobian. Variation 1",
  "response": "```python\nimport numpy as np\nfrom scipy.optimize import minimize, basinhopping\n\ndef min_max_dist_dim3_14():\n    \"\"\"\n    Places 14 points in 3D space to maximize the squared ratio (min pairwise distance / max pairwise distance)^2.\n    Uses Basin-hopping with low temperature and an analytical Jacobian for refinement.\n    \"\"\"\n    N = 14\n    k = 100.0  # Constant for smooth approximation of min and max\n\n    def get_obj_and_grad(x_flat):\n        \"\"\"\n        Calculates the objective (negative ratio of squared min/max distances) and its Jacobian.\n        Uses a smooth approximation (log-sum-exp) for differentiability.\n        \"\"\"\n        x = x_flat.reshape((N, 3))\n        \n        # All-pairs squared distances\n        diff = x[:, np.newaxis, :] - x[np.newaxis, :, :] # (N, N, 3)\n        dist_sq = np.sum(diff**2, axis=-1) # (N, N)\n        \n        # Indices for the upper triangle (excluding diagonal)\n        i, j = np.triu_indices(N, k=1)\n        d_sq = dist_sq[i, j]\n        \n        # Smooth min using log-sum-exp trick for numerical stability\n        d_sq_min_val = np.min(d_sq)\n        exp_k_min = np.exp(-k * (d_sq - d_sq_min_val))\n        sum_exp_k_min = np.sum(exp_k_min)\n        s_min = d_sq_min_val - (1.0/k) * np.log(sum_exp_k_min)\n        \n        # Smooth max using log-sum-exp trick\n        d_sq_max_val = np.max(d_sq)\n        exp_k_max = np.exp(k * (d_sq - d_sq_max_val))\n        sum_exp_k_max = np.sum(exp_k_max)\n        s_max = d_sq_max_val + (1.0/k) * np.log(sum_exp_k_max)\n        \n        # Objective: - (min_dist^2 / max_dist^2)\n        obj = -s_min / s_max\n        \n        # Gradient of s_min and s_max with respect to point coordinates\n        w_min = exp_k_min / sum_exp_k_min\n        w_max = exp_k_max / sum_exp_k_max\n        \n        # Construct weight matrices for vectorized gradient computation\n        W_min = np.zeros((N, N))\n        W_min[i, j] = w_min\n        W_min[j, i] = w_min\n        \n        W_max = np.zeros((N, N))\n        W_max[i, j] = w_max\n        W_max[j, i] = w_max\n        \n        # grad_s[i] = 2 * sum_j W[i, j] * (x_i - x_j)\n        grad_s_min = 2 * (np.diag(np.sum(W_min, axis=1)) @ x - W_min @ x)\n        grad_s_max = 2 * (np.diag(np.sum(W_max, axis=1)) @ x - W_max @ x)\n        \n        # Derivative of the quotient: d(-s_min/s_max) = -(grad_s_min * s_max - s_min * grad_s_max) / s_max^2\n        grad = -(grad_s_min * s_max - s_min * grad_s_max) / (s_max**2)\n        \n        return obj, grad.flatten()\n\n    def get_structured_initial_guess():\n        \"\"\"\n        Provides a bicapped hexagonal antiprism as a structured starting configuration.\n        \"\"\"\n        pts = []\n        h, r = 0.6, 1.0\n        # Hexagonal antiprism (12 points)\n        for j in range(6):\n            theta1 = j * 2 * np.pi / 6\n            pts.append([r * np.cos(theta1), r * np.sin(theta1), h])\n            theta2 = (j + 0.5) * 2 * np.pi / 6\n            pts.append([r * np.cos(theta2), r * np.sin(theta2), -h])\n        # Two caps (2 points)\n        pts.append([0, 0, 1.5])\n        pts.append([0, 0, -1.5])\n        return np.array(pts).flatten()\n\n    # Optimization pipeline\n    best_x = None\n    best_obj = float('inf')\n    \n    # 1. Try different initializations to avoid poor local minima\n    initializations = [get_structured_initial_guess()]\n    for _ in range(3):\n        initializations.append(np.random.randn(N * 3))\n        \n    for x0 in initializations:\n        res = minimize(get_obj_and_grad, x0, jac=True, method='L-BFGS-B', options={'maxiter': 500})\n        if res.fun < best_obj:\n            best_obj = res.fun\n            best_x = res.x\n            \n    # 2. Refine the best solution found with Basin-hopping\n    # Using low temperature and small step size as specified\n    minimizer_kwargs = {\"method\": \"L-BFGS-B\", \"jac\": True}\n    res_bh = basinhopping(\n        get_obj_and_grad, \n        best_x, \n        minimizer_kwargs=minimizer_kwargs, \n        niter=30, \n        T=0.0001, \n        stepsize=0.01\n    )\n    \n    # Final points reshaped to (14, 3)\n    return res_bh.x.reshape((N, 3))\n\nif __name__ == \"__main__\":\n    points = min_max_dist_dim3_14()\n    print(points.shape)\n```",
  "reasoning": "",
  "timestamp": "2026-01-21T17:59:50.730455",
  "cost_data": null,
  "model_config": {
    "model": "gemini-3-flash-preview",
    "temperature": 0.7,
    "provider_debug": {
      "finish_reason": "STOP",
      "usage_metadata": {
        "prompt_token_count": 206,
        "candidates_token_count": 1485,
        "thoughts_token_count": 19071
      },
      "response_text_len": 4132
    }
  }
}