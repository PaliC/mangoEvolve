{
  "trial_id": "trial_9_11",
  "generation": 9,
  "parent_id": "trial_8_0",
  "code": "import numpy as np\nfrom scipy.optimize import basinhopping\n\ndef min_max_dist_dim3_14():\n    \"\"\"\n    Place 14 points in 3D space to maximize the squared ratio (min pairwise distance / max pairwise distance)^2.\n    Uses Basin-hopping with low temperature and tiny step size, focusing on the analytical Jacobian.\n    \"\"\"\n    N = 14\n    dim = 3\n    \n    def get_dists_sq(x):\n        \"\"\"Compute all-pairs squared distances between N points.\"\"\"\n        # x is (N, dim)\n        diffs = x[:, np.newaxis, :] - x[np.newaxis, :, :] # (N, N, dim)\n        return np.sum(diffs**2, axis=2) # (N, N)\n\n    def objective(x_flat):\n        \"\"\"Smooth approximation of the negative squared ratio for minimization.\"\"\"\n        x = x_flat.reshape((N, dim))\n        dists_sq = get_dists_sq(x)\n        triu_idx = np.triu_indices(N, 1)\n        dists_sq_triu = dists_sq[triu_idx]\n        \n        # Soft-min and soft-max parameter\n        k = 100\n        min_val = np.min(dists_sq_triu)\n        max_val = np.max(dists_sq_triu)\n        \n        # Log-sum-exp trick for numerical stability\n        soft_min = min_val - np.log(np.sum(np.exp(-k * (dists_sq_triu - min_val)))) / k\n        soft_max = max_val + np.log(np.sum(np.exp(k * (dists_sq_triu - max_val)))) / k\n        \n        # Minimize -soft_min / soft_max to maximize the ratio\n        return -soft_min / soft_max\n\n    def grad(x_flat):\n        \"\"\"Analytical Jacobian of the smooth objective function.\"\"\"\n        x = x_flat.reshape((N, dim))\n        diffs = x[:, np.newaxis, :] - x[np.newaxis, :, :] # (N, N, dim)\n        dists_sq = np.sum(diffs**2, axis=2)\n        triu_idx = np.triu_indices(N, 1)\n        dists_sq_triu = dists_sq[triu_idx]\n        \n        k = 100\n        min_val = np.min(dists_sq_triu)\n        max_val = np.max(dists_sq_triu)\n        \n        # Soft-min calculations\n        exp_min_triu = np.exp(-k * (dists_sq_triu - min_val))\n        sum_exp_min = np.sum(exp_min_triu)\n        soft_min = min_val - np.log(sum_exp_min) / k\n        \n        # Soft-max calculations\n        exp_max_triu = np.exp(k * (dists_sq_triu - max_val))\n        sum_exp_max = np.sum(exp_max_triu)\n        soft_max = max_val + np.log(sum_exp_max) / k\n        \n        # Expand upper triangle values to full symmetric matrices\n        exp_min_full = np.zeros((N, N))\n        exp_min_full[triu_idx] = exp_min_triu\n        exp_min_full = exp_min_full + exp_min_full.T\n        \n        exp_max_full = np.zeros((N, N))\n        exp_max_full[triu_idx] = exp_max_triu\n        exp_max_full = exp_max_full + exp_max_full.T\n        \n        # Vectorized gradient calculations\n        # d(soft_min)/dx_i = (2.0 / sum_exp_min) * sum_j exp_min_full[i, j] * (x_i - x_j)\n        grad_soft_min = (2.0 / sum_exp_min) * np.sum(exp_min_full[:, :, np.newaxis] * diffs, axis=1)\n        grad_soft_max = (2.0 / sum_exp_max) * np.sum(exp_max_full[:, :, np.newaxis] * diffs, axis=1)\n        \n        # d(-soft_min / soft_max) = -(grad_soft_min * soft_max - soft_min * grad_soft_max) / (soft_max^2)\n        g = -(grad_soft_min * soft_max - soft_min * grad_soft_max) / (soft_max**2)\n        return g.flatten()\n\n    # Create several initial guesses to explore different basins\n    \n    # 1. Rhombic dodecahedron (cube vertices + face centers shifted outward)\n    h = np.sqrt(3)\n    x1 = np.array([\n        [1, 1, 1], [1, 1, -1], [1, -1, 1], [1, -1, -1],\n        [-1, 1, 1], [-1, 1, -1], [-1, -1, 1], [-1, -1, -1],\n        [h, 0, 0], [-h, 0, 0], [0, h, 0], [0, -h, 0], [0, 0, h], [0, 0, -h]\n    ], dtype=float).flatten()\n\n    # 2. Fibonacci sphere distribution (even spread on a sphere)\n    phi = np.pi * (3. - np.sqrt(5.))\n    pts2 = []\n    for i in range(N):\n        y = 1 - (i / float(N - 1)) * 2\n        radius = np.sqrt(1 - y * y)\n        theta = phi * i\n        pts2.append([radius * np.cos(theta), y, radius * np.sin(theta)])\n    x2 = np.array(pts2).flatten()\n    \n    best_x = x1\n    best_val = float('inf')\n    \n    # Run Basin-hopping refinement for each initial guess\n    for x0 in [x1, x2]:\n        # T is the temperature for the Metropolis criterion; stepsize is for the random displacement\n        res = basinhopping(objective, x0, niter=40, T=0.0001, stepsize=0.005,\n                          minimizer_kwargs={\"jac\": grad, \"method\": \"L-BFGS-B\"})\n        if res.fun < best_val:\n            best_val = res.fun\n            best_x = res.x\n            \n    return best_x.reshape((N, dim))",
  "metrics": {
    "valid": true,
    "score": 0.9366228662125391,
    "min_max_ratio": 0.22483356784299974,
    "min_distance": 216.13322863307258,
    "max_distance": 455.8174686144398,
    "eval_time": 0.8601832389831543,
    "error": null
  },
  "prompt": "Refine the best known solution trial_8_0 using Basin-hopping with a very low temperature and tiny step size. Focus on the analytical Jacobian. Variation 2",
  "response": "```python\nimport numpy as np\nfrom scipy.optimize import basinhopping\n\ndef min_max_dist_dim3_14():\n    \"\"\"\n    Place 14 points in 3D space to maximize the squared ratio (min pairwise distance / max pairwise distance)^2.\n    Uses Basin-hopping with low temperature and tiny step size, focusing on the analytical Jacobian.\n    \"\"\"\n    N = 14\n    dim = 3\n    \n    def get_dists_sq(x):\n        \"\"\"Compute all-pairs squared distances between N points.\"\"\"\n        # x is (N, dim)\n        diffs = x[:, np.newaxis, :] - x[np.newaxis, :, :] # (N, N, dim)\n        return np.sum(diffs**2, axis=2) # (N, N)\n\n    def objective(x_flat):\n        \"\"\"Smooth approximation of the negative squared ratio for minimization.\"\"\"\n        x = x_flat.reshape((N, dim))\n        dists_sq = get_dists_sq(x)\n        triu_idx = np.triu_indices(N, 1)\n        dists_sq_triu = dists_sq[triu_idx]\n        \n        # Soft-min and soft-max parameter\n        k = 100\n        min_val = np.min(dists_sq_triu)\n        max_val = np.max(dists_sq_triu)\n        \n        # Log-sum-exp trick for numerical stability\n        soft_min = min_val - np.log(np.sum(np.exp(-k * (dists_sq_triu - min_val)))) / k\n        soft_max = max_val + np.log(np.sum(np.exp(k * (dists_sq_triu - max_val)))) / k\n        \n        # Minimize -soft_min / soft_max to maximize the ratio\n        return -soft_min / soft_max\n\n    def grad(x_flat):\n        \"\"\"Analytical Jacobian of the smooth objective function.\"\"\"\n        x = x_flat.reshape((N, dim))\n        diffs = x[:, np.newaxis, :] - x[np.newaxis, :, :] # (N, N, dim)\n        dists_sq = np.sum(diffs**2, axis=2)\n        triu_idx = np.triu_indices(N, 1)\n        dists_sq_triu = dists_sq[triu_idx]\n        \n        k = 100\n        min_val = np.min(dists_sq_triu)\n        max_val = np.max(dists_sq_triu)\n        \n        # Soft-min calculations\n        exp_min_triu = np.exp(-k * (dists_sq_triu - min_val))\n        sum_exp_min = np.sum(exp_min_triu)\n        soft_min = min_val - np.log(sum_exp_min) / k\n        \n        # Soft-max calculations\n        exp_max_triu = np.exp(k * (dists_sq_triu - max_val))\n        sum_exp_max = np.sum(exp_max_triu)\n        soft_max = max_val + np.log(sum_exp_max) / k\n        \n        # Expand upper triangle values to full symmetric matrices\n        exp_min_full = np.zeros((N, N))\n        exp_min_full[triu_idx] = exp_min_triu\n        exp_min_full = exp_min_full + exp_min_full.T\n        \n        exp_max_full = np.zeros((N, N))\n        exp_max_full[triu_idx] = exp_max_triu\n        exp_max_full = exp_max_full + exp_max_full.T\n        \n        # Vectorized gradient calculations\n        # d(soft_min)/dx_i = (2.0 / sum_exp_min) * sum_j exp_min_full[i, j] * (x_i - x_j)\n        grad_soft_min = (2.0 / sum_exp_min) * np.sum(exp_min_full[:, :, np.newaxis] * diffs, axis=1)\n        grad_soft_max = (2.0 / sum_exp_max) * np.sum(exp_max_full[:, :, np.newaxis] * diffs, axis=1)\n        \n        # d(-soft_min / soft_max) = -(grad_soft_min * soft_max - soft_min * grad_soft_max) / (soft_max^2)\n        g = -(grad_soft_min * soft_max - soft_min * grad_soft_max) / (soft_max**2)\n        return g.flatten()\n\n    # Create several initial guesses to explore different basins\n    \n    # 1. Rhombic dodecahedron (cube vertices + face centers shifted outward)\n    h = np.sqrt(3)\n    x1 = np.array([\n        [1, 1, 1], [1, 1, -1], [1, -1, 1], [1, -1, -1],\n        [-1, 1, 1], [-1, 1, -1], [-1, -1, 1], [-1, -1, -1],\n        [h, 0, 0], [-h, 0, 0], [0, h, 0], [0, -h, 0], [0, 0, h], [0, 0, -h]\n    ], dtype=float).flatten()\n\n    # 2. Fibonacci sphere distribution (even spread on a sphere)\n    phi = np.pi * (3. - np.sqrt(5.))\n    pts2 = []\n    for i in range(N):\n        y = 1 - (i / float(N - 1)) * 2\n        radius = np.sqrt(1 - y * y)\n        theta = phi * i\n        pts2.append([radius * np.cos(theta), y, radius * np.sin(theta)])\n    x2 = np.array(pts2).flatten()\n    \n    best_x = x1\n    best_val = float('inf')\n    \n    # Run Basin-hopping refinement for each initial guess\n    for x0 in [x1, x2]:\n        # T is the temperature for the Metropolis criterion; stepsize is for the random displacement\n        res = basinhopping(objective, x0, niter=40, T=0.0001, stepsize=0.005,\n                          minimizer_kwargs={\"jac\": grad, \"method\": \"L-BFGS-B\"})\n        if res.fun < best_val:\n            best_val = res.fun\n            best_x = res.x\n            \n    return best_x.reshape((N, dim))\n\n```",
  "reasoning": "",
  "timestamp": "2026-01-21T17:59:53.705023",
  "cost_data": null,
  "model_config": {
    "model": "gemini-3-flash-preview",
    "temperature": 0.7,
    "provider_debug": {
      "finish_reason": "STOP",
      "usage_metadata": {
        "prompt_token_count": 206,
        "candidates_token_count": 1633,
        "thoughts_token_count": 22146
      },
      "response_text_len": 4404
    }
  }
}