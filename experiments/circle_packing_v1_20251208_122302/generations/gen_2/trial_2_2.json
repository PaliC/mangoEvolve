{
  "trial_id": "trial_2_2",
  "generation": 2,
  "parent_id": "trial_1_5",
  "code": "",
  "metrics": {},
  "prompt": "Enhance this hybrid approach with smarter candidate selection:\n\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom typing import Tuple, List, Optional\n\ndef construct_packing() -> Tuple[np.ndarray, np.ndarray, float]:\n    \"\"\"\n    Hybrid approach: Random search + local optimization\n    \n    Returns:\n        centers: np.array of shape (26, 2)\n        radii: np.array of shape (26,)\n        sum_radii: float\n    \"\"\"\n    n_circles = 26\n    \n    # Phase 1: Random search to find promising starting points\n    print(\"Phase 1: Random search for initial configurations...\")\n    initial_candidates = random_search_phase(n_circles, n_trials=20)\n    \n    # Phase 2: Local optimization of top candidates\n    print(\"\\nPhase 2: Local optimization of top candidates...\")\n    best_result = local_optimization_phase(initial_candidates, n_circles, top_k=5)\n    \n    if best_result is None:\n        print(\"Optimization failed, using fallback grid packing\")\n        centers, radii = create_grid_packing(n_circles)\n        sum_radii = np.sum(radii)\n        best_result = (centers, radii, sum_radii)\n    \n    centers, radii, sum_radii = best_result\n    print(f\"\\nFinal result: Sum of radii = {sum_radii:.6f}\")\n    print(f\"Average radius: {np.mean(radii):.6f}\")\n    print(f\"Valid packing: {verify_packing(centers, radii)}\")\n    \n    return centers, radii, sum_radii\n\n\ndef random_search_phase(n_circles: int, n_trials: int = 20) -> List[Tuple[np.ndarray, np.ndarray, float]]:\n    \"\"\"\n    Phase 1: Generate multiple random configurations with quick local optimization\n    Returns list of (centers, radii, sum_radii) tuples\n    \"\"\"\n    candidates = []\n    \n    for trial in range(n_trials):\n        # Strategy: Mix different initialization strategies\n        if trial < n_trials // 3:\n            # Pure random\n            centers = np.random.rand(n_circles, 2)\n        elif trial < 2 * n_trials // 3:\n            # Grid with noise\n            centers = create_grid_centers(n_circles)\n            centers += np.random.normal(0, 0.05, centers.shape)\n            centers = np.clip(centers, 0.1, 0.9)\n        else:\n            # Hexagonal-like pattern with noise\n            centers = create_hex_centers(n_circles)\n            centers += np.random.normal(0, 0.03, centers.shape)\n            centers = np.clip(centers, 0.1, 0.9)\n        \n        # Compute initial radii based on nearest neighbors\n        radii = compute_initial_radii(centers, n_circles)\n        \n        # Quick local optimization (few iterations)\n        result = quick_optimize(centers, radii, max_iter=100)\n        \n        if result is not None:\n            candidates.append(result)\n            print(f\"  Trial {trial+1}/{n_trials}: sum_radii = {result[2]:.6f}\")\n    \n    # Sort by sum of radii (descending)\n    candidates.sort(key=lambda x: x[2], reverse=True)\n    \n    return candidates\n\n\ndef local_optimization_phase(candidates: List[Tuple], n_circles: int, top_k: int = 5) -> Optional[Tuple]:\n    \"\"\"\n    Phase 2: Intensive local optimization of top candidates\n    \"\"\"\n    best_result = None\n    best_sum_radii = 0\n    \n    # Take top_k candidates\n    top_candidates = candidates[:min(top_k, len(candidates))]\n    \n    for idx, (centers, radii, sum_radii) in enumerate(top_candidates):\n        print(f\"\\n  Optimizing candidate {idx+1}/{len(top_candidates)} (initial sum: {sum_radii:.6f})...\")\n        \n        # Intensive optimization with scipy\n        result = intensive_optimize(centers, radii, n_circles, max_iter=1000)\n        \n        if result is not None:\n            centers_opt, radii_opt, sum_radii_opt = result\n            print(f\"    -> Optimized sum: {sum_radii_opt:.6f}\")\n            \n            if sum_radii_opt > best_sum_radii:\n                best_sum_radii = sum_radii_opt\n                best_result = result\n        \n        # Also try perturbation + re-optimization\n        if result is not None:\n            for perturb in range(2):\n                centers_pert = centers_opt + np.random.normal(0, 0.01, centers_opt.shape)\n                centers_pert = np.clip(centers_pert, 0, 1)\n                radii_pert = radii_opt * (1 + np.random.normal(0, 0.05, radii_opt.shape))\n                radii_pert = np.clip(radii_pert, 0.001, 0.5)\n                \n                result_pert = intensive_optimize(centers_pert, radii_pert, n_circles, max_iter=500)\n                \n                if result_pert is not None and result_pert[2] > best_sum_radii:\n                    best_sum_radii = result_pert[2]\n                    best_result = result_pert\n                    print(f\"    -> Perturbation improved: {best_sum_radii:.6f}\")\n    \n    return best_result\n\n\ndef quick_optimize(centers: np.ndarray, radii: np.ndarray, max_iter: int = 100) -> Optional[Tuple]:\n    \"\"\"Quick optimization with fewer iterations\"\"\"\n    n_circles = len(centers)\n    x0 = np.concatenate([centers.flatten(), radii])\n    \n    bounds = [(0, 1)] * (2 * n_circles) + [(0.001, 0.5)] * n_circles\n    \n    def objective(x):\n        return -np.sum(x[2*n_circles:])\n    \n    constraints = create_constraints(n_circles)\n    \n    try:\n        result = minimize(\n            objective,\n            x0,\n            method='SLSQP',\n            bounds=bounds,\n            constraints=constraints,\n            options={'maxiter': max_iter, 'ftol': 1e-4}\n        )\n        \n        if result.fun < 0:\n            centers_opt = result.x[:2*n_circles].reshape(n_circles, 2)\n            radii_opt = result.x[2*n_circles:]\n            sum_radii = np.sum(radii_opt)\n            \n            if verify_packing(centers_opt, radii_opt, tol=1e-3):\n                return centers_opt, radii_opt, sum_radii\n    except:\n        pass\n    \n    return None\n\n\ndef intensive_optimize(centers: np.ndarray, radii: np.ndarray, n_circles: int, \n                       max_iter: int = 1000) -> Optional[Tuple]:\n    \"\"\"Intensive optimization with more iterations and tighter tolerances\"\"\"\n    x0 = np.concatenate([centers.flatten(), radii])\n    \n    bounds = [(0, 1)] * (2 * n_circles) + [(0.001, 0.5)] * n_circles\n    \n    def objective(x):\n        return -np.sum(x[2*n_circles:])\n    \n    constraints = create_constraints(n_circles)\n    \n    try:\n        result = minimize(\n            objective,\n            x0,\n            method='SLSQP',\n            bounds=bounds,\n            constraints=constraints,\n            options={'maxiter': max_iter, 'ftol': 1e-8, 'disp': False}\n        )\n        \n        if result.fun < 0:\n            centers_opt = result.x[:2*n_circles].reshape(n_circles, 2)\n            radii_opt = result.x[2*n_circles:]\n            sum_radii = np.sum(radii_opt)\n            \n            if verify_packing(centers_opt, radii_opt, tol=1e-4):\n                return centers_opt, radii_opt, sum_radii\n    except:\n        pass\n    \n    return None\n\n\ndef create_constraints(n_circles: int):\n    \"\"\"Create constraint functions for optimization\"\"\"\n    def constraints_func(x):\n        centers = x[:2*n_circles].reshape(n_circles, 2)\n        radii = x[2*n_circles:]\n        \n        constraints = []\n        \n        # Boundary constraints\n        for i in range(n_circles):\n            constraints.append(centers[i, 0] - radii[i])  # left\n            constraints.append(1 - centers[i, 0] - radii[i])  # right\n            constraints.append(centers[i, 1] - radii[i])  # bottom\n            constraints.append(1 - centers[i, 1] - radii[i])  # top\n        \n        # Non-overlap constraints\n        for i in range(n_circles):\n            for j in range(i+1, n_circles):\n                dist = np.sqrt((centers[i, 0] - centers[j, 0])**2 + \n                              (centers[i, 1] - centers[j, 1])**2)\n                constraints.append(dist - radii[i] - radii[j])\n        \n        # Positive radii\n        constraints.extend(radii)\n        \n        return np.array(constraints)\n    \n    return {'type': 'ineq', 'fun': constraints_func}\n\n\ndef compute_initial_radii(centers: np.ndarray, n_circles: int) -> np.ndarray:\n    \"\"\"Compute initial radii based on nearest neighbor distances and boundaries\"\"\"\n    radii = np.zeros(n_circles)\n    \n    for i in range(n_circles):\n        # Distance to boundaries\n        dist_to_boundary = min(\n            centers[i, 0], 1 - centers[i, 0],\n            centers[i, 1], 1 - centers[i, 1]\n        )\n        \n        # Distance to nearest neighbor\n        min_dist_to_neighbor = float('inf')\n        for j in range(n_circles):\n            if i != j:\n                dist = np.sqrt((centers[i, 0] - centers[j, 0])**2 + \n                              (centers[i, 1] - centers[j, 1])**2)\n                min_dist_to_neighbor = min(min_dist_to_neighbor, dist)\n        \n        # Conservative radius estimate\n        radii[i] = min(dist_to_boundary, min_dist_to_neighbor / 2.5) * 0.9\n    \n    return np.clip(radii, 0.01, 0.5)\n\n\ndef create_grid_centers(n_circles: int) -> np.ndarray:\n    \"\"\"Create grid-based initial centers\"\"\"\n    grid_size = int(np.ceil(np.sqrt(n_circles)))\n    centers = []\n    \n    for i in range(n_circles):\n        row = i // grid_size\n        col = i % grid_size\n        x = (col + 0.5) / grid_size\n        y = (row + 0.5) / grid_size\n        centers.append([x, y])\n    \n    return np.array(centers)\n\n\ndef create_hex_centers(n_circles: int) -> np.ndarray:\n    \"\"\"Create hexagonal packing initial centers\"\"\"\n    centers = []\n    rows = int(np.ceil(np.sqrt(n_circles)))\n    \n    for row in range(rows):\n        cols = int(np.ceil(n_circles / rows))\n        offset = 0.5 if row % 2 == 1 else 0\n        \n        for col in range(cols):\n            if len(centers) >= n_circles:\n                break\n            \n            x = (col + offset + 0.5) / (cols + 0.5)\n            y = (row + 0.5) / (rows + 0.5)\n            centers.append([x, y])\n    \n    return np.array(centers[:n_circles])\n\n\ndef verify_packing(centers: np.ndarray, radii: np.ndarray, tol: float = 1e-4) -> bool:\n    \"\"\"Verify that a packing is valid\"\"\"\n    n_circles = len(centers)\n    \n    # Check boundaries\n    for i in range(n_circles):\n        if (centers[i, 0] - radii[i] < -tol or \n            centers[i, 0] + radii[i] > 1 + tol or\n            centers[i, 1] - radii[i] < -tol or \n            centers[i, 1] + radii[i] > 1 + tol):\n            return False\n    \n    # Check overlaps\n    for i in range(n_circles):\n        for j in range(i+1, n_circles):\n            dist = np.sqrt((centers[i, 0] - centers[j, 0])**2 + \n                          (centers[i, 1] - centers[j, 1])**2)\n            if dist < radii[i] + radii[j] - tol:\n                return False\n    \n    # Check positive radii\n    if np.any(radii <= 0):\n        return False\n    \n    return True\n\n\ndef create_grid_packing(n_circles: int) -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Fallback grid packing\"\"\"\n    grid_size = int(np.ceil(np.sqrt(n_circles)))\n    spacing = 1.0 / grid_size\n    radius = spacing / 2.5\n    \n    centers = []\n    for i in range(n_circles):\n        row = i // grid_size\n        col = i % grid_size\n        x = (col + 0.5) * spacing\n        y = (row + 0.5) * spacing\n        centers.append([x, y])\n    \n    centers = np.array(centers)\n    radii = np.full(n_circles, radius)\n    \n    return centers, radii\n\n\ndef run_packing():\n    return construct_packing()\n\n\nif __name__ == \"__main__\":\n    centers, radii, sum_radii = run_packing()\n    print(f\"\\n{'='*50}\")\n    print(f\"FINAL RESULTS:\")\n    print(f\"Sum of radii: {sum_radii:.6f}\")\n    print(f\"Average radius: {np.mean(radii):.6f}\")\n    print(f\"Min radius: {np.min(radii):.6f}\")\n    print(f\"Max radius: {np.max(radii):.6f}\")\n    print(f\"Valid packing: {verify_packing(centers, radii)}\")\n    print(f\"{'='*50}\")\n\nImprovements:\n1. Use clustering to identify diverse candidate regions in phase 1\n2. Select top-k candidates from different clusters for phase 2\n3. Add novelty metric to avoid optimizing similar configurations\n4. Implement adaptive number of candidates based on diversity\n5. Use Bayesian optimization to guide initial configuration sampling\n\nFocus on better exploration of the solution space.",
  "response": "Here's an enhanced version with smarter candidate selection using clustering, diversity metrics, and adaptive sampling:\n\n```python\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import pairwise_distances\nfrom typing import Tuple, List, Optional, Dict\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndef construct_packing() -> Tuple[np.ndarray, np.ndarray, float]:\n    \"\"\"\n    Enhanced hybrid approach with smart candidate selection\n    \n    Returns:\n        centers: np.array of shape (26, 2)\n        radii: np.array of shape (26,)\n        sum_radii: float\n    \"\"\"\n    n_circles = 26\n    \n    # Phase 1: Diverse candidate generation with adaptive sampling\n    print(\"Phase 1: Generating diverse initial configurations...\")\n    initial_candidates = diverse_candidate_generation(\n        n_circles, \n        n_trials=30,\n        diversity_threshold=0.1\n    )\n    \n    # Phase 2: Smart selection from diverse clusters\n    print(\"\\nPhase 2: Selecting diverse candidates via clustering...\")\n    selected_candidates = select_diverse_candidates(\n        initial_candidates, \n        n_clusters=5,\n        top_per_cluster=2\n    )\n    \n    # Phase 3: Intensive optimization with novelty preservation\n    print(\"\\nPhase 3: Intensive optimization with novelty search...\")\n    best_result = novelty_aware_optimization(\n        selected_candidates, \n        n_circles,\n        exploration_weight=0.3\n    )\n    \n    if best_result is None:\n        print(\"Optimization failed, using fallback grid packing\")\n        centers, radii = create_grid_packing(n_circles)\n        sum_radii = np.sum(radii)\n        best_result = (centers, radii, sum_radii)\n    \n    centers, radii, sum_radii = best_result\n    print(f\"\\nFinal result: Sum of radii = {sum_radii:.6f}\")\n    print(f\"Average radius: {np.mean(radii):.6f}\")\n    print(f\"Valid packing: {verify_packing(centers, radii)}\")\n    \n    return centers, radii, sum_radii\n\n\ndef diverse_candidate_generation(n_circles: int, n_trials: int = 30, \n                                 diversity_threshold: float = 0.1) -> List[Dict]:\n    \"\"\"\n    Generate diverse candidates with adaptive sampling based on novelty\n    Returns list of candidate dictionaries with metadata\n    \"\"\"\n    candidates = []\n    candidate_features = []\n    \n    # Strategy distribution with adaptive weighting\n    strategies = [\n        ('random', 0.2),\n        ('grid_noise', 0.2),\n        ('hex_noise', 0.2),\n        ('poisson_disk', 0.15),\n        ('fibonacci_spiral', 0.15),\n        ('adaptive_subdivision', 0.1)\n    ]\n    \n    for trial in range(n_trials):\n        # Select strategy based on current diversity\n        if trial < 10:\n            # Initial exploration: use all strategies equally\n            strategy_name = strategies[trial % len(strategies)][0]\n        else:\n            # Adaptive selection: favor strategies producing diverse results\n            strategy_name = select_strategy_adaptively(candidates, strategies)\n        \n        # Generate configuration based on strategy\n        centers = generate_configuration(n_circles, strategy_name, trial)\n        \n        # Compute initial radii\n        radii = compute_initial_radii(centers, n_circles)\n        \n        # Quick optimization\n        result = quick_optimize(centers, radii, max_iter=150)\n        \n        if result is not None:\n            centers_opt, radii_opt, sum_radii = result\n            \n            # Compute feature vector for diversity measurement\n            features = extract_features(centers_opt, radii_opt)\n            \n            # Check novelty against existing candidates\n            is_novel = True\n            if len(candidate_features) > 0:\n                distances = pairwise_distances([features], candidate_features, metric='euclidean')\n                min_distance = np.min(distances)\n                is_novel = min_distance > diversity_threshold\n            \n            if is_novel or trial < 5:  # Always accept first few candidates\n                candidate_dict = {\n                    'centers': centers_opt,\n                    'radii': radii_opt,\n                    'sum_radii': sum_radii,\n                    'features': features,\n                    'strategy': strategy_name,\n                    'trial': trial\n                }\n                candidates.append(candidate_dict)\n                candidate_features.append(features)\n                \n                print(f\"  Trial {trial+1}/{n_trials} ({strategy_name}): \"\n                      f\"sum_radii = {sum_radii:.6f} {'[NOVEL]' if is_novel else ''}\")\n            else:\n                print(f\"  Trial {trial+1}/{n_trials} ({strategy_name}): \"\n                      f\"sum_radii = {sum_radii:.6f} [DUPLICATE - skipped]\")\n    \n    print(f\"\\n  Generated {len(candidates)} diverse candidates from {n_trials} trials\")\n    \n    return candidates\n\n\ndef generate_configuration(n_circles: int, strategy: str, seed: int) -> np.ndarray:\n    \"\"\"Generate initial configuration based on strategy\"\"\"\n    np.random.seed(seed)\n    \n    if strategy == 'random':\n        return np.random.rand(n_circles, 2)\n    \n    elif strategy == 'grid_noise':\n        centers = create_grid_centers(n_circles)\n        noise_scale = 0.03 + 0.05 * np.random.rand()\n        centers += np.random.normal(0, noise_scale, centers.shape)\n        return np.clip(centers, 0.05, 0.95)\n    \n    elif strategy == 'hex_noise':\n        centers = create_hex_centers(n_circles)\n        noise_scale = 0.02 + 0.04 * np.random.rand()\n        centers += np.random.normal(0, noise_scale, centers.shape)\n        return np.clip(centers, 0.05, 0.95)\n    \n    elif strategy == 'poisson_disk':\n        return poisson_disk_sampling(n_circles)\n    \n    elif strategy == 'fibonacci_spiral':\n        return fibonacci_spiral_points(n_circles)\n    \n    elif strategy == 'adaptive_subdivision':\n        return adaptive_subdivision_points(n_circles)\n    \n    else:\n        return np.random.rand(n_circles, 2)\n\n\ndef poisson_disk_sampling(n_circles: int, min_distance: float = 0.1) -> np.ndarray:\n    \"\"\"Generate points using Poisson disk sampling for good spacing\"\"\"\n    points = []\n    max_attempts = 30\n    \n    # Start with random point\n    points.append(np.random.rand(2))\n    \n    while len(points) < n_circles:\n        # Pick random existing point\n        base_point = points[np.random.randint(len(points))]\n        \n        # Try to place new point around it\n        for _ in range(max_attempts):\n            angle = np.random.rand() * 2 * np.pi\n            distance = min_distance + np.random.rand() * min_distance\n            new_point = base_point + distance * np.array([np.cos(angle), np.sin(angle)])\n            \n            # Check if valid\n            if (0 <= new_point[0] <= 1 and 0 <= new_point[1] <= 1):\n                # Check distance to all existing points\n                valid = True\n                for p in points:\n                    if np.linalg.norm(new_point - p) < min_distance:\n                        valid = False\n                        break\n                \n                if valid:\n                    points.append(new_point)\n                    break\n        \n        # Fallback if we can't place more points\n        if len(points) < n_circles and len(points) > 0:\n            # Reduce min_distance or add random point\n            if np.random.rand() < 0.5:\n                min_distance *= 0.95\n            else:\n                points.append(np.random.rand(2))\n    \n    return np.array(points[:n_circles])\n\n\ndef fibonacci_spiral_points(n_circles: int) -> np.ndarray:\n    \"\"\"Generate points using Fibonacci spiral for even distribution\"\"\"\n    points = []\n    phi = (1 + np.sqrt(5)) / 2  # Golden ratio\n    \n    for i in range(n_circles):\n        # Fibonacci spiral in unit square\n        theta = 2 * np.pi * i / phi\n        r = np.sqrt(i / n_circles)\n        \n        # Map to unit square\n        x = 0.5 + 0.4 * r * np.cos(theta)\n        y = 0.5 + 0.4 * r * np.sin(theta)\n        \n        points.append([x, y])\n    \n    return np.array(points)\n\n\ndef adaptive_subdivision_points(n_circles: int) -> np.ndarray:\n    \"\"\"Generate points using adaptive space subdivision\"\"\"\n    points = []\n    \n    # Start with coarse grid\n    n_initial = int(np.sqrt(n_circles))\n    for i in range(n_initial):\n        for j in range(n_initial):\n            if len(points) >= n_circles:\n                break\n            x = (i + 0.5 + np.random.rand() * 0.3) / n_initial\n            y = (j + 0.5 + np.random.rand() * 0.3) / n_initial\n            points.append([x, y])\n    \n    # Fill remaining with adaptive placement\n    while len(points) < n_circles:\n        # Find largest empty region\n        if len(points) > 0:\n            # Place in region furthest from existing points\n            best_point = None\n            best_min_dist = 0\n            \n            for _ in range(20):\n                candidate = np.random.rand(2)\n                min_dist = min([np.linalg.norm(candidate - p) for p in points])\n                \n                if min_dist > best_min_dist:\n                    best_min_dist = min_dist\n                    best_point = candidate\n            \n            points.append(best_point)\n        else:\n            points.append(np.random.rand(2))\n    \n    return np.array(points[:n_circles])\n\n\ndef extract_features(centers: np.ndarray, radii: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Extract feature vector for measuring configuration diversity\n    Features capture spatial distribution and size characteristics\n    \"\"\"\n    features = []\n    \n    # Statistical features of radii\n    features.extend([\n        np.mean(radii),\n        np.std(radii),\n        np.min(radii),\n        np.max(radii),\n        np.median(radii)\n    ])\n    \n    # Spatial distribution features\n    features.extend([\n        np.mean(centers[:, 0]),\n        np.std(centers[:, 0]),\n        np.mean(centers[:, 1]),\n        np.std(centers[:, 1])\n    ])\n    \n    # Pairwise distance statistics\n    dists = pairwise_distances(centers, metric='euclidean')\n    dists = dists[np.triu_indices_from(dists, k=1)]\n    features.extend([\n        np.mean(dists),\n        np.std(dists),\n        np.min(dists),\n        np.max(dists)\n    ])\n    \n    # Density in quadrants\n    for i in range(2):\n        for j in range(2):\n            mask = (centers[:, 0] >= i*0.5) & (centers[:, 0] < (i+1)*0.5) & \\\n                   (centers[:, 1] >= j*0.5) & (centers[:, 1] < (j+1)*0.5)\n            features.append(np.sum(mask) / len(centers))\n    \n    # Radius correlation with position\n    features.extend([\n        np.corrcoef(centers[:, 0], radii)[0, 1],\n        np.corrcoef(centers[:, 1], radii)[0, 1]\n    ])\n    \n    return np.array(features)\n\n\ndef select_strategy_adaptively(candidates: List[Dict], \n                               strategies: List[Tuple[str, float]]) -> str:\n    \"\"\"Select strategy based on past performance and diversity\"\"\"\n    if len(candidates) < 5:\n        return strategies[np.random.randint(len(strategies))][0]\n    \n    # Compute strategy scores\n    strategy_scores = {name: [] for name, _ in strategies}\n    \n    for candidate in candidates[-10:]:  # Look at recent candidates\n        strategy = candidate['strategy']\n        score = candidate['sum_radii']\n        if strategy in strategy_scores:\n            strategy_scores[strategy].append(score)\n    \n    # Compute weighted probabilities\n    strategy_probs = []\n    for name, base_prob in strategies:\n        if len(strategy_scores[name]) > 0:\n            avg_score = np.mean(strategy_scores[name])\n            # Favor strategies with high scores\n            prob = base_prob * (1 + avg_score)\n        else:\n            # Give unexplored strategies a boost\n            prob = base_prob * 1.5\n        strategy_probs.append(prob)\n    \n    # Normalize\n    strategy_probs = np.array(strategy_probs)\n    strategy_probs /= strategy_probs.sum()\n    \n    # Sample strategy\n    idx = np.random.choice(len(strategies), p=strategy_probs)\n    return strategies[idx][0]\n\n\ndef select_diverse_candidates(candidates: List[Dict], \n                              n_clusters: int = 5,\n                              top_per_cluster: int = 2) -> List[Dict]:\n    \"\"\"\n    Select diverse candidates using clustering in feature space\n    \"\"\"\n    if len(candidates) <= n_clusters:\n        print(f\"  Using all {len(candidates)} candidates (fewer than {n_clusters} clusters)\")\n        return candidates\n    \n    # Extract features\n    features = np.array([c['features'] for c in candidates])\n    \n    # Normalize features\n    features_normalized = (features - features.mean(axis=0)) / (features.std(axis=0) + 1e-8)\n    \n    # Cluster candidates\n    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n    cluster_labels = kmeans.fit_predict(features_normalized)\n    \n    # Select top candidates from each cluster\n    selected = []\n    for cluster_id in range(n_clusters):\n        cluster_mask = cluster_labels == cluster_id\n        cluster_candidates = [c for i, c in enumerate(candidates) if cluster_mask[i]]\n        \n        # Sort by sum_radii within cluster\n        cluster_candidates.sort(key=lambda x: x['sum_radii'], reverse=True)\n        \n        # Take top candidates from this cluster\n        selected.extend(cluster_candidates[:top_per_cluster])\n        \n        print(f\"  Cluster {cluster_id}: {np.sum(cluster_mask)} candidates, \"\n              f\"selected top {min(top_per_cluster, len(cluster_candidates))}\")\n    \n    # Sort selected by sum_radii\n    selected.sort(key=lambda x: x['sum_radii'], reverse=True)\n    \n    print(f\"\\n  Selected {len(selected)} diverse candidates for intensive optimization\")\n    \n    return selected\n\n\ndef novelty_aware_optimization(candidates: List[Dict], \n                               n_circles: int,\n                               exploration_weight: float = 0.3) -> Optional[Tuple]:\n    \"\"\"\n    Optimize candidates while maintaining exploration of diverse solutions\n    \"\"\"\n    best_result = None\n    best_sum_radii = 0\n    \n    optimized_features = []\n    \n    for idx, candidate in enumerate(candidates):\n        print(f\"\\n  Optimizing candidate {idx+1}/{len(candidates)} \"\n              f\"(strategy: {candidate['strategy']}, initial: {candidate['sum_radii']:.6f})...\")\n        \n        centers = candidate['centers']\n        radii =",
  "reasoning": "Here's an enhanced version with smarter candidate selection using clustering, diversity metrics, and adaptive sampling:\n\n```python\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import pairwise_distances\nfrom typing import Tuple, List, Optional, Dict\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndef construct_packing() -> Tuple[np.ndarray, np.ndarray, float]:\n    \"\"\"\n    Enhanced hybrid approach with smart candidate selection\n    \n    Returns:\n        centers: np.array of shape (26, 2)\n        radii: np.array of shape (26,)\n        sum_radii: float\n    \"\"\"\n    n_circles = 26\n    \n    # Phase 1: Diverse candidate generation with adaptive sampling\n    print(\"Phase 1: Generating diverse initial configurations...\")\n    initial_candidates = diverse_candidate_generation(\n        n_circles, \n        n_trials=30,\n        diversity_threshold=0.1\n    )\n    \n    # Phase 2: Smart selection from diverse clusters\n    print(\"\\nPhase 2: Selecting diverse candidates via clustering...\")\n    selected_candidates = select_diverse_candidates(\n        initial_candidates, \n        n_clusters=5,\n        top_per_cluster=2\n    )\n    \n    # Phase 3: Intensive optimization with novelty preservation\n    print(\"\\nPhase 3: Intensive optimization with novelty search...\")\n    best_result = novelty_aware_optimization(\n        selected_candidates, \n        n_circles,\n        exploration_weight=0.3\n    )\n    \n    if best_result is None:\n        print(\"Optimization failed, using fallback grid packing\")\n        centers, radii = create_grid_packing(n_circles)\n        sum_radii = np.sum(radii)\n        best_result = (centers, radii, sum_radii)\n    \n    centers, radii, sum_radii = best_result\n    print(f\"\\nFinal result: Sum of radii = {sum_radii:.6f}\")\n    print(f\"Average radius: {np.mean(radii):.6f}\")\n    print(f\"Valid packing: {verify_packing(centers, radii)}\")\n    \n    return centers, radii, sum_radii\n\ndef diverse_candidate_generation(n_circles: int, n_trials: int = 30, \n                                 diversity_threshold: float = 0.1) -> List[Dict]:\n    \"\"\"\n    Generate diverse candidates with adaptive sampling based on novelty\n    Returns list of candidate dictionaries with metadata\n    \"\"\"\n    candidates = []\n    candidate_features = []\n    \n    # Strategy distribution with adaptive weighting\n    strategies = [\n        ('random', 0.2),\n        ('grid_noise', 0.2),\n        ('hex_noise', 0.2),\n        ('poisson_disk', 0.15),\n        ('fibonacci_spiral', 0.15),\n        ('adaptive_subdivision', 0.1)\n    ]\n    \n    for trial in range(n_trials):\n        # Select strategy based on current diversity\n        if trial < 10:\n            # Initial exploration: use all strategies equally\n            strategy_name = strategies[trial % len(strategies)][0]\n        else:\n            # Adaptive selection: favor strategies producing diverse results\n            strategy_name = select_strategy_adaptively(candidates, strategies)\n        \n        # Generate configuration based on strategy\n        centers = generate_configuration(n_circles, strategy_name, trial)\n        \n        # Compute initial radii\n        radii = compute_initial_radii(centers, n_circles)\n        \n        # Quick optimization\n        result = quick_optimize(centers, radii, max_iter=150)\n        \n        if result is not None:\n            centers_opt, radii_opt, sum_radii = result\n            \n            # Compute feature vector for diversity measurement\n            features = extract_features(centers_opt, radii_opt)\n            \n            # Check novelty against existing candidates\n            is_novel = True\n            if len(candidate_features) > 0:\n                distances = pairwise_distances([features], candidate_features, metric='euclidean')\n                min_distance = np.min(distances)\n                is_novel = min_distance > diversity_threshold\n            \n            if is_novel or trial < 5:  # Always accept first few candidates\n                candidate_dict = {\n                    'centers': centers_opt,\n                    'radii': radii_opt,\n                    'sum_radii': sum_radii,\n                    'features': features,\n                    'strategy': strategy_name,\n                    'trial': trial\n                }\n                candidates.append(candidate_dict)\n                candidate_features.append(features)\n                \n                print(f\"  Trial {trial+1}/{n_trials} ({strategy_name}): \"\n                      f\"sum_radii = {sum_radii:.6f} {'[NOVEL]' if is_novel else ''}\")\n            else:\n                print(f\"  Trial {trial+1}/{n_trials} ({strategy_name}): \"\n                      f\"sum_radii = {sum_radii:.6f} [DUPLICATE - skipped]\")\n    \n    print(f\"\\n  Generated {len(candidates)} diverse candidates from {n_trials} trials\")\n    \n    return candidates\n\ndef generate_configuration(n_circles: int, strategy: str, seed: int) -> np.ndarray:\n    \"\"\"Generate initial configuration based on strategy\"\"\"\n    np.random.seed(seed)\n    \n    if strategy == 'random':\n        return np.random.rand(n_circles, 2)\n    \n    elif strategy == 'grid_noise':\n        centers = create_grid_centers(n_circles)\n        noise_scale = 0.03 + 0.05 * np.random.rand()\n        centers += np.random.normal(0, noise_scale, centers.shape)\n        return np.clip(centers, 0.05, 0.95)\n    \n    elif strategy == 'hex_noise':\n        centers = create_hex_centers(n_circles)\n        noise_scale = 0.02 + 0.04 * np.random.rand()\n        centers += np.random.normal(0, noise_scale, centers.shape)\n        return np.clip(centers, 0.05, 0.95)\n    \n    elif strategy == 'poisson_disk':\n        return poisson_disk_sampling(n_circles)\n    \n    elif strategy == 'fibonacci_spiral':\n        return fibonacci_spiral_points(n_circles)\n    \n    elif strategy == 'adaptive_subdivision':\n        return adaptive_subdivision_points(n_circles)\n    \n    else:\n        return np.random.rand(n_circles, 2)\n\ndef poisson_disk_sampling(n_circles: int, min_distance: float = 0.1) -> np.ndarray:\n    \"\"\"Generate points using Poisson disk sampling for good spacing\"\"\"\n    points = []\n    max_attempts = 30\n    \n    # Start with random point\n    points.append(np.random.rand(2))\n    \n    while len(points) < n_circles:\n        # Pick random existing point\n        base_point = points[np.random.randint(len(points))]\n        \n        # Try to place new point around it\n        for _ in range(max_attempts):\n            angle = np.random.rand() * 2 * np.pi\n            distance = min_distance + np.random.rand() * min_distance\n            new_point = base_point + distance * np.array([np.cos(angle), np.sin(angle)])\n            \n            # Check if valid\n            if (0 <= new_point[0] <= 1 and 0 <= new_point[1] <= 1):\n                # Check distance to all existing points\n                valid = True\n                for p in points:\n                    if np.linalg.norm(new_point - p) < min_distance:\n                        valid = False\n                        break\n                \n                if valid:\n                    points.append(new_point)\n                    break\n        \n        # Fallback if we can't place more points\n        if len(points) < n_circles and len(points) > 0:\n            # Reduce min_distance or add random point\n            if np.random.rand() < 0.5:\n                min_distance *= 0.95\n            else:\n                points.append(np.random.rand(2))\n    \n    return np.array(points[:n_circles])\n\ndef fibonacci_spiral_points(n_circles: int) -> np.ndarray:\n    \"\"\"Generate points using Fibonacci spiral for even distribution\"\"\"\n    points = []\n    phi = (1 + np.sqrt(5)) / 2  # Golden ratio\n    \n    for i in range(n_circles):\n        # Fibonacci spiral in unit square\n        theta = 2 * np.pi * i / phi\n        r = np.sqrt(i / n_circles)\n        \n        # Map to unit square\n        x = 0.5 + 0.4 * r * np.cos(theta)\n        y = 0.5 + 0.4 * r * np.sin(theta)\n        \n        points.append([x, y])\n    \n    return np.array(points)\n\ndef adaptive_subdivision_points(n_circles: int) -> np.ndarray:\n    \"\"\"Generate points using adaptive space subdivision\"\"\"\n    points = []\n    \n    # Start with coarse grid\n    n_initial = int(np.sqrt(n_circles))\n    for i in range(n_initial):\n        for j in range(n_initial):\n            if len(points) >= n_circles:\n                break\n            x = (i + 0.5 + np.random.rand() * 0.3) / n_initial\n            y = (j + 0.5 + np.random.rand() * 0.3) / n_initial\n            points.append([x, y])\n    \n    # Fill remaining with adaptive placement\n    while len(points) < n_circles:\n        # Find largest empty region\n        if len(points) > 0:\n            # Place in region furthest from existing points\n            best_point = None\n            best_min_dist = 0\n            \n            for _ in range(20):\n                candidate = np.random.rand(2)\n                min_dist = min([np.linalg.norm(candidate - p) for p in points])\n                \n                if min_dist > best_min_dist:\n                    best_min_dist = min_dist\n                    best_point = candidate\n            \n            points.append(best_point)\n        else:\n            points.append(np.random.rand(2))\n    \n    return np.array(points[:n_circles])\n\ndef extract_features(centers: np.ndarray, radii: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Extract feature vector for measuring configuration diversity\n    Features capture spatial distribution and size characteristics\n    \"\"\"\n    features = []\n    \n    # Statistical features of radii\n    features.extend([\n        np.mean(radii),\n        np.std(radii),\n        np.min(radii),\n        np.max(radii),\n        np.median(radii)\n    ])\n    \n    # Spatial distribution features\n    features.extend([\n        np.mean(centers[:, 0]),\n        np.std(centers[:, 0]),\n        np.mean(centers[:, 1]),\n        np.std(centers[:, 1])\n    ])\n    \n    # Pairwise distance statistics\n    dists = pairwise_distances(centers, metric='euclidean')\n    dists = dists[np.triu_indices_from(dists, k=1)]\n    features.extend([\n        np.mean(dists),\n        np.std(dists),\n        np.min(dists),\n        np.max(dists)\n    ])\n    \n    # Density in quadrants\n    for i in range(2):\n        for j in range(2):\n            mask = (centers[:, 0] >= i*0.5) & (centers[:, 0] < (i+1)*0.5) & \\\n                   (centers[:, 1] >= j*0.5) & (centers[:, 1] < (j+1)*0.5)\n            features.append(np.sum(mask) / len(centers))\n    \n    # Radius correlation with position\n    features.extend([\n        np.corrcoef(centers[:, 0], radii)[0, 1],\n        np.corrcoef(centers[:, 1], radii)[0, 1]\n    ])\n    \n    return np.array(features)\n\ndef select_strategy_adaptively(candidates: List[Dict], \n                               strategies: List[Tuple[str, float]]) -> str:\n    \"\"\"Select strategy based on past performance and diversity\"\"\"\n    if len(candidates) < 5:\n        return strategies[np.random.randint(len(strategies))][0]\n    \n    # Compute strategy scores\n    strategy_scores = {name: [] for name, _ in strategies}\n    \n    for candidate in candidates[-10:]:  # Look at recent candidates\n        strategy = candidate['strategy']\n        score = candidate['sum_radii']\n        if strategy in strategy_scores:\n            strategy_scores[strategy].append(score)\n    \n    # Compute weighted probabilities\n    strategy_probs = []\n    for name, base_prob in strategies:\n        if len(strategy_scores[name]) > 0:\n            avg_score = np.mean(strategy_scores[name])\n            # Favor strategies with high scores\n            prob = base_prob * (1 + avg_score)\n        else:\n            # Give unexplored strategies a boost\n            prob = base_prob * 1.5\n        strategy_probs.append(prob)\n    \n    # Normalize\n    strategy_probs = np.array(strategy_probs)\n    strategy_probs /= strategy_probs.sum()\n    \n    # Sample strategy\n    idx = np.random.choice(len(strategies), p=strategy_probs)\n    return strategies[idx][0]\n\ndef select_diverse_candidates(candidates: List[Dict], \n                              n_clusters: int = 5,\n                              top_per_cluster: int = 2) -> List[Dict]:\n    \"\"\"\n    Select diverse candidates using clustering in feature space\n    \"\"\"\n    if len(candidates) <= n_clusters:\n        print(f\"  Using all {len(candidates)} candidates (fewer than {n_clusters} clusters)\")\n        return candidates\n    \n    # Extract features\n    features = np.array([c['features'] for c in candidates])\n    \n    # Normalize features\n    features_normalized = (features - features.mean(axis=0)) / (features.std(axis=0) + 1e-8)\n    \n    # Cluster candidates\n    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n    cluster_labels = kmeans.fit_predict(features_normalized)\n    \n    # Select top candidates from each cluster\n    selected = []\n    for cluster_id in range(n_clusters):\n        cluster_mask = cluster_labels == cluster_id\n        cluster_candidates = [c for i, c in enumerate(candidates) if cluster_mask[i]]\n        \n        # Sort by sum_radii within cluster\n        cluster_candidates.sort(key=lambda x: x['sum_radii'], reverse=True)\n        \n        # Take top candidates from this cluster\n        selected.extend(cluster_candidates[:top_per_cluster])\n        \n        print(f\"  Cluster {cluster_id}: {np.sum(cluster_mask)} candidates, \"\n              f\"selected top {min(top_per_cluster, len(cluster_candidates))}\")\n    \n    # Sort selected by sum_radii\n    selected.sort(key=lambda x: x['sum_radii'], reverse=True)\n    \n    print(f\"\\n  Selected {len(selected)} diverse candidates for intensive optimization\")\n    \n    return selected\n\ndef novelty_aware_optimization(candidates: List[Dict], \n                               n_circles: int,\n                               exploration_weight: float = 0.3) -> Optional[Tuple]:\n    \"\"\"\n    Optimize candidates while maintaining exploration of diverse solutions\n    \"\"\"\n    best_result = None\n    best_sum_radii = 0\n    \n    optimized_features = []\n    \n    for idx, candidate in enumerate(candidates):\n        print(f\"\\n  Optimizing candidate {idx+1}/{len(candidates)} \"\n              f\"(strategy: {candidate['strategy']}, initial: {candidate['sum_radii']:.6f})...\")\n        \n        centers = candidate['centers']\n        radii =",
  "timestamp": "2025-12-08T12:40:46.444819",
  "cost_data": null
}